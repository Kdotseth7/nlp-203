{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "LgRVQByUTOqE",
      "metadata": {
        "id": "LgRVQByUTOqE"
      },
      "source": [
        "# Download and Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70d2c80a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70d2c80a",
        "outputId": "26f3e171-e68c-4d89-d73e-52951ae4f33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: rouge, spacy\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.5.2\n",
            "    Uninstalling spacy-3.5.2:\n",
            "      Successfully uninstalled spacy-3.5.2\n",
            "Successfully installed rouge-1.0.1 spacy-3.5.3\n",
            "2023-06-03 06:53:20.693491: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-06-03 06:53:20.746626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-03 06:53:21.787990: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade matplotlib rouge spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LoZBDuEcTX6t",
      "metadata": {
        "id": "LoZBDuEcTX6t"
      },
      "source": [
        "# Import Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f152c025",
      "metadata": {
        "id": "f152c025"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "import spacy\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if sys.version_info[0] < 3:\n",
        "    from StringIO import StringIO\n",
        "else:\n",
        "    from io import StringIO\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NibcZndLTcHS",
      "metadata": {
        "id": "NibcZndLTcHS"
      },
      "source": [
        "# Mount Google Drive and Set Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "MMjoEnVk2tQJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMjoEnVk2tQJ",
        "outputId": "4fa3260e-efcd-4256-d501-ee4da6919604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_folder = 'hw3'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "os.chdir(os.path.join(drive_path, project_folder))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PB2Idnw2099c",
      "metadata": {
        "id": "PB2Idnw2099c"
      },
      "source": [
        "# Set Random Seed for Reproducible Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "127_tomy1Ck_",
      "metadata": {
        "id": "127_tomy1Ck_"
      },
      "outputs": [],
      "source": [
        "SEED = 32\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_NpmJtff1LqO",
      "metadata": {
        "id": "_NpmJtff1LqO"
      },
      "source": [
        "# Set GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "575c65af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "575c65af",
        "outputId": "8ddbe1a0-7a23-4091-f7ca-6e9ca48f3695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"using {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_CNib4QcTqrA",
      "metadata": {
        "id": "_CNib4QcTqrA"
      },
      "source": [
        "# Set Hyper-Parameters and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "27ffa629",
      "metadata": {
        "id": "27ffa629"
      },
      "outputs": [],
      "source": [
        "START = \"<START>\"\n",
        "STOP = \"<STOP>\"\n",
        "PADDING = \"<PAD>\"\n",
        "UNK_TOKEN = \"<UNK>\"\n",
        "DEVICE = device\n",
        "TRUNCATE_SIZE = 512\n",
        "MAX_VOCAB_SIZE = 512\n",
        "LEAST_FREQ = 0\n",
        "BATCH_SIZE = 8\n",
        "ENC_EMB_DIM = 64\n",
        "DEC_EMB_DIM = 64\n",
        "ENC_HID_DIM = 128\n",
        "DEC_HID_DIM = 128\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "IMPROVEMENT = False\n",
        "if IMPROVEMENT:\n",
        "  NAME = \"improvement\"\n",
        "else:\n",
        "  NAME = \"baseline\"\n",
        "LINE_CONSTRAINT = 50000\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VSTlmoSOTvXB",
      "metadata": {
        "id": "VSTlmoSOTvXB"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "59b01804",
      "metadata": {
        "id": "59b01804"
      },
      "outputs": [],
      "source": [
        "def display_stage(stage_title: str):\n",
        "    \"\"\"Display stage title with padding.\"\"\"\n",
        "    total_length = 100\n",
        "    title_length = len(stage_title)\n",
        "    pad_count = int((total_length - title_length) / 2)\n",
        "    padding = \"*\" * pad_count\n",
        "    formatted_title = f\"{padding}{stage_title}{padding}\"\n",
        "    print(formatted_title)\n",
        "\n",
        "\n",
        "def epoch_time(start_time: float, \n",
        "               end_time: float\n",
        "            ):\n",
        "    \"\"\"Calculate the time in minutes and seconds for each epoch.\"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "def read_files(file_name, lines_constraint=None):\n",
        "    results = list()\n",
        "    with open(file_name, encoding=\"utf-8\") as f:\n",
        "        count = 0\n",
        "        for line in f:\n",
        "            results.append(line.replace(\"\\n\", \"\"))\n",
        "            if lines_constraint:\n",
        "                count += 1\n",
        "                if count >= lines_constraint:\n",
        "                    break\n",
        "    return results\n",
        "\n",
        "\n",
        "def write_predictions(preds, split, name):\n",
        "    with open(f\"./{split}_summaries.txt\", \"w\") as f:\n",
        "        f.write(\"\\n\".join(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "573a9acf",
      "metadata": {
        "id": "573a9acf"
      },
      "source": [
        "# Set FilePaths and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9dc7d6ae",
      "metadata": {
        "id": "9dc7d6ae"
      },
      "outputs": [],
      "source": [
        "SRC_FILENAME = {\n",
        "    \"train\": \"./data/train.txt.src\",\n",
        "    \"train_truncate\": \"./data/truncated_train.txt.src\",\n",
        "    \"dev\": \"./data/val.txt.src\",\n",
        "    \"test\": \"./data/test.txt.src\",\n",
        "}\n",
        "TGT_FILENAME = {\n",
        "    \"train\": \"./data/train.txt.tgt\",\n",
        "    \"dev\": \"./data/val.txt.tgt\",\n",
        "    \"test\": \"./data/test.txt.tgt\",\n",
        "}\n",
        "\n",
        "def add_special_tokens(tokens):\n",
        "    return [START] + tokens.split() + [STOP]\n",
        "\n",
        "\n",
        "def build_data_points(data_df):\n",
        "    return data_df.apply(lambda row: (add_special_tokens(row[\"text\"]), add_special_tokens(row[\"summary\"])), axis=1)\n",
        "\n",
        "def load_data(split, line_constraint=None, truncate=False):\n",
        "    inputs = read_files(\n",
        "        SRC_FILENAME[f\"{split}_truncate\"]\n",
        "        if truncate and split == \"train\"\n",
        "        else SRC_FILENAME[split],\n",
        "        line_constraint,\n",
        "    )\n",
        "    outputs = read_files(TGT_FILENAME[split], line_constraint)\n",
        "    df = pd.DataFrame({\"text\": inputs, \"summary\": outputs})\n",
        "    max_len_summary = max(\n",
        "        outputs, key=lambda summary: len(summary.split())\n",
        "    ).split()\n",
        "    max_len = len(max_len_summary)\n",
        "    data_point_df = build_data_points(df)\n",
        "    return data_point_df, max_len"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2919ed",
      "metadata": {
        "id": "fe2919ed"
      },
      "source": [
        "# CNN Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3585d6b6",
      "metadata": {
        "id": "3585d6b6"
      },
      "outputs": [],
      "source": [
        "class CNNDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.X = list()\n",
        "        self.y = list()\n",
        "        self.raw_y = list()\n",
        "        for text, summary in data:\n",
        "            if len(text) > 0:\n",
        "                self.X.append(text)\n",
        "                self.y.append(summary)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.X[index], self.y[index], index\n",
        "\n",
        "\n",
        "def reverse_map(_map):\n",
        "    reversed_map = {}\n",
        "    for key, val in _map.items():\n",
        "        reversed_map[val] = key\n",
        "    return reversed_map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d3c96e",
      "metadata": {
        "id": "87d3c96e"
      },
      "source": [
        "# Create Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6f42d784",
      "metadata": {
        "id": "6f42d784"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, tokens, base_map={}, max_size=None, least_freq=0):\n",
        "        self.token2idx = base_map\n",
        "        self.freq = Counter(\n",
        "            [token for sequence in tokens for token in sequence]\n",
        "        )\n",
        "\n",
        "        vocab_size = 0\n",
        "        # store the token start from higher frequency\n",
        "        for word, count in sorted(\n",
        "            self.freq.items(), key=lambda item: item[1], reverse=True\n",
        "        ):\n",
        "            if count < least_freq:\n",
        "                break\n",
        "            if max_size is not None and vocab_size > max_size:\n",
        "                break\n",
        "            self.insert(word)\n",
        "            vocab_size += 1\n",
        "\n",
        "        self.idx2token = reverse_map(self.token2idx)\n",
        "\n",
        "    def insert(self, token):\n",
        "        if token in self.token2idx.keys():\n",
        "            return\n",
        "        self.token2idx[token] = len(self.token2idx)\n",
        "\n",
        "    def lookup_index(self, word):\n",
        "        if word not in self.token2idx.keys():\n",
        "            word = UNK_TOKEN\n",
        "        return self.token2idx[word]\n",
        "\n",
        "    def lookup_token(self, idx):\n",
        "        return self.idx2token[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token2idx)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.token2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02e03b42",
      "metadata": {
        "id": "02e03b42"
      },
      "source": [
        "# Load Data and Create DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "64daa8c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64daa8c5",
        "outputId": "beb53ab3-61b4-4f95-8133-769c9765a31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****************************************Load Training Data*****************************************\n",
            "*******************************************Load Dev Data*******************************************\n",
            "*******************************************Load Test Data*******************************************\n",
            "*******************************************Building Vocab*******************************************\n",
            "Vocab Length= 515\n"
          ]
        }
      ],
      "source": [
        "display_stage(\"Load Training Data\")\n",
        "train_data, _ = load_data(\"train\", LINE_CONSTRAINT)[:]\n",
        "display_stage(\"Load Dev Data\")\n",
        "dev_data, dev_summary_max_len = load_data(\"dev\")[:]\n",
        "display_stage(\"Load Test Data\")\n",
        "test_data, test_summary_max_len = load_data(\"test\")[:]\n",
        "\n",
        "summary_max_len = max(dev_summary_max_len, test_summary_max_len)\n",
        "\n",
        "train_set = CNNDataset(train_data)\n",
        "dev_set = CNNDataset(dev_data)\n",
        "test_set = CNNDataset(test_data)\n",
        "\n",
        "display_stage(\"Building Vocab\")\n",
        "word_vocab = Vocab(\n",
        "    train_set.X + train_set.y,\n",
        "    base_map={PADDING: 0, UNK_TOKEN: 1},\n",
        "    max_size=MAX_VOCAB_SIZE,\n",
        "    least_freq=LEAST_FREQ,\n",
        ")\n",
        "\n",
        "print(\"Vocab Length=\", len(word_vocab))\n",
        "\n",
        "\n",
        "def text_pipeline(sentence, truncate_size=TRUNCATE_SIZE):\n",
        "    if len(sentence) > truncate_size:\n",
        "        sentence = sentence[:truncate_size]\n",
        "    return [word_vocab.lookup_index(token) for token in sentence]\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    summary_list, text_list, index_list = list(), list(), list()\n",
        "    for _text, _summary, _index in batch:\n",
        "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.long,))\n",
        "        summary_list.append(\n",
        "            torch.tensor(\n",
        "                text_pipeline(_summary, truncate_size=len(_summary)),\n",
        "                dtype=torch.long,\n",
        "            )\n",
        "        )\n",
        "        index_list.append(torch.tensor(_index, dtype=torch.long))\n",
        "\n",
        "    len_list = torch.tensor(list(map(len, text_list)), dtype=torch.long)\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=0)\n",
        "    summary_list = pad_sequence(\n",
        "        summary_list, batch_first=True, padding_value=0\n",
        "    )\n",
        "    index_list = torch.tensor(index_list, dtype=torch.long)\n",
        "\n",
        "    # sort the batch according to the sequence length in the descending order\n",
        "    len_list, perm_idx = len_list.sort(0, descending=True)\n",
        "    text_list = text_list[perm_idx]\n",
        "    summary_list = summary_list[perm_idx]\n",
        "    index_list = index_list[perm_idx]\n",
        "\n",
        "    return text_list.to(DEVICE), summary_list.to(DEVICE), len_list, index_list\n",
        "\n",
        "\n",
        "def get_data_loader(batch_size: int = 1, set_name=\"train\"):\n",
        "    assert set_name in [\"train\", \"dev\", \"test\"]\n",
        "    if set_name == \"train\":\n",
        "        return DataLoader(dataset=train_set, batch_size=batch_size, collate_fn=collate_batch, shuffle=False)\n",
        "    elif set_name == \"dev\":\n",
        "        return DataLoader(dataset=dev_set, batch_size=batch_size, collate_fn=collate_batch, shuffle=False)\n",
        "    else:\n",
        "        return DataLoader(dataset=test_set, batch_size=batch_size, collate_fn=collate_batch, shuffle=False,)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AYOqBbwyUg3y",
      "metadata": {
        "id": "AYOqBbwyUg3y"
      },
      "source": [
        "# Encoder Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cdf96d9b",
      "metadata": {
        "id": "cdf96d9b"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to(\"cpu\"))\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)))\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KxgcZEZY808R",
      "metadata": {
        "id": "KxgcZEZY808R"
      },
      "source": [
        "# Attention Mechanism and Decoder Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "E3-_IoNU83gL",
      "metadata": {
        "id": "E3-_IoNU83gL"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "        a = a.unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        assert (output == hidden).all()\n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bayHESiK86Ze",
      "metadata": {
        "id": "bayHESiK86Ze"
      },
      "source": [
        "# Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "qd3Dhm2288oc",
      "metadata": {
        "id": "qd3Dhm2288oc"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "        input = trg[0, :]\n",
        "        mask = self.create_mask(src)\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n_pPo3XXUqM5",
      "metadata": {
        "id": "n_pPo3XXUqM5"
      },
      "source": [
        "# Truncate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d6b9c3c9",
      "metadata": {
        "id": "d6b9c3c9"
      },
      "outputs": [],
      "source": [
        "def truncate(tokens, size):\n",
        "    if size <= 0:\n",
        "        return list()\n",
        "    if len(tokens) <= size:\n",
        "        return tokens\n",
        "    to_remove_count = int((len(tokens) - size) / 2)\n",
        "    return tokens[to_remove_count:-to_remove_count]\n",
        "\n",
        "\n",
        "def apply_truncate(row, truncate_size):\n",
        "    text = list(row[\"text\"])\n",
        "    if row[\"text_len\"] <= truncate_size:\n",
        "        return text\n",
        "    clean_text = row[\"clean_text\"]\n",
        "    clean_text_len = row[\"clean_text_len\"]\n",
        "    if clean_text_len <= truncate_size:\n",
        "        return clean_text\n",
        "    sents = row[\"sents\"]\n",
        "    truncated_text = list()\n",
        "    for sent in sents:\n",
        "        tokens = [token for token in list(sent) if is_clean(token)]\n",
        "        size = round(truncate_size * len(tokens) / clean_text_len)\n",
        "        truncated_text += truncate(tokens, size)\n",
        "    return truncated_text\n",
        "\n",
        "\n",
        "def is_clean(spacy_token):\n",
        "    return (\n",
        "        not spacy_token.is_stop\n",
        "        and not spacy_token.is_punct\n",
        "        and len(str(spacy_token).strip()) > 0\n",
        "    )\n",
        "\n",
        "\n",
        "def apply_spacy(df):\n",
        "    results = list()\n",
        "    for text in tqdm(df[\"text\"]):\n",
        "        spacy_text = spacy_en(text)\n",
        "        results += [spacy_text]\n",
        "    return results\n",
        "\n",
        "\n",
        "def get_truncated_df(df):\n",
        "    display_stage(\"applying spacy\")\n",
        "    df[\"text\"] = apply_spacy(df)\n",
        "    df[\"text_len\"] = df[\"text\"].apply(len)\n",
        "    display_stage(\"cleaning text\")\n",
        "    df[\"clean_text\"] = df[\"text\"].apply(\n",
        "        lambda doc: [token for token in list(doc) if is_clean(token)]\n",
        "    )\n",
        "    df[\"clean_text_len\"] = df[\"clean_text\"].apply(len)\n",
        "\n",
        "    display_stage(\"sentencizing\")\n",
        "    df[\"sents\"] = df[\"text\"].apply(lambda doc: list(doc.sents))\n",
        "\n",
        "    display_stage(\"truncating\")\n",
        "    average_text_len = int(df[\"clean_text_len\"].median())\n",
        "    df[\"truncated_text\"] = df.apply(\n",
        "        lambda row: apply_truncate(row, average_text_len), axis=1\n",
        "    )\n",
        "    df[\"truncated_text_len\"] = df[\"truncated_text\"].apply(len)\n",
        "    df[\"truncated_full_text\"] = df[\"truncated_text\"].apply(\n",
        "        lambda tokens: \" \".join([str(token) for token in list(tokens)])\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "def write_truncated_train_file(df):\n",
        "    display_stage(\"writing to file\")\n",
        "    file_name = \"./truncated_train.txt.src\"\n",
        "    with open(file_name, \"w\") as f:\n",
        "        for text in tqdm(df):\n",
        "            f.write(f\"{text}\\n\")\n",
        "\n",
        "\n",
        "def generate_truncated_dataset():\n",
        "    inputs = read_files(SRC_FILENAME[\"train\"])\n",
        "    df = pd.DataFrame({\"text\": inputs})\n",
        "    df = get_truncated_df(df)\n",
        "    write_truncated_train_file(df[\"truncated_full_text\"])\n",
        "\n",
        "generate_truncated_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PwsijDx8U0T1",
      "metadata": {
        "id": "PwsijDx8U0T1"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "00b05d92",
      "metadata": {
        "id": "00b05d92"
      },
      "outputs": [],
      "source": [
        "METRICS = [\"epoch\", \"loss\", \"perplexity\"] + [\n",
        "    f\"rouge-{x}-{m}\"\n",
        "    for x in [\"1\", \"2\", \"l\"]\n",
        "    for m in [\"precision\", \"recall\", \"f1\"]\n",
        "]\n",
        "\n",
        "\n",
        "def init_report():\n",
        "    return {metric: list() for metric in METRICS}\n",
        "\n",
        "def inference(model, loader, device):\n",
        "    model.eval()\n",
        "    preds = list()\n",
        "    for batch in tqdm(loader):\n",
        "        text, _, text_len, _ = batch\n",
        "        text = text = text.view(-1, 1)\n",
        "        pred = summarize(text, text_len, model, device)\n",
        "        preds.append(\" \".join(pred))\n",
        "    return preds\n",
        "\n",
        "\n",
        "def summarize(text_tensor, lens, model, device, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(text_tensor, lens)\n",
        "    mask = model.create_mask(text_tensor)\n",
        "    summary_idxes = [word_vocab.lookup_index(START)]\n",
        "    attentions = torch.zeros(max_len, 1, len(text_tensor)).to(device)\n",
        "    for i in range(max_len):\n",
        "        last_output = torch.LongTensor([summary_idxes[-1]]).to(device)\n",
        "        with torch.no_grad():\n",
        "            output, hidden, attention = model.decoder(last_output, hidden, encoder_outputs, mask)\n",
        "        attentions[i] = attention\n",
        "        pred_token = output.argmax(1).item()\n",
        "        summary_idxes.append(pred_token)\n",
        "        if pred_token == word_vocab.lookup_index(STOP):\n",
        "            break\n",
        "    summary_tokens = [word_vocab.lookup_token(idx) for idx in summary_idxes]\n",
        "    return summary_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uwhkYMoPU2dU",
      "metadata": {
        "id": "uwhkYMoPU2dU"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b7a42ed0",
      "metadata": {
        "id": "b7a42ed0"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "\n",
        "sys.setrecursionlimit(summary_max_len * summary_max_len + 10)\n",
        "\n",
        "def calculate_rouges(preds, golds):\n",
        "\n",
        "    result = {}\n",
        "    scores = rouge.get_scores(preds, golds)\n",
        "    score_df = pd.DataFrame(scores)\n",
        "    for k in [\"1\", \"2\", \"l\"]:\n",
        "        for m in [\"p\", \"r\", \"f\"]:\n",
        "            key = f\"rouge-{k}-{m}\"\n",
        "            value = (score_df[f\"rouge-{k}\"].apply(lambda score_dict: score_dict[m]).mean())\n",
        "            result[key] = value\n",
        "    return result\n",
        "\n",
        "\n",
        "def evaluate_test(name):\n",
        "    display_stage(\"Modeling\")\n",
        "    INPUT_DIM = len(word_vocab)\n",
        "    OUTPUT_DIM = len(word_vocab)\n",
        "    SRC_PAD_IDX = 0\n",
        "    TRG_PAD_IDX = 0\n",
        "    test_loader = get_data_loader(1, \"test\")\n",
        "    test_golds = test_data.apply(lambda data: \" \".join(data[1][1:])).tolist()\n",
        "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, DEVICE).to(DEVICE)\n",
        "    display_stage(\"Evaluate Test Set\")\n",
        "    model.load_state_dict(torch.load(f\"{name}-best-model.pt\"))\n",
        "    test_predictions = inference(model, test_loader, DEVICE)\n",
        "    write_predictions(test_predictions, \"test\", name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o5sB1RIeU4qq",
      "metadata": {
        "id": "o5sB1RIeU4qq"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebde307",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "3ebde307",
        "outputId": "a4ab11a9-5638-4ca7-efb8-aa9989d227a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********************************************baseline**********************************************\n",
            "**********************************************Modeling**********************************************\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(515, 64)\n",
            "    (rnn): GRU(64, 128, bidirectional=True)\n",
            "    (fc): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (attn): Linear(in_features=384, out_features=128, bias=True)\n",
            "      (v): Linear(in_features=128, out_features=1, bias=False)\n",
            "    )\n",
            "    (embedding): Embedding(515, 64)\n",
            "    (rnn): GRU(320, 128)\n",
            "    (fc_out): Linear(in_features=448, out_features=515, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n",
            "The model has 701,251 trainable parameters\n",
            "*****************************************Training baseline*****************************************\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6250/6250 [18:03<00:00,  5.77it/s]\n",
            "100%|██████████| 1671/1671 [02:48<00:00,  9.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 20m 51s\n",
            "\tTrain Loss: 3.532 | Train PPL:  34.185\n",
            "\t Val. Loss: 3.585 |  Val. PPL:  36.071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 681/6250 [01:55<15:43,  5.90it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a93e0b357c54>\u001b[0m in \u001b[0;36m<cell line: 176>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mdisplay_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mtraining_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0;31m# evaluate_test(\"seq2seq_batch_8_enc_emb_64_hid_128_dec_emb_64_hid_128_truncate_512_vocab_512_freq_0\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-a93e0b357c54>\u001b[0m in \u001b[0;36mtraining_pipeline\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-a93e0b357c54>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-89b0d9ccde41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-89b0d9ccde41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    999\u001b[0m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m   1000\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if \"weight\" in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def training_pipeline(name=\"seq2seq\"):\n",
        "    display_stage(\"Modeling\")\n",
        "    INPUT_DIM = len(word_vocab)\n",
        "    OUTPUT_DIM = len(word_vocab)\n",
        "    SRC_PAD_IDX = 0\n",
        "    TRG_PAD_IDX = 0\n",
        "\n",
        "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "\n",
        "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, DEVICE).to(DEVICE)\n",
        "    model.apply(init_weights)\n",
        "    print(model)\n",
        "    print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
        "    # Adam optimizer\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    # Cross entropy loss\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
        "\n",
        "    display_stage(f\"Training {name}\")\n",
        "    \n",
        "    # Number of epochs\n",
        "    N_EPOCHS = 3\n",
        "    CLIP = 1\n",
        "\n",
        "    # Load the data loader\n",
        "    train_loader = get_data_loader(BATCH_SIZE, \"train\")\n",
        "    val_loader = get_data_loader(BATCH_SIZE, \"dev\")\n",
        "    test_loader = get_data_loader(1, \"test\")\n",
        "\n",
        "    dev_golds = dev_data.apply(lambda data: \" \".join(data[1][1:])).tolist()\n",
        "    test_golds = test_data.apply(lambda data: \" \".join(data[1][1:])).tolist()\n",
        "\n",
        "    best_valid_loss = float(\"inf\")\n",
        "    best_dev_predictions = list()\n",
        "    best_dev_scores = None\n",
        "    train_report = init_report()\n",
        "    valid_report = init_report()\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "        valid_loss, dev_output = evaluate(model, val_loader, criterion)\n",
        "        end_time = time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        dev_predictions = list()\n",
        "        for dev_pred in dev_output:\n",
        "            dev_pred_indexes = dev_pred.argmax(1)\n",
        "            dev_pred = [word_vocab.lookup_token(idx.item()) for idx in dev_pred_indexes]\n",
        "            dev_pred_str = \" \".join(dev_pred[1:])\n",
        "            dev_predictions.append(dev_pred_str)\n",
        "        rouge_scores = calculate_rouges(dev_predictions, dev_golds)\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            best_dev_predictions = dev_predictions\n",
        "            write_predictions(best_dev_predictions, \"dev\", name)\n",
        "            torch.save(model.state_dict(), f\"{name}-best-model.pt\")\n",
        "\n",
        "        train_ppl = math.exp(train_loss)\n",
        "        train_report[\"epoch\"].append(epoch)\n",
        "        train_report[\"loss\"].append(train_loss)\n",
        "        train_report[\"perplexity\"].append(train_ppl)\n",
        "        valid_ppl = math.exp(valid_loss)\n",
        "        valid_report[\"epoch\"].append(epoch)\n",
        "        valid_report[\"loss\"].append(valid_loss)\n",
        "        valid_report[\"perplexity\"].append(valid_ppl)\n",
        "        for k in [\"1\", \"2\", \"l\"]:\n",
        "            for m in [\"precision\", \"recall\", \"f1\"]:\n",
        "                valid_report[f\"rouge-{k}-{m}\"].append(\n",
        "                    rouge_scores[f\"rouge-{k}-{m[0]}\"]\n",
        "                )\n",
        "        print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "        print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}\")\n",
        "        print(f\"\\t Dev Loss: {valid_loss:.3f} |  Dev PPL: {valid_ppl:7.3f}\")\n",
        "\n",
        "    display_stage(\"Evaluate Test Set\")\n",
        "    model.load_state_dict(torch.load(f\"{name}-best-model.pt\"))\n",
        "    test_predictions = inference(model, test_loader, DEVICE)\n",
        "    write_predictions(test_predictions, \"test\", name)\n",
        "\n",
        "\n",
        "# train the model\n",
        "def train(model, loader, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(loader):\n",
        "        text, summary, text_len, _ = batch\n",
        "        batch_size = text.shape[0]\n",
        "        text = text.view(-1, batch_size)\n",
        "        summary = summary.view(-1, batch_size)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text, text_len, summary)\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        summary = summary[1:].view(-1)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, summary)\n",
        "        # back prop\n",
        "        loss.backward()\n",
        "        # clip gradient for avoiding gradient explosion\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)\n",
        "\n",
        "\n",
        "def tensor_to_sentences(batch_token_tensor):\n",
        "    return [\n",
        "        \" \".join(\n",
        "            [\n",
        "                word_vocab.lookup_token(token.item())\n",
        "                for token in batch_token_tensor[:, batch_idx]\n",
        "            ]\n",
        "        )\n",
        "        for batch_idx in range(batch_token_tensor.shape[1])\n",
        "    ]\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    outputs = list()\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader):\n",
        "            text, summary, text_len, _ = batch\n",
        "            batch_size = text.shape[0]\n",
        "            text = text.view(-1, batch_size)\n",
        "            summary = summary.view(-1, batch_size)\n",
        "            output = model(text, text_len, summary, 0)\n",
        "            output_dim = output.shape[-1]\n",
        "            for batch_idx in range(batch_size):\n",
        "                outputs.append(output[:, batch_idx, :])\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            summary = summary[1:].view(-1)\n",
        "            loss = criterion(output, summary)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader), outputs\n",
        "\n",
        "display_stage(NAME)\n",
        "training_pipeline(NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8239c8bc",
      "metadata": {
        "id": "8239c8bc"
      },
      "source": [
        "# Generate Rouge Scores on Test Dataset from Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50709f6e",
      "metadata": {
        "id": "50709f6e",
        "outputId": "390199e4-b3ef-4f01-eddf-d2e3ef88f82e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val {'rouge-1': {'r': 0.5053851792910492, 'p': 0.12835899534670617, 'f': 0.19760514884490066}, 'rouge-2': {'r': 0.06536187460399981, 'p': 0.023953552731043302, 'f': 0.03344606138963046}, 'rouge-l': {'r': 0.4815303212127281, 'p': 0.12205303180572001, 'f': 0.1879470677062643}}\n"
          ]
        }
      ],
      "source": [
        "!python3 ./evaluate.py test_summaries.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}