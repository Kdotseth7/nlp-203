{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4yid4cQvtqc"
      },
      "source": [
        "# Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr8nCL6C5Ct2",
        "outputId": "2b71b49a-52c7-4c07-a77d-b40ce3ff7785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m962.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.12.1+cu113)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.12.1+cu113)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11170694 sha256=c8c5942afa6b15f45826a03e980f53e212117a206a3234b5c92e1a3d4868971e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=effbff8d7788f3e36e6494784b6b6634ce19cba935bee2f6f5855c6544ea0804\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=8d2cb7f795f24de4962d0a06a1e13c072f45683838814bd62ad91f936f60fe20\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install fairseq\n",
        "!pip install sacremoses\n",
        "!pip install sacrebleu\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK-Xg1sUloaK"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF694Ii20-Id",
        "outputId": "49d9c4a2-b92a-48df-f1a4-9b522021d6c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdw3DQmnlsd5"
      },
      "source": [
        "# Set Working Directory for files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XqRyY75i2-An"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "project_folder = 'hw1'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(os.path.join(drive_path, project_folder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Dpvjkf4A0a",
        "outputId": "1541e028-c1ce-42b3-93f5-7bc69938ab15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PWD:\n",
            "/content/drive/MyDrive/hw1\n",
            "List of Files:\n",
            "checkpoints  iwslt13_fr_en\t   NLP203_Spring2023_A1.pdf  prepare_data.sh\n",
            "data\t     iwslt13_fr_en_no_bpe  output_dataset.ipynb      result\n",
            "data-bin     mosesdecoder\t   prepare_data_no_bpe.sh    subword-nmt\n"
          ]
        }
      ],
      "source": [
        "print(\"PWD:\")\n",
        "!pwd\n",
        "print(\"List of Files:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Hsds3cr6TZ"
      },
      "source": [
        "# Check if CUDA available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kN7Gyyj8qY-",
        "outputId": "d17bfb5b-9453-46e2-c4ee-372b5176cc43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdvCTHEz5IFh"
      },
      "source": [
        "# Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EOmDqUP55Jb6"
      },
      "outputs": [],
      "source": [
        "!bash prepare_data_no_bpe.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTwNvFh15WHX"
      },
      "source": [
        "# Preprocess the data using Shared Vocab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd8OOB4j5PTW",
        "outputId": "9ffdc794-55b8-432d-bf24-44987e7c8a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 01:11:36 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='fr', target_lang='en', trainpref='iwslt13_fr_en/train', validpref='iwslt13_fr_en/dev', testpref='iwslt13_fr_en/test', align_suffix=None, destdir='data-bin/iwslt13_fr_en_shared_vocab', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=True, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "2023-05-01 01:11:45 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 39176 types\n",
            "2023-05-01 01:11:57 | INFO | fairseq_cli.preprocess | [fr] iwslt13_fr_en/train.fr: 156951 sents, 3686301 tokens, 0.0% replaced (by <unk>)\n",
            "2023-05-01 01:11:57 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 39176 types\n",
            "2023-05-01 01:11:59 | INFO | fairseq_cli.preprocess | [fr] iwslt13_fr_en/dev.fr: 829 sents, 21526 tokens, 0.209% replaced (by <unk>)\n",
            "2023-05-01 01:11:59 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 39176 types\n",
            "2023-05-01 01:12:01 | INFO | fairseq_cli.preprocess | [fr] iwslt13_fr_en/test.fr: 1664 sents, 37447 tokens, 0.27% replaced (by <unk>)\n",
            "2023-05-01 01:12:01 | INFO | fairseq_cli.preprocess | [en] Dictionary: 39176 types\n",
            "2023-05-01 01:12:11 | INFO | fairseq_cli.preprocess | [en] iwslt13_fr_en/train.en: 156951 sents, 3435502 tokens, 0.0% replaced (by <unk>)\n",
            "2023-05-01 01:12:11 | INFO | fairseq_cli.preprocess | [en] Dictionary: 39176 types\n",
            "2023-05-01 01:12:13 | INFO | fairseq_cli.preprocess | [en] iwslt13_fr_en/dev.en: 829 sents, 21026 tokens, 0.0238% replaced (by <unk>)\n",
            "2023-05-01 01:12:13 | INFO | fairseq_cli.preprocess | [en] Dictionary: 39176 types\n",
            "2023-05-01 01:12:15 | INFO | fairseq_cli.preprocess | [en] iwslt13_fr_en/test.en: 1664 sents, 34926 tokens, 0.00286% replaced (by <unk>)\n",
            "2023-05-01 01:12:15 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/iwslt13_fr_en_shared_vocab\n"
          ]
        }
      ],
      "source": [
        "!TEXT=iwslt13_fr_en ; mkdir -p data-bin ; fairseq-preprocess --source-lang fr --target-lang en     --trainpref $TEXT/train --validpref $TEXT/dev --testpref $TEXT/test     --destdir data-bin/iwslt13_fr_en_shared_vocab     --workers 20 --joined-dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzihiuxcv1KW"
      },
      "source": [
        "# Train CNN using Shared Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ze6vhPW46AI",
        "outputId": "6b64a9f7-3522-490a-df93-13f663746866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 01:12:18 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.5], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/cnn_shared_vocab', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='fconv', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.5], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/cnn_shared_vocab', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/iwslt13_fr_en_shared_vocab', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, momentum=0.99, weight_decay=0.0, force_anneal=50, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, dropout=0.1, no_seed_provided=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_layers='[(512, 3)] * 20', decoder_embed_dim=512, decoder_embed_path=None, decoder_layers='[(512, 3)] * 20', decoder_out_embed_dim=256, decoder_attention='True', share_input_output_embed=False, _name='fconv'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_shared_vocab', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [0.5]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': 50, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.5]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 01:12:18 | INFO | fairseq.tasks.translation | [fr] dictionary: 39176 types\n",
            "2023-05-01 01:12:18 | INFO | fairseq.tasks.translation | [en] dictionary: 39176 types\n",
            "2023-05-01 01:12:21 | INFO | fairseq_cli.train | FConvModel(\n",
            "  (encoder): FConvEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(39176, 512, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 512, padding_idx=1)\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "      (4): None\n",
            "      (5): None\n",
            "      (6): None\n",
            "      (7): None\n",
            "      (8): None\n",
            "      (9): None\n",
            "      (10): None\n",
            "      (11): None\n",
            "      (12): None\n",
            "      (13): None\n",
            "      (14): None\n",
            "      (15): None\n",
            "      (16): None\n",
            "      (17): None\n",
            "      (18): None\n",
            "      (19): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (1): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (2): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (3): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (4): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (5): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (6): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (7): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (8): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (9): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (10): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (11): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (12): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (13): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (14): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (15): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (16): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (17): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (18): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (19): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "    )\n",
            "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): FConvDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(39176, 512, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 512, padding_idx=1)\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "      (4): None\n",
            "      (5): None\n",
            "      (6): None\n",
            "      (7): None\n",
            "      (8): None\n",
            "      (9): None\n",
            "      (10): None\n",
            "      (11): None\n",
            "      (12): None\n",
            "      (13): None\n",
            "      (14): None\n",
            "      (15): None\n",
            "      (16): None\n",
            "      (17): None\n",
            "      (18): None\n",
            "      (19): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (1): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (2): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (3): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (4): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (5): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (6): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (7): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (8): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (9): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (10): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (11): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (12): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (13): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (14): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (15): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (16): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (17): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (18): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (19): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "    )\n",
            "    (attention): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (3): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (4): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (5): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (6): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (7): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (8): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (9): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (10): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (11): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (12): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (13): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (14): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (15): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (16): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (17): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (18): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (19): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (fc3): Linear(in_features=256, out_features=39176, bias=True)\n",
            "  )\n",
            ")\n",
            "2023-05-01 01:12:21 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-05-01 01:12:21 | INFO | fairseq_cli.train | model: FConvModel\n",
            "2023-05-01 01:12:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-05-01 01:12:21 | INFO | fairseq_cli.train | num. shared model params: 125,716,496 (num. trained: 125,716,496)\n",
            "2023-05-01 01:12:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-05-01 01:12:21 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.fr\n",
            "2023-05-01 01:12:21 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.en\n",
            "2023-05-01 01:12:21 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab valid fr-en 829 examples\n",
            "2023-05-01 01:12:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-01 01:12:23 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-05-01 01:12:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-01 01:12:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-05-01 01:12:23 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-05-01 01:12:23 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/cnn_shared_vocab/checkpoint_last.pt\n",
            "2023-05-01 01:12:23 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/cnn_shared_vocab/checkpoint_last.pt\n",
            "2023-05-01 01:12:23 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-05-01 01:12:23 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en_shared_vocab/train.fr-en.fr\n",
            "2023-05-01 01:12:23 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en_shared_vocab/train.fr-en.en\n",
            "2023-05-01 01:12:23 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab train fr-en 156951 examples\n",
            "2023-05-01 01:12:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 001:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:12:23 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-05-01 01:12:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1028: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
            "epoch 001:   1% 7/1348 [00:02<04:34,  4.88it/s]2023-05-01 01:12:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   1% 9/1348 [00:02<04:02,  5.52it/s]2023-05-01 01:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   1% 10/1348 [00:02<03:41,  6.05it/s]2023-05-01 01:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:   1% 11/1348 [00:02<03:29,  6.38it/s]2023-05-01 01:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 001:   1% 13/1348 [00:03<03:37,  6.13it/s]2023-05-01 01:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "epoch 001:   1% 14/1348 [00:03<03:33,  6.26it/s]2023-05-01 01:12:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
            "epoch 001:   1% 16/1348 [00:03<03:26,  6.44it/s]2023-05-01 01:12:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
            "epoch 001: 100% 1347/1348 [03:59<00:00,  5.73it/s, loss=7.316, nll_loss=6.084, ppl=67.85, wps=13889.9, ups=5.53, wpb=2513.4, bsz=99.4, num_updates=1300, lr=0.5, gnorm=1.407, clip=100, loss_scale=1, train_wall=18, gb_free=11.3, wall=233]2023-05-01 01:16:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 1/12 [00:00<00:10,  1.02it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 2/12 [00:02<00:10,  1.05s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 3/12 [00:03<00:09,  1.01s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 4/12 [00:04<00:08,  1.01s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 5/12 [00:05<00:07,  1.13s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 6/12 [00:06<00:07,  1.28s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.38s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 8/12 [00:10<00:05,  1.46s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 9/12 [00:12<00:05,  1.68s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 10/12 [00:14<00:03,  1.97s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 11/12 [00:18<00:02,  2.54s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 12/12 [00:20<00:00,  2.30s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:16:44 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.578 | nll_loss 6.412 | ppl 85.15 | bleu 7.7 | wps 987.9 | wpb 1752.2 | bsz 69.1 | num_updates 1341\n",
            "2023-05-01 01:16:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1341 updates\n",
            "2023-05-01 01:16:44 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint1.pt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n",
            "2023-05-01 01:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint1.pt\n",
            "2023-05-01 01:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint1.pt (epoch 1 @ 1341 updates, score 7.7) (writing took 12.033050583999966 seconds)\n",
            "2023-05-01 01:16:56 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-05-01 01:16:56 | INFO | train | epoch 001 | loss 8.515 | nll_loss 7.457 | ppl 175.68 | wps 12579.6 | ups 4.94 | wpb 2548.5 | bsz 116.7 | num_updates 1341 | lr 0.5 | gnorm 0.969 | clip 100 | loss_scale 1 | train_wall 237 | gb_free 11.1 | wall 273\n",
            "2023-05-01 01:16:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 002:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:16:56 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-05-01 01:16:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1347/1348 [04:05<00:00,  5.50it/s, loss=6.145, nll_loss=4.73, ppl=26.54, wps=14003.8, ups=5.47, wpb=2562.1, bsz=122, num_updates=2600, lr=0.5, gnorm=0.362, clip=100, loss_scale=1, train_wall=18, gb_free=11.2, wall=502]2023-05-01 01:21:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 1/12 [00:01<00:12,  1.13s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 2/12 [00:02<00:11,  1.17s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 3/12 [00:03<00:10,  1.22s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 4/12 [00:04<00:10,  1.26s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 5/12 [00:06<00:09,  1.32s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 6/12 [00:07<00:08,  1.42s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 7/12 [00:09<00:07,  1.51s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 8/12 [00:11<00:06,  1.64s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 9/12 [00:14<00:05,  1.93s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 10/12 [00:17<00:04,  2.44s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 11/12 [00:21<00:02,  2.86s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 12/12 [00:27<00:00,  3.80s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:21:29 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.686 | nll_loss 5.367 | ppl 41.27 | bleu 15.48 | wps 733.4 | wpb 1752.2 | bsz 69.1 | num_updates 2689 | best_bleu 15.48\n",
            "2023-05-01 01:21:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2689 updates\n",
            "2023-05-01 01:21:29 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint2.pt\n",
            "2023-05-01 01:21:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint2.pt\n",
            "2023-05-01 01:21:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint2.pt (epoch 2 @ 2689 updates, score 15.48) (writing took 11.971643801000027 seconds)\n",
            "2023-05-01 01:21:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-05-01 01:21:41 | INFO | train | epoch 002 | loss 6.52 | nll_loss 5.161 | ppl 35.77 | wps 12050.7 | ups 4.73 | wpb 2548.6 | bsz 116.4 | num_updates 2689 | lr 0.5 | gnorm 0.527 | clip 100 | loss_scale 1 | train_wall 242 | gb_free 11.4 | wall 558\n",
            "2023-05-01 01:21:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 003:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:21:41 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-05-01 01:21:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1347/1348 [04:06<00:00,  5.64it/s, loss=5.626, nll_loss=4.126, ppl=17.46, wps=14119.4, ups=5.56, wpb=2539.4, bsz=114, num_updates=4000, lr=0.5, gnorm=0.362, clip=100, loss_scale=1, train_wall=18, gb_free=11.1, wall=797]2023-05-01 01:25:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 1/12 [00:01<00:11,  1.08s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 2/12 [00:02<00:11,  1.11s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 3/12 [00:03<00:09,  1.04s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 4/12 [00:04<00:08,  1.03s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 5/12 [00:05<00:07,  1.09s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 6/12 [00:06<00:07,  1.21s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.33s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 8/12 [00:10<00:06,  1.52s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 9/12 [00:12<00:05,  1.72s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 10/12 [00:15<00:04,  2.00s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 11/12 [00:19<00:02,  2.59s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 12/12 [00:25<00:00,  3.68s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:26:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.277 | nll_loss 4.872 | ppl 29.27 | bleu 18.36 | wps 801.1 | wpb 1752.2 | bsz 69.1 | num_updates 4037 | best_bleu 18.36\n",
            "2023-05-01 01:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4037 updates\n",
            "2023-05-01 01:26:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint3.pt\n",
            "2023-05-01 01:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint3.pt\n",
            "2023-05-01 01:26:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint3.pt (epoch 3 @ 4037 updates, score 18.36) (writing took 13.206581052000047 seconds)\n",
            "2023-05-01 01:26:26 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-05-01 01:26:26 | INFO | train | epoch 003 | loss 5.766 | nll_loss 4.285 | ppl 19.49 | wps 12063.2 | ups 4.73 | wpb 2548.6 | bsz 116.4 | num_updates 4037 | lr 0.5 | gnorm 0.36 | clip 100 | loss_scale 1 | train_wall 243 | gb_free 11.3 | wall 843\n",
            "2023-05-01 01:26:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 004:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:26:26 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-05-01 01:26:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1347/1348 [04:06<00:00,  5.57it/s, loss=5.241, nll_loss=3.679, ppl=12.81, wps=13968.8, ups=5.47, wpb=2553.1, bsz=117.9, num_updates=5300, lr=0.5, gnorm=0.325, clip=100, loss_scale=1, train_wall=18, gb_free=11.2, wall=1073]2023-05-01 01:30:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 1/12 [00:01<00:11,  1.03s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 2/12 [00:02<00:10,  1.08s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 3/12 [00:03<00:10,  1.15s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 4/12 [00:04<00:08,  1.12s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 5/12 [00:05<00:08,  1.21s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 6/12 [00:07<00:07,  1.33s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.33s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 8/12 [00:10<00:06,  1.52s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 9/12 [00:12<00:05,  1.74s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 10/12 [00:15<00:04,  2.08s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 11/12 [00:19<00:02,  2.61s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 12/12 [00:25<00:00,  3.58s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:30:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.947 | nll_loss 4.48 | ppl 22.32 | bleu 21.91 | wps 796.2 | wpb 1752.2 | bsz 69.1 | num_updates 5385 | best_bleu 21.91\n",
            "2023-05-01 01:30:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5385 updates\n",
            "2023-05-01 01:30:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint4.pt\n",
            "2023-05-01 01:31:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint4.pt\n",
            "2023-05-01 01:31:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint4.pt (epoch 4 @ 5385 updates, score 21.91) (writing took 13.110492174 seconds)\n",
            "2023-05-01 01:31:10 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-05-01 01:31:10 | INFO | train | epoch 004 | loss 5.299 | nll_loss 3.741 | ppl 13.37 | wps 12064.9 | ups 4.73 | wpb 2548.6 | bsz 116.4 | num_updates 5385 | lr 0.5 | gnorm 0.343 | clip 100 | loss_scale 1 | train_wall 243 | gb_free 11.2 | wall 1127\n",
            "2023-05-01 01:31:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 005:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:31:10 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-05-01 01:31:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1347/1348 [04:05<00:00,  5.66it/s, loss=4.977, nll_loss=3.374, ppl=10.37, wps=14169.2, ups=5.46, wpb=2595.6, bsz=115.2, num_updates=6700, lr=0.5, gnorm=0.314, clip=100, loss_scale=1, train_wall=18, gb_free=11.2, wall=1367]2023-05-01 01:35:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 1/12 [00:01<00:11,  1.08s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 2/12 [00:02<00:11,  1.11s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 3/12 [00:03<00:10,  1.14s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 4/12 [00:04<00:08,  1.09s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 5/12 [00:05<00:08,  1.23s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 6/12 [00:07<00:07,  1.25s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.40s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 8/12 [00:10<00:06,  1.55s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 9/12 [00:12<00:04,  1.62s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 10/12 [00:15<00:03,  1.96s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 11/12 [00:19<00:02,  2.53s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 12/12 [00:25<00:00,  3.63s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:35:41 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.796 | nll_loss 4.309 | ppl 19.82 | bleu 22.98 | wps 800.9 | wpb 1752.2 | bsz 69.1 | num_updates 6733 | best_bleu 22.98\n",
            "2023-05-01 01:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6733 updates\n",
            "2023-05-01 01:35:41 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint5.pt\n",
            "2023-05-01 01:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint5.pt\n",
            "2023-05-01 01:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint5.pt (epoch 5 @ 6733 updates, score 22.98) (writing took 13.33250490499995 seconds)\n",
            "2023-05-01 01:35:55 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-05-01 01:35:55 | INFO | train | epoch 005 | loss 4.974 | nll_loss 3.364 | ppl 10.29 | wps 12086.5 | ups 4.74 | wpb 2548.6 | bsz 116.4 | num_updates 6733 | lr 0.5 | gnorm 0.327 | clip 100 | loss_scale 1 | train_wall 242 | gb_free 11.3 | wall 1412\n",
            "2023-05-01 01:35:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 006:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:35:55 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-05-01 01:35:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 1347/1348 [04:05<00:00,  5.49it/s, loss=4.728, nll_loss=3.086, ppl=8.49, wps=14100.6, ups=5.5, wpb=2564.4, bsz=125.6, num_updates=8000, lr=0.5, gnorm=0.305, clip=100, loss_scale=1, train_wall=18, gb_free=11.2, wall=1642]2023-05-01 01:40:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 1/12 [00:01<00:12,  1.11s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 2/12 [00:02<00:11,  1.11s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 3/12 [00:03<00:10,  1.13s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 4/12 [00:04<00:09,  1.14s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 5/12 [00:05<00:08,  1.23s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 6/12 [00:07<00:07,  1.22s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.30s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 8/12 [00:09<00:05,  1.33s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 9/12 [00:11<00:04,  1.53s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 10/12 [00:14<00:03,  1.75s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 11/12 [00:17<00:02,  2.34s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 12/12 [00:23<00:00,  3.38s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:40:24 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.686 | nll_loss 4.145 | ppl 17.69 | bleu 24.6 | wps 858.8 | wpb 1752.2 | bsz 69.1 | num_updates 8081 | best_bleu 24.6\n",
            "2023-05-01 01:40:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8081 updates\n",
            "2023-05-01 01:40:24 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint6.pt\n",
            "2023-05-01 01:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint6.pt\n",
            "2023-05-01 01:40:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint6.pt (epoch 6 @ 8081 updates, score 24.6) (writing took 12.511590260999583 seconds)\n",
            "2023-05-01 01:40:36 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-05-01 01:40:36 | INFO | train | epoch 006 | loss 4.732 | nll_loss 3.083 | ppl 8.48 | wps 12206.4 | ups 4.79 | wpb 2548.6 | bsz 116.4 | num_updates 8081 | lr 0.5 | gnorm 0.315 | clip 100 | loss_scale 1 | train_wall 242 | gb_free 11 | wall 1693\n",
            "2023-05-01 01:40:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 007:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:40:36 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-05-01 01:40:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 1347/1348 [04:05<00:00,  5.77it/s, loss=4.559, nll_loss=2.888, ppl=7.4, wps=14176.6, ups=5.48, wpb=2585.4, bsz=120.9, num_updates=9400, lr=0.5, gnorm=0.297, clip=100, loss_scale=1, train_wall=18, gb_free=11.5, wall=1934]2023-05-01 01:44:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 2/12 [00:01<00:08,  1.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.09it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.04it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 5/12 [00:04<00:07,  1.03s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 6/12 [00:06<00:06,  1.15s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 7/12 [00:07<00:06,  1.24s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 8/12 [00:09<00:05,  1.37s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 9/12 [00:11<00:04,  1.61s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 10/12 [00:13<00:03,  1.72s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 11/12 [00:17<00:02,  2.30s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 12/12 [00:23<00:00,  3.46s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:45:05 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.574 | nll_loss 4.035 | ppl 16.39 | bleu 25.57 | wps 867 | wpb 1752.2 | bsz 69.1 | num_updates 9429 | best_bleu 25.57\n",
            "2023-05-01 01:45:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 9429 updates\n",
            "2023-05-01 01:45:05 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint7.pt\n",
            "2023-05-01 01:45:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint7.pt\n",
            "2023-05-01 01:45:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint7.pt (epoch 7 @ 9429 updates, score 25.57) (writing took 13.84003957799996 seconds)\n",
            "2023-05-01 01:45:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-05-01 01:45:19 | INFO | train | epoch 007 | loss 4.548 | nll_loss 2.869 | ppl 7.31 | wps 12138.9 | ups 4.76 | wpb 2548.6 | bsz 116.4 | num_updates 9429 | lr 0.5 | gnorm 0.309 | clip 100 | loss_scale 1 | train_wall 243 | gb_free 11 | wall 1976\n",
            "2023-05-01 01:45:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 008:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:45:19 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-05-01 01:45:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 1347/1348 [04:05<00:00,  5.42it/s, loss=4.481, nll_loss=2.795, ppl=6.94, wps=14136, ups=5.61, wpb=2520.7, bsz=102.6, num_updates=10700, lr=0.5, gnorm=0.307, clip=100, loss_scale=1, train_wall=18, gb_free=11.2, wall=2208]2023-05-01 01:49:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 1/12 [00:00<00:08,  1.24it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 2/12 [00:01<00:08,  1.17it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.09it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 4/12 [00:03<00:08,  1.02s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 5/12 [00:05<00:08,  1.20s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 6/12 [00:06<00:07,  1.24s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.31s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 8/12 [00:10<00:05,  1.49s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 9/12 [00:12<00:05,  1.71s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 10/12 [00:14<00:03,  1.98s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 11/12 [00:18<00:02,  2.55s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 12/12 [00:21<00:00,  2.70s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:49:46 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.495 | nll_loss 3.952 | ppl 15.47 | bleu 26.22 | wps 925.7 | wpb 1752.2 | bsz 69.1 | num_updates 10777 | best_bleu 26.22\n",
            "2023-05-01 01:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10777 updates\n",
            "2023-05-01 01:49:46 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint8.pt\n",
            "2023-05-01 01:49:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint8.pt\n",
            "2023-05-01 01:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint8.pt (epoch 8 @ 10777 updates, score 26.22) (writing took 12.530979473000116 seconds)\n",
            "2023-05-01 01:49:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-05-01 01:49:59 | INFO | train | epoch 008 | loss 4.392 | nll_loss 2.689 | ppl 6.45 | wps 12271.5 | ups 4.82 | wpb 2548.6 | bsz 116.4 | num_updates 10777 | lr 0.5 | gnorm 0.299 | clip 100 | loss_scale 1 | train_wall 242 | gb_free 11.1 | wall 2256\n",
            "2023-05-01 01:49:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 009:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:49:59 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-05-01 01:49:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 1347/1348 [04:05<00:00,  5.83it/s, loss=4.293, nll_loss=2.575, ppl=5.96, wps=13943.6, ups=5.49, wpb=2539.1, bsz=113.9, num_updates=12100, lr=0.5, gnorm=0.312, clip=100, loss_scale=1, train_wall=18, gb_free=11.1, wall=2497]2023-05-01 01:54:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.13it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 2/12 [00:01<00:09,  1.07it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.12it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.03it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 5/12 [00:05<00:07,  1.12s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 6/12 [00:06<00:07,  1.17s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 7/12 [00:07<00:06,  1.22s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 8/12 [00:09<00:05,  1.44s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 9/12 [00:11<00:04,  1.66s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 10/12 [00:13<00:03,  1.82s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 11/12 [00:17<00:02,  2.45s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 12/12 [00:23<00:00,  3.45s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:54:28 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.469 | nll_loss 3.914 | ppl 15.08 | bleu 26.84 | wps 850.5 | wpb 1752.2 | bsz 69.1 | num_updates 12125 | best_bleu 26.84\n",
            "2023-05-01 01:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12125 updates\n",
            "2023-05-01 01:54:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint9.pt\n",
            "2023-05-01 01:54:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint9.pt\n",
            "2023-05-01 01:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint9.pt (epoch 9 @ 12125 updates, score 26.84) (writing took 14.28943759699996 seconds)\n",
            "2023-05-01 01:54:43 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-05-01 01:54:43 | INFO | train | epoch 009 | loss 4.256 | nll_loss 2.531 | ppl 5.78 | wps 12117.9 | ups 4.75 | wpb 2548.6 | bsz 116.4 | num_updates 12125 | lr 0.5 | gnorm 0.296 | clip 100 | loss_scale 1 | train_wall 242 | gb_free 11.2 | wall 2540\n",
            "2023-05-01 01:54:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 010:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 01:54:43 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-05-01 01:54:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 1347/1348 [04:05<00:00,  5.53it/s, loss=4.191, nll_loss=2.455, ppl=5.48, wps=14119.8, ups=5.58, wpb=2530.1, bsz=115.8, num_updates=13400, lr=0.5, gnorm=0.292, clip=100, loss_scale=1, train_wall=18, gb_free=11.1, wall=2773]2023-05-01 01:58:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.15it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 2/12 [00:01<00:08,  1.15it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 3/12 [00:02<00:07,  1.14it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.06it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 5/12 [00:04<00:07,  1.06s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 6/12 [00:06<00:07,  1.26s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.33s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 8/12 [00:09<00:05,  1.37s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 9/12 [00:11<00:04,  1.49s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 10/12 [00:13<00:03,  1.81s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 11/12 [00:17<00:02,  2.39s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 12/12 [00:23<00:00,  3.51s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 01:59:12 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.449 | nll_loss 3.885 | ppl 14.78 | bleu 27.49 | wps 851.6 | wpb 1752.2 | bsz 69.1 | num_updates 13473 | best_bleu 27.49\n",
            "2023-05-01 01:59:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 13473 updates\n",
            "2023-05-01 01:59:12 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint10.pt\n",
            "2023-05-01 01:59:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_shared_vocab/checkpoint10.pt\n",
            "2023-05-01 01:59:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_shared_vocab/checkpoint10.pt (epoch 10 @ 13473 updates, score 27.49) (writing took 13.797192893000101 seconds)\n",
            "2023-05-01 01:59:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-05-01 01:59:26 | INFO | train | epoch 010 | loss 4.142 | nll_loss 2.399 | ppl 5.28 | wps 12118.9 | ups 4.76 | wpb 2548.6 | bsz 116.4 | num_updates 13473 | lr 0.5 | gnorm 0.29 | clip 100 | loss_scale 1 | train_wall 243 | gb_free 11.2 | wall 2823\n",
            "2023-05-01 01:59:26 | INFO | fairseq_cli.train | done training in 2822.9 seconds\n"
          ]
        }
      ],
      "source": [
        "!fairseq-train data-bin/iwslt13_fr_en_shared_vocab --arch fconv \\\n",
        "    --dropout 0.1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --optimizer nag --clip-norm 0.1 \\\n",
        "    --lr 0.5 --lr-scheduler fixed --force-anneal 50 \\\n",
        "    --max-tokens 3000 --eval-bleu --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints/cnn_shared_vocab \\\n",
        "    --fp16 --patience 10 \\\n",
        "    --max-epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQUH-rDv5RU"
      },
      "source": [
        "# Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEIEY7W-qo5k",
        "outputId": "77bc61bc-a322-481f-e857-27d2779aef03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 01:59:42 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cnn_shared_vocab/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/cnn_shared_vocab.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_shared_vocab', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 01:59:42 | INFO | fairseq.tasks.translation | [fr] dictionary: 39176 types\n",
            "2023-05-01 01:59:42 | INFO | fairseq.tasks.translation | [en] dictionary: 39176 types\n",
            "2023-05-01 01:59:42 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cnn_shared_vocab/checkpoint_best.pt\n",
            "2023-05-01 01:59:45 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.fr\n",
            "2023-05-01 01:59:45 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.en\n",
            "2023-05-01 01:59:45 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab valid fr-en 829 examples\n",
            "2023-05-01 02:00:16 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-05-01 02:00:16 | INFO | fairseq_cli.generate | Translated 829 sentences (19,971 tokens) in 20.8s (39.90 sentences/s, 961.15 tokens/s)\n",
            "2023-05-01 02:00:16 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-05-01 02:00:16 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-05-01 02:00:16 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_shared_vocab \\\n",
        "    --path checkpoints/cnn_shared_vocab/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 --remove-bpe  \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/cnn_shared_vocab.pred --gen-subset valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBioU9aMv9rz"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma3BlxvnsFtu",
        "outputId": "a8a61af5-7026-4ec1-ff83-73935848febe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 02:00:25 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cnn_shared_vocab/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/cnn_shared_vocab.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_shared_vocab', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 02:00:25 | INFO | fairseq.tasks.translation | [fr] dictionary: 39176 types\n",
            "2023-05-01 02:00:25 | INFO | fairseq.tasks.translation | [en] dictionary: 39176 types\n",
            "2023-05-01 02:00:25 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cnn_shared_vocab/checkpoint_best.pt\n",
            "2023-05-01 02:00:28 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_shared_vocab/test.fr-en.fr\n",
            "2023-05-01 02:00:28 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_shared_vocab/test.fr-en.en\n",
            "2023-05-01 02:00:28 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab test fr-en 1664 examples\n",
            "2023-05-01 02:00:57 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-05-01 02:00:57 | INFO | fairseq_cli.generate | Translated 1,664 sentences (35,058 tokens) in 17.6s (94.79 sentences/s, 1997.01 tokens/s)\n",
            "2023-05-01 02:00:57 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-05-01 02:00:57 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-05-01 02:00:57 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_shared_vocab \\\n",
        "    --path checkpoints/cnn_shared_vocab/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --remove-bpe  \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/cnn_shared_vocab.pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyrV8q0-wBNF"
      },
      "source": [
        "# Train Transformer using Shared Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM29Uqpct9u8",
        "outputId": "c0a50372-0b10-470e-ef93-f52f6188f1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 02:01:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/tf_shared_vocab', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/tf_shared_vocab', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/iwslt13_fr_en_shared_vocab', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_shared_vocab', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 02:01:06 | INFO | fairseq.tasks.translation | [fr] dictionary: 39176 types\n",
            "2023-05-01 02:01:06 | INFO | fairseq.tasks.translation | [en] dictionary: 39176 types\n",
            "2023-05-01 02:01:07 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(39176, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(39176, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=39176, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-05-01 02:01:07 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-05-01 02:01:07 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-05-01 02:01:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-05-01 02:01:07 | INFO | fairseq_cli.train | num. shared model params: 103,156,736 (num. trained: 103,156,736)\n",
            "2023-05-01 02:01:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-05-01 02:01:07 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.fr\n",
            "2023-05-01 02:01:07 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.en\n",
            "2023-05-01 02:01:07 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab valid fr-en 829 examples\n",
            "2023-05-01 02:01:09 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2023-05-01 02:01:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-01 02:01:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-05-01 02:01:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-05-01 02:01:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-05-01 02:01:09 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-05-01 02:01:09 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/tf_shared_vocab/checkpoint_last.pt\n",
            "2023-05-01 02:01:09 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/tf_shared_vocab/checkpoint_last.pt\n",
            "2023-05-01 02:01:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-05-01 02:01:09 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en_shared_vocab/train.fr-en.fr\n",
            "2023-05-01 02:01:09 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en_shared_vocab/train.fr-en.en\n",
            "2023-05-01 02:01:09 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab train fr-en 156951 examples\n",
            "2023-05-01 02:01:09 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-05-01 02:01:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 001:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:01:09 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-05-01 02:01:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1347/1348 [08:39<00:00,  2.73it/s, loss=8.682, nll_loss=7.653, ppl=201.22, wps=6568.1, ups=2.61, wpb=2515.8, bsz=100, num_updates=1300, lr=0.0001625, gnorm=1.576, train_wall=38, gb_free=11.6, wall=502]2023-05-01 02:09:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 1/12 [00:01<00:14,  1.32s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 2/12 [00:02<00:12,  1.23s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 3/12 [00:03<00:10,  1.19s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 4/12 [00:04<00:09,  1.19s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 5/12 [00:06<00:08,  1.20s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 6/12 [00:07<00:07,  1.27s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.30s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 8/12 [00:10<00:05,  1.34s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 9/12 [00:11<00:04,  1.41s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 10/12 [00:13<00:03,  1.50s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 11/12 [00:15<00:01,  1.66s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 12/12 [00:18<00:00,  2.05s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 02:10:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.664 | nll_loss 7.617 | ppl 196.28 | bleu 1.52 | wps 1127.2 | wpb 1752.2 | bsz 69.1 | num_updates 1348\n",
            "2023-05-01 02:10:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1348 updates\n",
            "2023-05-01 02:10:07 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint1.pt\n",
            "2023-05-01 02:10:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint1.pt\n",
            "2023-05-01 02:10:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint1.pt (epoch 1 @ 1348 updates, score 1.52) (writing took 16.976201598999978 seconds)\n",
            "2023-05-01 02:10:24 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-05-01 02:10:24 | INFO | train | epoch 001 | loss 9.975 | nll_loss 9.151 | ppl 568.68 | wps 6200.5 | ups 2.43 | wpb 2548.6 | bsz 116.4 | num_updates 1348 | lr 0.0001685 | gnorm 2.119 | train_wall 516 | gb_free 11.3 | wall 555\n",
            "2023-05-01 02:10:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 002:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:10:24 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-05-01 02:10:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1347/1348 [08:36<00:00,  2.62it/s, loss=7.695, nll_loss=6.536, ppl=92.82, wps=6671.8, ups=2.6, wpb=2568.7, bsz=121.8, num_updates=2600, lr=0.000325, gnorm=1.291, train_wall=38, gb_free=11.7, wall=1036]2023-05-01 02:19:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 1/12 [00:00<00:10,  1.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 2/12 [00:02<00:10,  1.03s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 3/12 [00:03<00:09,  1.05s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 4/12 [00:04<00:09,  1.13s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 5/12 [00:05<00:08,  1.19s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 6/12 [00:07<00:07,  1.30s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 7/12 [00:08<00:06,  1.35s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 8/12 [00:10<00:05,  1.43s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 9/12 [00:11<00:04,  1.52s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 10/12 [00:13<00:03,  1.62s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 11/12 [00:15<00:01,  1.77s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 12/12 [00:18<00:00,  2.07s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 02:19:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.931 | nll_loss 6.758 | ppl 108.24 | bleu 2.82 | wps 1093.9 | wpb 1752.2 | bsz 69.1 | num_updates 2696 | best_bleu 2.82\n",
            "2023-05-01 02:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2696 updates\n",
            "2023-05-01 02:19:20 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint2.pt\n",
            "2023-05-01 02:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint2.pt\n",
            "2023-05-01 02:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint2.pt (epoch 2 @ 2696 updates, score 2.82) (writing took 16.80754229499962 seconds)\n",
            "2023-05-01 02:19:37 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-05-01 02:19:37 | INFO | train | epoch 002 | loss 8.048 | nll_loss 6.934 | ppl 122.29 | wps 6215.4 | ups 2.44 | wpb 2548.6 | bsz 116.4 | num_updates 2696 | lr 0.000337 | gnorm 1.429 | train_wall 514 | gb_free 11.7 | wall 1108\n",
            "2023-05-01 02:19:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 003:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:19:37 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-05-01 02:19:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1347/1348 [08:34<00:00,  2.59it/s, loss=7.118, nll_loss=5.88, ppl=58.88, wps=6646.4, ups=2.64, wpb=2517.6, bsz=113, num_updates=4000, lr=0.0005, gnorm=1.373, train_wall=38, gb_free=11.6, wall=1606]2023-05-01 02:28:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 1/12 [00:01<00:12,  1.11s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 2/12 [00:02<00:10,  1.06s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 3/12 [00:03<00:09,  1.05s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 4/12 [00:04<00:08,  1.07s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 5/12 [00:05<00:07,  1.08s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 6/12 [00:06<00:06,  1.13s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  58% 7/12 [00:07<00:05,  1.15s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 8/12 [00:09<00:04,  1.18s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 9/12 [00:10<00:03,  1.23s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 10/12 [00:11<00:02,  1.34s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 11/12 [00:13<00:01,  1.48s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 12/12 [00:14<00:00,  1.29s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 02:28:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.308 | nll_loss 6.033 | ppl 65.5 | bleu 4.93 | wps 1430.7 | wpb 1752.2 | bsz 69.1 | num_updates 4044 | best_bleu 4.93\n",
            "2023-05-01 02:28:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4044 updates\n",
            "2023-05-01 02:28:27 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint3.pt\n",
            "2023-05-01 02:28:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint3.pt\n",
            "2023-05-01 02:28:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint3.pt (epoch 3 @ 4044 updates, score 4.93) (writing took 14.767064672999368 seconds)\n",
            "2023-05-01 02:28:41 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-05-01 02:28:41 | INFO | train | epoch 003 | loss 7.345 | nll_loss 6.139 | ppl 70.46 | wps 6310.1 | ups 2.48 | wpb 2548.6 | bsz 116.4 | num_updates 4044 | lr 0.000497272 | gnorm 1.31 | train_wall 511 | gb_free 11.5 | wall 1652\n",
            "2023-05-01 02:28:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 004:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:28:41 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-05-01 02:28:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1347/1348 [08:33<00:00,  2.60it/s, loss=6.153, nll_loss=4.786, ppl=27.58, wps=6693.2, ups=2.63, wpb=2548, bsz=118.2, num_updates=5300, lr=0.000434372, gnorm=1.303, train_wall=38, gb_free=11.7, wall=2131]2023-05-01 02:37:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 1/12 [00:01<00:11,  1.05s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 2/12 [00:02<00:10,  1.03s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 3/12 [00:03<00:09,  1.02s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 4/12 [00:04<00:08,  1.05s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 5/12 [00:05<00:07,  1.06s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 6/12 [00:06<00:06,  1.09s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  58% 7/12 [00:07<00:05,  1.14s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 8/12 [00:08<00:04,  1.17s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 9/12 [00:10<00:03,  1.27s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 10/12 [00:11<00:02,  1.36s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 11/12 [00:13<00:01,  1.36s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 12/12 [00:14<00:00,  1.25s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 02:37:30 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.568 | nll_loss 5.131 | ppl 35.04 | bleu 12.71 | wps 1462 | wpb 1752.2 | bsz 69.1 | num_updates 5392 | best_bleu 12.71\n",
            "2023-05-01 02:37:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5392 updates\n",
            "2023-05-01 02:37:30 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint4.pt\n",
            "2023-05-01 02:37:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint4.pt\n",
            "2023-05-01 02:37:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint4.pt (epoch 4 @ 5392 updates, score 12.71) (writing took 15.73196384899984 seconds)\n",
            "2023-05-01 02:37:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-05-01 02:37:46 | INFO | train | epoch 004 | loss 6.485 | nll_loss 5.163 | ppl 35.82 | wps 6311.7 | ups 2.48 | wpb 2548.6 | bsz 116.4 | num_updates 5392 | lr 0.000430651 | gnorm 1.368 | train_wall 511 | gb_free 11.4 | wall 2197\n",
            "2023-05-01 02:37:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 005:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:37:46 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-05-01 02:37:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1347/1348 [08:33<00:00,  2.64it/s, loss=5.616, nll_loss=4.175, ppl=18.06, wps=6771.2, ups=2.6, wpb=2600.7, bsz=112.6, num_updates=6700, lr=0.000386334, gnorm=1.303, train_wall=38, gb_free=11.6, wall=2695]2023-05-01 02:46:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 2/12 [00:01<00:08,  1.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 3/12 [00:02<00:07,  1.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.11it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 5/12 [00:04<00:06,  1.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 6/12 [00:05<00:05,  1.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  58% 7/12 [00:06<00:05,  1.00s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 8/12 [00:07<00:04,  1.04s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 9/12 [00:08<00:03,  1.08s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 10/12 [00:10<00:02,  1.15s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 11/12 [00:11<00:01,  1.29s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 12/12 [00:14<00:00,  1.68s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 02:46:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.097 | nll_loss 4.615 | ppl 24.51 | bleu 15.96 | wps 1427.6 | wpb 1752.2 | bsz 69.1 | num_updates 6740 | best_bleu 15.96\n",
            "2023-05-01 02:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6740 updates\n",
            "2023-05-01 02:46:34 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint5.pt\n",
            "2023-05-01 02:46:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint5.pt\n",
            "2023-05-01 02:46:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint5.pt (epoch 5 @ 6740 updates, score 15.96) (writing took 15.488442073000442 seconds)\n",
            "2023-05-01 02:46:49 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-05-01 02:46:49 | INFO | train | epoch 005 | loss 5.76 | nll_loss 4.339 | ppl 20.24 | wps 6318.5 | ups 2.48 | wpb 2548.6 | bsz 116.4 | num_updates 6740 | lr 0.000385186 | gnorm 1.35 | train_wall 510 | gb_free 11.6 | wall 2740\n",
            "2023-05-01 02:46:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 006:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:46:49 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-05-01 02:46:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 1347/1348 [08:33<00:00,  2.71it/s, loss=5.209, nll_loss=3.713, ppl=13.11, wps=6724.4, ups=2.61, wpb=2572.7, bsz=123.6, num_updates=8000, lr=0.000353553, gnorm=1.325, train_wall=38, gb_free=11.3, wall=3221]2023-05-01 02:55:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 1/12 [00:01<00:11,  1.04s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 2/12 [00:01<00:09,  1.01it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.04it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.04it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 5/12 [00:04<00:06,  1.00it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 6/12 [00:06<00:06,  1.03s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  58% 7/12 [00:07<00:05,  1.05s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 8/12 [00:08<00:04,  1.10s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 9/12 [00:09<00:03,  1.12s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 10/12 [00:10<00:02,  1.21s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 11/12 [00:12<00:01,  1.30s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 12/12 [00:15<00:00,  1.73s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 02:55:38 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.769 | nll_loss 4.226 | ppl 18.72 | bleu 19.64 | wps 1371.8 | wpb 1752.2 | bsz 69.1 | num_updates 8088 | best_bleu 19.64\n",
            "2023-05-01 02:55:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8088 updates\n",
            "2023-05-01 02:55:38 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint6.pt\n",
            "2023-05-01 02:55:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint6.pt\n",
            "2023-05-01 02:55:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint6.pt (epoch 6 @ 8088 updates, score 19.64) (writing took 16.999353826000515 seconds)\n",
            "2023-05-01 02:55:55 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-05-01 02:55:55 | INFO | train | epoch 006 | loss 5.298 | nll_loss 3.815 | ppl 14.08 | wps 6294.8 | ups 2.47 | wpb 2548.6 | bsz 116.4 | num_updates 8088 | lr 0.000351625 | gnorm 1.29 | train_wall 510 | gb_free 11.3 | wall 3286\n",
            "2023-05-01 02:55:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 007:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 02:55:55 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-05-01 02:55:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 1347/1348 [08:32<00:00,  2.70it/s, loss=4.951, nll_loss=3.422, ppl=10.72, wps=6759.9, ups=2.61, wpb=2586.2, bsz=121.8, num_updates=9400, lr=0.000326164, gnorm=1.276, train_wall=38, gb_free=11.7, wall=3786]2023-05-01 03:04:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 2/12 [00:01<00:09,  1.10it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.10it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.05it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 5/12 [00:04<00:06,  1.06it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 6/12 [00:05<00:05,  1.01it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  58% 7/12 [00:06<00:05,  1.02s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 8/12 [00:07<00:04,  1.04s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 9/12 [00:09<00:03,  1.11s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 10/12 [00:10<00:02,  1.14s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 11/12 [00:11<00:01,  1.16s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 12/12 [00:12<00:00,  1.16s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 03:04:41 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.605 | nll_loss 4.061 | ppl 16.69 | bleu 20.41 | wps 1633.4 | wpb 1752.2 | bsz 69.1 | num_updates 9436 | best_bleu 20.41\n",
            "2023-05-01 03:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 9436 updates\n",
            "2023-05-01 03:04:41 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint7.pt\n",
            "2023-05-01 03:04:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint7.pt\n",
            "2023-05-01 03:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint7.pt (epoch 7 @ 9436 updates, score 20.41) (writing took 14.330551791999824 seconds)\n",
            "2023-05-01 03:04:56 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-05-01 03:04:56 | INFO | train | epoch 007 | loss 4.992 | nll_loss 3.467 | ppl 11.06 | wps 6356.8 | ups 2.49 | wpb 2548.6 | bsz 116.4 | num_updates 9436 | lr 0.000325541 | gnorm 1.244 | train_wall 510 | gb_free 11.3 | wall 3827\n",
            "2023-05-01 03:04:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 008:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 03:04:56 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-05-01 03:04:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 1347/1348 [08:33<00:00,  2.55it/s, loss=4.814, nll_loss=3.267, ppl=9.62, wps=6697.7, ups=2.63, wpb=2542.6, bsz=108.4, num_updates=10700, lr=0.000305709, gnorm=1.209, train_wall=38, gb_free=11.6, wall=4308]2023-05-01 03:13:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 1/12 [00:00<00:10,  1.02it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 2/12 [00:01<00:09,  1.05it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.09it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.07it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 5/12 [00:04<00:06,  1.05it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 6/12 [00:05<00:06,  1.01s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  58% 7/12 [00:06<00:05,  1.05s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 8/12 [00:08<00:04,  1.11s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 9/12 [00:09<00:03,  1.20s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 10/12 [00:11<00:02,  1.27s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 11/12 [00:12<00:01,  1.40s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 12/12 [00:14<00:00,  1.57s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 03:13:44 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.437 | nll_loss 3.86 | ppl 14.52 | bleu 23.79 | wps 1407.1 | wpb 1752.2 | bsz 69.1 | num_updates 10784 | best_bleu 23.79\n",
            "2023-05-01 03:13:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10784 updates\n",
            "2023-05-01 03:13:44 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint8.pt\n",
            "2023-05-01 03:13:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint8.pt\n",
            "2023-05-01 03:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint8.pt (epoch 8 @ 10784 updates, score 23.79) (writing took 14.875907030000235 seconds)\n",
            "2023-05-01 03:13:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-05-01 03:13:59 | INFO | train | epoch 008 | loss 4.782 | nll_loss 3.229 | ppl 9.38 | wps 6324.8 | ups 2.48 | wpb 2548.6 | bsz 116.4 | num_updates 10784 | lr 0.000304516 | gnorm 1.216 | train_wall 510 | gb_free 11.4 | wall 4370\n",
            "2023-05-01 03:13:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 009:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 03:13:59 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-05-01 03:13:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 1347/1348 [08:32<00:00,  2.80it/s, loss=4.618, nll_loss=3.046, ppl=8.26, wps=6624.9, ups=2.64, wpb=2513.7, bsz=111.6, num_updates=12100, lr=0.00028748, gnorm=1.249, train_wall=38, gb_free=11.4, wall=4871]2023-05-01 03:22:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.15it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 2/12 [00:01<00:09,  1.10it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 3/12 [00:02<00:07,  1.13it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.11it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 5/12 [00:04<00:06,  1.11it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 6/12 [00:05<00:05,  1.04it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 7/12 [00:06<00:05,  1.01s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 8/12 [00:07<00:04,  1.04s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 9/12 [00:09<00:03,  1.11s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 10/12 [00:10<00:02,  1.23s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 11/12 [00:12<00:01,  1.47s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 12/12 [00:14<00:00,  1.59s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 03:22:47 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.372 | nll_loss 3.798 | ppl 13.91 | bleu 23.98 | wps 1425 | wpb 1752.2 | bsz 69.1 | num_updates 12132 | best_bleu 23.98\n",
            "2023-05-01 03:22:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12132 updates\n",
            "2023-05-01 03:22:47 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint9.pt\n",
            "2023-05-01 03:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint9.pt\n",
            "2023-05-01 03:23:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint9.pt (epoch 9 @ 12132 updates, score 23.98) (writing took 15.02684144900013 seconds)\n",
            "2023-05-01 03:23:02 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-05-01 03:23:02 | INFO | train | epoch 009 | loss 4.618 | nll_loss 3.044 | ppl 8.25 | wps 6329 | ups 2.48 | wpb 2548.6 | bsz 116.4 | num_updates 12132 | lr 0.0002871 | gnorm 1.204 | train_wall 510 | gb_free 11.4 | wall 4913\n",
            "2023-05-01 03:23:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 010:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 03:23:02 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-05-01 03:23:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 1347/1348 [08:33<00:00,  2.66it/s, loss=4.46, nll_loss=2.866, ppl=7.29, wps=6710.9, ups=2.65, wpb=2536.6, bsz=122.6, num_updates=13400, lr=0.000273179, gnorm=1.222, train_wall=38, gb_free=11.7, wall=5396]2023-05-01 03:31:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 1/12 [00:00<00:10,  1.08it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 2/12 [00:01<00:09,  1.08it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 3/12 [00:02<00:08,  1.03it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 4/12 [00:03<00:07,  1.02it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 5/12 [00:04<00:06,  1.01it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 6/12 [00:05<00:06,  1.02s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  58% 7/12 [00:07<00:05,  1.04s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 8/12 [00:08<00:04,  1.05s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 9/12 [00:09<00:03,  1.11s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 10/12 [00:10<00:02,  1.19s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 11/12 [00:12<00:01,  1.30s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 12/12 [00:15<00:00,  1.80s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 03:31:50 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.284 | nll_loss 3.689 | ppl 12.89 | bleu 25.98 | wps 1354.7 | wpb 1752.2 | bsz 69.1 | num_updates 13480 | best_bleu 25.98\n",
            "2023-05-01 03:31:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 13480 updates\n",
            "2023-05-01 03:31:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint10.pt\n",
            "2023-05-01 03:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_shared_vocab/checkpoint10.pt\n",
            "2023-05-01 03:32:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_shared_vocab/checkpoint10.pt (epoch 10 @ 13480 updates, score 25.98) (writing took 15.406787277999683 seconds)\n",
            "2023-05-01 03:32:06 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-05-01 03:32:06 | INFO | train | epoch 010 | loss 4.492 | nll_loss 2.902 | ppl 7.48 | wps 6314.1 | ups 2.48 | wpb 2548.6 | bsz 116.4 | num_updates 13480 | lr 0.000272367 | gnorm 1.2 | train_wall 510 | gb_free 11.5 | wall 5457\n",
            "2023-05-01 03:32:06 | INFO | fairseq_cli.train | done training in 5456.5 seconds\n"
          ]
        }
      ],
      "source": [
        "!fairseq-train data-bin/iwslt13_fr_en_shared_vocab --arch transformer \\\n",
        "    --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 3000 --eval-bleu --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints/tf_shared_vocab --patience 10 \\\n",
        "    --max-epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB3CoA_4sB_U"
      },
      "source": [
        "# Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR5oIqLP8ZN8",
        "outputId": "1c8a4dab-8c10-4e87-b7dd-65983719b95c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 03:32:22 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/tf_shared_vocab/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/tf_shared_vocab.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_shared_vocab', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 03:32:22 | INFO | fairseq.tasks.translation | [fr] dictionary: 39176 types\n",
            "2023-05-01 03:32:22 | INFO | fairseq.tasks.translation | [en] dictionary: 39176 types\n",
            "2023-05-01 03:32:22 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/tf_shared_vocab/checkpoint_best.pt\n",
            "2023-05-01 03:32:25 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.fr\n",
            "2023-05-01 03:32:25 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en_shared_vocab/valid.fr-en.en\n",
            "2023-05-01 03:32:25 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab valid fr-en 829 examples\n",
            "2023-05-01 03:32:46 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-05-01 03:32:46 | INFO | fairseq_cli.generate | Translated 829 sentences (19,861 tokens) in 11.4s (72.55 sentences/s, 1738.20 tokens/s)\n",
            "2023-05-01 03:32:46 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-05-01 03:32:46 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-05-01 03:32:46 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_shared_vocab \\\n",
        "    --path checkpoints/tf_shared_vocab/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 --remove-bpe  \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/tf_shared_vocab.pred --gen-subset valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CS-19lksLOt"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp3Q7sH9sNYa",
        "outputId": "2dacdb88-9291-42b1-8599-0d9c37fd216c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 03:32:54 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/tf_shared_vocab/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/tf_shared_vocab.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_shared_vocab', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 03:32:55 | INFO | fairseq.tasks.translation | [fr] dictionary: 39176 types\n",
            "2023-05-01 03:32:55 | INFO | fairseq.tasks.translation | [en] dictionary: 39176 types\n",
            "2023-05-01 03:32:55 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/tf_shared_vocab/checkpoint_best.pt\n",
            "2023-05-01 03:32:57 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_shared_vocab/test.fr-en.fr\n",
            "2023-05-01 03:32:58 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_shared_vocab/test.fr-en.en\n",
            "2023-05-01 03:32:58 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_shared_vocab test fr-en 1664 examples\n",
            "2023-05-01 03:33:24 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-05-01 03:33:24 | INFO | fairseq_cli.generate | Translated 1,664 sentences (34,166 tokens) in 15.6s (106.81 sentences/s, 2193.14 tokens/s)\n",
            "2023-05-01 03:33:25 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-05-01 03:33:25 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-05-01 03:33:25 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_shared_vocab \\\n",
        "    --path checkpoints/tf_shared_vocab/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 --remove-bpe  \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/tf_shared_vocab.pred"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
