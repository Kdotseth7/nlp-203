{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4yid4cQvtqc"
      },
      "source": [
        "# Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr8nCL6C5Ct2",
        "outputId": "6ca6c7b7-8cec-4435-cb7e-b32f0751bda2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m960.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.12)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.7/272.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11170729 sha256=7090c966902dc00a732ce55a1feb8da38e51609e7d45c0945e7e478b26306c50\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=312155cf5ffb2e318c48a7a0817532794053c039a48a9833cc62dd6c9399cd88\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.7.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=867af08e2107ed0fdc19e82985b698bca38a5759bb5900818daaecc6fa379272\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.53\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install fairseq\n",
        "!pip install sacremoses\n",
        "!pip install sacrebleu\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK-Xg1sUloaK"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF694Ii20-Id",
        "outputId": "01b7ee91-4e39-45ed-c109-fef71ab7b096"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdw3DQmnlsd5"
      },
      "source": [
        "# Set Working Directory for files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XqRyY75i2-An"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "project_folder = 'hw1'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(os.path.join(drive_path, project_folder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Dpvjkf4A0a",
        "outputId": "efbda73a-bab8-4dfd-fd5c-46c89ff1f2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PWD:\n",
            "/content/drive/MyDrive/hw1\n",
            "List of Files:\n",
            "checkpoints  iwslt13_fr_en\t   NLP203_Spring2023_A1.pdf  prepare_data.sh\n",
            "data\t     iwslt13_fr_en_no_bpe  output_dataset.ipynb      result\n",
            "data-bin     mosesdecoder\t   prepare_data_no_bpe.sh    subword-nmt\n"
          ]
        }
      ],
      "source": [
        "print(\"PWD:\")\n",
        "!pwd\n",
        "print(\"List of Files:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Hsds3cr6TZ"
      },
      "source": [
        "# Check if CUDA available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kN7Gyyj8qY-",
        "outputId": "74167d5b-a913-4365-f456-137755accacf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hdvCTHEz5IFh"
      },
      "source": [
        "# Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOmDqUP55Jb6",
        "outputId": "5962c6c5-3579-4b40-e837-9319e3b9512f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pre-processing train data...\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 8\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "pre-processing dev data...\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 8\n",
            "\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "\n",
            "pre-processing test data...\n",
            "Tokenizer Version 1.1\n",
            "Language: fr\n",
            "Number of threads: 8\n",
            "\n",
            "Tokenizer Version 1.1\n",
            "Language: en\n",
            "Number of threads: 8\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash prepare_data_no_bpe.sh"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wTwNvFh15WHX"
      },
      "source": [
        "# Preprocess the data with Moses Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd8OOB4j5PTW",
        "outputId": "7f3294b5-fe21-4fcd-8056-53377ee9cfd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 22:29:54 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang='fr', target_lang='en', trainpref='iwslt13_fr_en_no_bpe/train', validpref='iwslt13_fr_en_no_bpe/dev', testpref='iwslt13_fr_en_no_bpe/test', align_suffix=None, destdir='data-bin/iwslt13_fr_en_no_bpe', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=False, padding_factor=8, workers=20, dict_only=False)\n",
            "2023-04-30 22:30:01 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 71632 types\n",
            "2023-04-30 22:30:14 | INFO | fairseq_cli.preprocess | [fr] iwslt13_fr_en_no_bpe/train.fr: 162681 sents, 3649842 tokens, 0.0% replaced (by <unk>)\n",
            "2023-04-30 22:30:14 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 71632 types\n",
            "2023-04-30 22:30:15 | INFO | fairseq_cli.preprocess | [fr] iwslt13_fr_en_no_bpe/dev.fr: 887 sents, 21395 tokens, 1.58% replaced (by <unk>)\n",
            "2023-04-30 22:30:15 | INFO | fairseq_cli.preprocess | [fr] Dictionary: 71632 types\n",
            "2023-04-30 22:30:16 | INFO | fairseq_cli.preprocess | [fr] iwslt13_fr_en_no_bpe/test.fr: 1664 sents, 36025 tokens, 1.09% replaced (by <unk>)\n",
            "2023-04-30 22:30:16 | INFO | fairseq_cli.preprocess | [en] Dictionary: 55752 types\n",
            "2023-04-30 22:30:26 | INFO | fairseq_cli.preprocess | [en] iwslt13_fr_en_no_bpe/train.en: 162681 sents, 3404336 tokens, 0.0% replaced (by <unk>)\n",
            "2023-04-30 22:30:26 | INFO | fairseq_cli.preprocess | [en] Dictionary: 55752 types\n",
            "2023-04-30 22:30:27 | INFO | fairseq_cli.preprocess | [en] iwslt13_fr_en_no_bpe/dev.en: 887 sents, 21149 tokens, 1.12% replaced (by <unk>)\n",
            "2023-04-30 22:30:27 | INFO | fairseq_cli.preprocess | [en] Dictionary: 55752 types\n",
            "2023-04-30 22:30:28 | INFO | fairseq_cli.preprocess | [en] iwslt13_fr_en_no_bpe/test.en: 1664 sents, 33827 tokens, 0.665% replaced (by <unk>)\n",
            "2023-04-30 22:30:28 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/iwslt13_fr_en_no_bpe\n"
          ]
        }
      ],
      "source": [
        "!TEXT=iwslt13_fr_en_no_bpe ; mkdir -p data-bin ; fairseq-preprocess --source-lang fr --target-lang en     --trainpref $TEXT/train --validpref $TEXT/dev --testpref $TEXT/test     --destdir data-bin/iwslt13_fr_en_no_bpe     --workers 20"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qzihiuxcv1KW"
      },
      "source": [
        "# Train CNN with Moses Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ze6vhPW46AI",
        "outputId": "0f389461-b3d1-4547-e193-3460fb436f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 22:30:31 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.5], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/cnn_no_bpe', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='fconv', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.5], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/cnn_no_bpe', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/iwslt13_fr_en_no_bpe', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, momentum=0.99, weight_decay=0.0, force_anneal=50, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, dropout=0.1, no_seed_provided=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_layers='[(512, 3)] * 20', decoder_embed_dim=512, decoder_embed_path=None, decoder_layers='[(512, 3)] * 20', decoder_out_embed_dim=256, decoder_attention='True', share_input_output_embed=False, _name='fconv'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_no_bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [0.5]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': 50, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.5]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 22:30:31 | INFO | fairseq.tasks.translation | [fr] dictionary: 71632 types\n",
            "2023-04-30 22:30:31 | INFO | fairseq.tasks.translation | [en] dictionary: 55752 types\n",
            "2023-04-30 22:30:34 | INFO | fairseq_cli.train | FConvModel(\n",
            "  (encoder): FConvEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(71632, 512, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 512, padding_idx=1)\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "      (4): None\n",
            "      (5): None\n",
            "      (6): None\n",
            "      (7): None\n",
            "      (8): None\n",
            "      (9): None\n",
            "      (10): None\n",
            "      (11): None\n",
            "      (12): None\n",
            "      (13): None\n",
            "      (14): None\n",
            "      (15): None\n",
            "      (16): None\n",
            "      (17): None\n",
            "      (18): None\n",
            "      (19): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (1): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (2): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (3): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (4): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (5): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (6): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (7): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (8): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (9): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (10): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (11): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (12): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (13): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (14): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (15): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (16): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (17): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (18): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "      (19): ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "    )\n",
            "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): FConvDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(55752, 512, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 512, padding_idx=1)\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "      (4): None\n",
            "      (5): None\n",
            "      (6): None\n",
            "      (7): None\n",
            "      (8): None\n",
            "      (9): None\n",
            "      (10): None\n",
            "      (11): None\n",
            "      (12): None\n",
            "      (13): None\n",
            "      (14): None\n",
            "      (15): None\n",
            "      (16): None\n",
            "      (17): None\n",
            "      (18): None\n",
            "      (19): None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (1): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (2): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (3): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (4): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (5): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (6): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (7): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (8): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (9): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (10): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (11): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (12): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (13): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (14): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (15): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (16): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (17): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (18): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "      (19): LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "    )\n",
            "    (attention): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (3): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (4): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (5): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (6): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (7): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (8): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (9): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (10): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (11): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (12): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (13): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (14): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (15): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (16): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (17): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (18): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (19): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (fc3): Linear(in_features=256, out_features=55752, bias=True)\n",
            "  )\n",
            ")\n",
            "2023-04-30 22:30:34 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-04-30 22:30:34 | INFO | fairseq_cli.train | model: FConvModel\n",
            "2023-04-30 22:30:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-04-30 22:30:34 | INFO | fairseq_cli.train | num. shared model params: 155,097,488 (num. trained: 155,097,488)\n",
            "2023-04-30 22:30:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-04-30 22:30:34 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.fr\n",
            "2023-04-30 22:30:34 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.en\n",
            "2023-04-30 22:30:34 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe valid fr-en 887 examples\n",
            "2023-04-30 22:30:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 22:30:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-04-30 22:30:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 22:30:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-04-30 22:30:36 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-04-30 22:30:36 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/cnn_no_bpe/checkpoint_last.pt\n",
            "2023-04-30 22:30:36 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/cnn_no_bpe/checkpoint_last.pt\n",
            "2023-04-30 22:30:36 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-04-30 22:30:36 | INFO | fairseq.data.data_utils | loaded 162,681 examples from: data-bin/iwslt13_fr_en_no_bpe/train.fr-en.fr\n",
            "2023-04-30 22:30:36 | INFO | fairseq.data.data_utils | loaded 162,681 examples from: data-bin/iwslt13_fr_en_no_bpe/train.fr-en.en\n",
            "2023-04-30 22:30:36 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe train fr-en 162681 examples\n",
            "2023-04-30 22:30:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 001:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 22:30:36 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-04-30 22:30:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1028: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
            "epoch 001:   0% 6/1348 [00:02<05:15,  4.26it/s]2023-04-30 22:30:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   1% 9/1348 [00:02<04:25,  5.05it/s]2023-04-30 22:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   1% 10/1348 [00:02<04:03,  5.49it/s]2023-04-30 22:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:   1% 11/1348 [00:02<03:43,  5.99it/s]2023-04-30 22:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 001:   1% 13/1348 [00:03<03:43,  5.97it/s]2023-04-30 22:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "epoch 001:   1% 14/1348 [00:03<03:38,  6.12it/s]2023-04-30 22:30:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
            "epoch 001:   1% 19/1348 [00:04<04:00,  5.52it/s]2023-04-30 22:30:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
            "epoch 001:  13% 179/1348 [00:34<03:43,  5.23it/s, loss=12.663, nll_loss=12.037, ppl=4203.14, wps=12602, ups=5.02, wpb=2511.8, bsz=102, num_updates=100, lr=0.5, gnorm=3.416, clip=100, loss_scale=1, train_wall=21, gb_free=10.4, wall=21]2023-04-30 22:31:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
            "epoch 001:  28% 372/1348 [01:11<03:02,  5.36it/s, loss=9.326, nll_loss=8.333, ppl=322.42, wps=13383.5, ups=5.3, wpb=2526.6, bsz=128.4, num_updates=300, lr=0.5, gnorm=0.698, clip=100, loss_scale=0.5, train_wall=19, gb_free=10.2, wall=59]2023-04-30 22:31:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25\n",
            "epoch 001:  43% 581/1348 [01:52<02:33,  4.99it/s, loss=8.495, nll_loss=7.381, ppl=166.72, wps=13139.4, ups=5.14, wpb=2555.9, bsz=125.8, num_updates=500, lr=0.5, gnorm=0.463, clip=100, loss_scale=0.25, train_wall=19, gb_free=10.1, wall=98]2023-04-30 22:32:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125\n",
            "epoch 001:  77% 1042/1348 [03:22<00:58,  5.19it/s, loss=7.433, nll_loss=6.149, ppl=70.95, wps=12696.7, ups=5.05, wpb=2514.3, bsz=132.5, num_updates=1000, lr=0.5, gnorm=0.866, clip=100, loss_scale=0.125, train_wall=20, gb_free=10.1, wall=197]2023-04-30 22:33:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625\n",
            "epoch 001:  94% 1271/1348 [04:07<00:15,  5.00it/s, loss=7.278, nll_loss=5.976, ppl=62.95, wps=12828.5, ups=5.08, wpb=2523, bsz=121.5, num_updates=1200, lr=0.5, gnorm=0.528, clip=100, loss_scale=0.0625, train_wall=19, gb_free=10.2, wall=236]2023-04-30 22:34:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125\n",
            "epoch 001: 100% 1347/1348 [04:22<00:00,  4.88it/s, loss=7.173, nll_loss=5.857, ppl=57.95, wps=12316.1, ups=5.01, wpb=2460.4, bsz=104.6, num_updates=1300, lr=0.5, gnorm=3.703, clip=100, loss_scale=0.0312, train_wall=20, gb_free=10.3, wall=256]2023-04-30 22:34:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 1/13 [00:01<00:13,  1.13s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 2/13 [00:02<00:12,  1.11s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 3/13 [00:03<00:11,  1.20s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 4/13 [00:04<00:11,  1.29s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 5/13 [00:06<00:10,  1.34s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 6/13 [00:07<00:09,  1.37s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 7/13 [00:09<00:08,  1.43s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 8/13 [00:11<00:07,  1.52s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 9/13 [00:13<00:06,  1.65s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 10/13 [00:15<00:05,  1.85s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 11/13 [00:17<00:04,  2.08s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 12/13 [00:21<00:02,  2.54s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 13/13 [00:27<00:00,  3.55s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 22:35:26 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.423 | nll_loss 6.138 | ppl 70.42 | bleu 9.52 | wps 751.9 | wpb 1626.8 | bsz 68.2 | num_updates 1336\n",
            "2023-04-30 22:35:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1336 updates\n",
            "2023-04-30 22:35:26 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint1.pt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n",
            "2023-04-30 22:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint1.pt\n",
            "2023-04-30 22:35:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_no_bpe/checkpoint1.pt (epoch 1 @ 1336 updates, score 9.52) (writing took 14.77108480000004 seconds)\n",
            "2023-04-30 22:35:41 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-04-30 22:35:41 | INFO | train | epoch 001 | loss 8.445 | nll_loss 7.31 | ppl 158.73 | wps 11098.2 | ups 4.39 | wpb 2527.1 | bsz 121.4 | num_updates 1336 | lr 0.5 | gnorm 1.129 | clip 100 | loss_scale 0.0312 | train_wall 260 | gb_free 10.4 | wall 305\n",
            "2023-04-30 22:35:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 002:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 22:35:41 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-04-30 22:35:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:   8% 114/1348 [00:24<04:16,  4.80it/s, loss=6.953, nll_loss=5.605, ppl=48.68, wps=4022.3, ups=1.59, wpb=2524.2, bsz=115.4, num_updates=1400, lr=0.5, gnorm=0.472, clip=100, loss_scale=0.0312, train_wall=20, gb_free=10.2, wall=319]2023-04-30 22:36:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625\n",
            "epoch 002:  19% 259/1348 [00:53<03:29,  5.19it/s, loss=6.719, nll_loss=5.326, ppl=40.12, wps=12025, ups=4.77, wpb=2520.7, bsz=130.2, num_updates=1500, lr=0.5, gnorm=0.657, clip=100, loss_scale=0.0156, train_wall=21, gb_free=10.5, wall=340]2023-04-30 22:36:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125\n",
            "epoch 002: 100% 1347/1348 [04:29<00:00,  5.03it/s, loss=6.069, nll_loss=4.576, ppl=23.86, wps=12599.8, ups=5, wpb=2520.4, bsz=128.7, num_updates=2600, lr=0.5, gnorm=0.376, clip=100, loss_scale=0.0078, train_wall=20, gb_free=10.4, wall=559]2023-04-30 22:40:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 1/13 [00:01<00:13,  1.13s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 2/13 [00:02<00:11,  1.08s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 3/13 [00:03<00:11,  1.13s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.19s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 5/13 [00:06<00:10,  1.26s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 6/13 [00:07<00:09,  1.31s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 7/13 [00:08<00:08,  1.37s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 8/13 [00:10<00:07,  1.44s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 9/13 [00:12<00:06,  1.60s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 10/13 [00:14<00:05,  1.82s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 11/13 [00:17<00:04,  2.01s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 12/13 [00:20<00:02,  2.42s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 13/13 [00:26<00:00,  3.40s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 22:40:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.575 | nll_loss 5.179 | ppl 36.23 | bleu 15.9 | wps 786.1 | wpb 1626.8 | bsz 68.2 | num_updates 2682 | best_bleu 15.9\n",
            "2023-04-30 22:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2682 updates\n",
            "2023-04-30 22:40:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint2.pt\n",
            "2023-04-30 22:40:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint2.pt\n",
            "2023-04-30 22:40:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_no_bpe/checkpoint2.pt (epoch 2 @ 2682 updates, score 15.9) (writing took 15.62283142800004 seconds)\n",
            "2023-04-30 22:40:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-04-30 22:40:53 | INFO | train | epoch 002 | loss 6.409 | nll_loss 4.97 | ppl 31.34 | wps 10918.3 | ups 4.32 | wpb 2525.8 | bsz 120.9 | num_updates 2682 | lr 0.5 | gnorm 44.192 | clip 100 | loss_scale 0.0078 | train_wall 266 | gb_free 10.7 | wall 617\n",
            "2023-04-30 22:40:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 003:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 22:40:53 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-04-30 22:40:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1347/1348 [04:28<00:00,  4.95it/s, loss=5.57, nll_loss=3.995, ppl=15.95, wps=12807.5, ups=5.05, wpb=2535.6, bsz=122.4, num_updates=4000, lr=0.5, gnorm=1.092, clip=100, loss_scale=0.0078, train_wall=20, gb_free=10.3, wall=880]2023-04-30 22:45:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 1/13 [00:01<00:12,  1.00s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.00it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.08s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.14s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.21s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  46% 6/13 [00:07<00:08,  1.27s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  54% 7/13 [00:08<00:08,  1.34s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 8/13 [00:10<00:07,  1.43s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 9/13 [00:12<00:06,  1.60s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 10/13 [00:14<00:05,  1.76s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 11/13 [00:16<00:03,  1.94s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 12/13 [00:19<00:02,  2.33s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 13/13 [00:25<00:00,  3.37s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 22:45:48 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.2 | nll_loss 4.705 | ppl 26.08 | bleu 18.56 | wps 799.9 | wpb 1626.8 | bsz 68.2 | num_updates 4030 | best_bleu 18.56\n",
            "2023-04-30 22:45:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4030 updates\n",
            "2023-04-30 22:45:48 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint3.pt\n",
            "2023-04-30 22:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint3.pt\n",
            "2023-04-30 22:46:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_no_bpe/checkpoint3.pt (epoch 3 @ 4030 updates, score 18.56) (writing took 16.115742554000008 seconds)\n",
            "2023-04-30 22:46:04 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-04-30 22:46:04 | INFO | train | epoch 003 | loss 5.695 | nll_loss 4.14 | ppl 17.63 | wps 10944.8 | ups 4.33 | wpb 2525.5 | bsz 120.7 | num_updates 4030 | lr 0.5 | gnorm 5.865 | clip 100 | loss_scale 0.0078 | train_wall 266 | gb_free 10.6 | wall 928\n",
            "2023-04-30 22:46:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 004:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 22:46:04 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-04-30 22:46:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1347/1348 [04:28<00:00,  5.07it/s, loss=5.195, nll_loss=3.56, ppl=11.79, wps=12596.5, ups=5.02, wpb=2511.5, bsz=123, num_updates=5300, lr=0.5, gnorm=0.327, clip=100, loss_scale=0.0078, train_wall=20, gb_free=10.2, wall=1181]2023-04-30 22:50:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 1/13 [00:00<00:10,  1.15it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 3/13 [00:02<00:10,  1.02s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.08s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 5/13 [00:05<00:08,  1.12s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  46% 6/13 [00:06<00:07,  1.12s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 7/13 [00:07<00:07,  1.23s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.34s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.49s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 10/13 [00:13<00:04,  1.55s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 11/13 [00:15<00:03,  1.77s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 12/13 [00:18<00:02,  2.23s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 13/13 [00:24<00:00,  3.31s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 22:50:57 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.963 | nll_loss 4.445 | ppl 21.79 | bleu 20.94 | wps 840.6 | wpb 1626.8 | bsz 68.2 | num_updates 5378 | best_bleu 20.94\n",
            "2023-04-30 22:50:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5378 updates\n",
            "2023-04-30 22:50:57 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint4.pt\n",
            "2023-04-30 22:51:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint4.pt\n",
            "2023-04-30 22:51:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_no_bpe/checkpoint4.pt (epoch 4 @ 5378 updates, score 20.94) (writing took 17.832952336000062 seconds)\n",
            "2023-04-30 22:51:15 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-04-30 22:51:15 | INFO | train | epoch 004 | loss 5.274 | nll_loss 3.645 | ppl 12.51 | wps 10945.2 | ups 4.33 | wpb 2525.5 | bsz 120.7 | num_updates 5378 | lr 0.5 | gnorm 4.908 | clip 100 | loss_scale 0.0078 | train_wall 266 | gb_free 10.3 | wall 1239\n",
            "2023-04-30 22:51:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 005:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 22:51:15 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-04-30 22:51:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1347/1348 [04:28<00:00,  4.94it/s, loss=5.09, nll_loss=3.414, ppl=10.66, wps=12227.3, ups=5.02, wpb=2434.8, bsz=118.3, num_updates=6700, lr=0.5, gnorm=0.355, clip=100, loss_scale=0.0078, train_wall=20, gb_free=10.4, wall=1502]2023-04-30 22:55:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 1/13 [00:01<00:12,  1.02s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 2/13 [00:02<00:11,  1.00s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.09s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.11s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.22s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 6/13 [00:06<00:08,  1.19s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  54% 7/13 [00:08<00:07,  1.26s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.27s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.36s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 10/13 [00:13<00:04,  1.59s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 11/13 [00:15<00:03,  1.68s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 12/13 [00:18<00:02,  2.22s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 13/13 [00:24<00:00,  3.27s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 22:56:08 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.844 | nll_loss 4.274 | ppl 19.34 | bleu 21.86 | wps 849 | wpb 1626.8 | bsz 68.2 | num_updates 6726 | best_bleu 21.86\n",
            "2023-04-30 22:56:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6726 updates\n",
            "2023-04-30 22:56:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint5.pt\n",
            "2023-04-30 22:56:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/checkpoint5.pt\n",
            "2023-04-30 22:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_no_bpe/checkpoint5.pt (epoch 5 @ 6726 updates, score 21.86) (writing took 16.191833852000173 seconds)\n",
            "2023-04-30 22:56:24 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-04-30 22:56:24 | INFO | train | epoch 005 | loss 4.98 | nll_loss 3.29 | ppl 9.78 | wps 11013.1 | ups 4.36 | wpb 2525.5 | bsz 120.7 | num_updates 6726 | lr 0.5 | gnorm 1.011 | clip 100 | loss_scale 0.0078 | train_wall 265 | gb_free 10.3 | wall 1548\n",
            "2023-04-30 22:56:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 006:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 22:56:24 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-04-30 22:56:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  58% 784/1348 [02:36<01:55,  4.90it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625\n",
            "epoch 006:  58% 787/1348 [02:37<01:52,  4.97it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.001953125\n",
            "epoch 006:  58% 788/1348 [02:37<01:43,  5.39it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0009765625\n",
            "epoch 006:  59% 789/1348 [02:37<01:37,  5.76it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00048828125\n",
            "epoch 006:  59% 790/1348 [02:37<01:32,  6.01it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.000244140625\n",
            "epoch 006:  59% 791/1348 [02:38<01:32,  6.01it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0001220703125\n",
            "epoch 006:  59% 793/1348 [02:38<01:41,  5.46it/s, loss=7.908, nll_loss=3.889, ppl=14.81, wps=12656.6, ups=5.09, wpb=2485.6, bsz=110.4, num_updates=7500, lr=0.5, gnorm=7.166, clip=100, loss_scale=0.0078, train_wall=19, gb_free=10.2, wall=1703]2023-04-30 22:59:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/crash.pt\n",
            "2023-04-30 22:59:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_no_bpe/crash.pt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 930, in train_step\n",
            "    grad_norm = self.clip_grad_norm(self.cfg.optimization.clip_norm)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 1264, in clip_grad_norm\n",
            "    return self.optimizer.clip_grad_norm(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/optim/fp16_optimizer.py\", line 201, in clip_grad_norm\n",
            "    self.scaler.check_overflow(grad_norm)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/optim/dynamic_loss_scaler.py\", line 61, in check_overflow\n",
            "    raise FloatingPointError(\n",
            "FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 557, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/distributed/utils.py\", line 369, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 190, in main\n",
            "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq_cli/train.py\", line 316, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/trainer.py\", line 977, in train_step\n",
            "    self.task.train_step(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/tasks/fairseq_task.py\", line 515, in train_step\n",
            "    loss, sample_size, logging_output = criterion(model, sample)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/criterions/label_smoothed_cross_entropy.py\", line 79, in forward\n",
            "    net_output = model(**sample[\"net_input\"])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1148, in _call_impl\n",
            "    result = forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fairseq/models/fairseq_model.py\", line 322, in forward\n",
            "    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1163, in _call_impl\n",
            "    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
            "StopIteration\n"
          ]
        }
      ],
      "source": [
        "!fairseq-train data-bin/iwslt13_fr_en_no_bpe --arch fconv \\\n",
        "    --dropout 0.1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --optimizer nag --clip-norm 0.1 \\\n",
        "    --lr 0.5 --lr-scheduler fixed --force-anneal 50 \\\n",
        "    --max-tokens 3000 --eval-bleu =--eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints/cnn_no_bpe \\\n",
        "    --fp16 --patience 10 \\\n",
        "    --max-epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQUH-rDv5RU"
      },
      "source": [
        "# Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEIEY7W-qo5k",
        "outputId": "244b0843-086f-46bd-f3c3-1b1b87e3bb1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 22:59:23 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cnn_no_bpe/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/cnn_no_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_no_bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 22:59:23 | INFO | fairseq.tasks.translation | [fr] dictionary: 71632 types\n",
            "2023-04-30 22:59:23 | INFO | fairseq.tasks.translation | [en] dictionary: 55752 types\n",
            "2023-04-30 22:59:23 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cnn_no_bpe/checkpoint_best.pt\n",
            "2023-04-30 22:59:27 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.fr\n",
            "2023-04-30 22:59:27 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.en\n",
            "2023-04-30 22:59:27 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe valid fr-en 887 examples\n",
            "2023-04-30 22:59:59 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-04-30 22:59:59 | INFO | fairseq_cli.generate | Translated 887 sentences (20,541 tokens) in 23.0s (38.55 sentences/s, 892.66 tokens/s)\n",
            "2023-04-30 22:59:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-04-30 22:59:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-04-30 22:59:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_no_bpe \\\n",
        "    --path checkpoints/cnn_no_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/cnn_no_bpe.pred --gen-subset valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBioU9aMv9rz"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma3BlxvnsFtu",
        "outputId": "5ecff0a7-86d3-4056-a1f3-b5b10f80c5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 23:00:08 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cnn_no_bpe/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/cnn_no_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_no_bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 23:00:08 | INFO | fairseq.tasks.translation | [fr] dictionary: 71632 types\n",
            "2023-04-30 23:00:08 | INFO | fairseq.tasks.translation | [en] dictionary: 55752 types\n",
            "2023-04-30 23:00:08 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cnn_no_bpe/checkpoint_best.pt\n",
            "2023-04-30 23:00:12 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_no_bpe/test.fr-en.fr\n",
            "2023-04-30 23:00:12 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_no_bpe/test.fr-en.en\n",
            "2023-04-30 23:00:12 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe test fr-en 1664 examples\n",
            "2023-04-30 23:00:49 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-04-30 23:00:49 | INFO | fairseq_cli.generate | Translated 1,664 sentences (34,087 tokens) in 26.2s (63.59 sentences/s, 1302.60 tokens/s)\n",
            "2023-04-30 23:00:49 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-04-30 23:00:49 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-04-30 23:00:49 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_no_bpe \\\n",
        "    --path checkpoints/cnn_no_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 --beam 5 \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/cnn_no_bpe.pred"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pyrV8q0-wBNF"
      },
      "source": [
        "# Train Transformer with Moses Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM29Uqpct9u8",
        "outputId": "bc798020-f9ae-4b7a-804e-94a8aa3b0835"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 23:00:58 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/tf_no_bpe', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/tf_no_bpe', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/iwslt13_fr_en_no_bpe', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_no_bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 23:00:58 | INFO | fairseq.tasks.translation | [fr] dictionary: 71632 types\n",
            "2023-04-30 23:00:58 | INFO | fairseq.tasks.translation | [en] dictionary: 55752 types\n",
            "2023-04-30 23:01:00 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(71632, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(55752, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=55752, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-04-30 23:01:00 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-04-30 23:01:00 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-04-30 23:01:00 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-04-30 23:01:00 | INFO | fairseq_cli.train | num. shared model params: 128,261,120 (num. trained: 128,261,120)\n",
            "2023-04-30 23:01:00 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-04-30 23:01:00 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.fr\n",
            "2023-04-30 23:01:00 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.en\n",
            "2023-04-30 23:01:00 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe valid fr-en 887 examples\n",
            "2023-04-30 23:01:02 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2023-04-30 23:01:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 23:01:02 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-04-30 23:01:02 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 23:01:02 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-04-30 23:01:02 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-04-30 23:01:02 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/tf_no_bpe/checkpoint_last.pt\n",
            "2023-04-30 23:01:02 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/tf_no_bpe/checkpoint_last.pt\n",
            "2023-04-30 23:01:02 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-04-30 23:01:02 | INFO | fairseq.data.data_utils | loaded 162,681 examples from: data-bin/iwslt13_fr_en_no_bpe/train.fr-en.fr\n",
            "2023-04-30 23:01:02 | INFO | fairseq.data.data_utils | loaded 162,681 examples from: data-bin/iwslt13_fr_en_no_bpe/train.fr-en.en\n",
            "2023-04-30 23:01:02 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe train fr-en 162681 examples\n",
            "2023-04-30 23:01:02 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-04-30 23:01:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 001:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 23:01:02 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-04-30 23:01:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1347/1348 [09:39<00:00,  2.17it/s, loss=8.609, nll_loss=7.504, ppl=181.57, wps=5789.3, ups=2.33, wpb=2481.3, bsz=106.5, num_updates=1300, lr=0.0001625, gnorm=1.648, train_wall=43, gb_free=10.9, wall=559]2023-04-30 23:10:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 1/13 [00:01<00:15,  1.29s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  15% 2/13 [00:02<00:12,  1.13s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  23% 3/13 [00:03<00:11,  1.15s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.18s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.21s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  46% 6/13 [00:07<00:08,  1.21s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 7/13 [00:08<00:07,  1.19s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.22s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.30s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 10/13 [00:12<00:04,  1.42s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 11/13 [00:14<00:02,  1.47s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  92% 12/13 [00:16<00:01,  1.62s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 13/13 [00:18<00:00,  1.93s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 23:11:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.544 | nll_loss 7.397 | ppl 168.5 | bleu 1.43 | wps 1115.4 | wpb 1626.8 | bsz 68.2 | num_updates 1348\n",
            "2023-04-30 23:11:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1348 updates\n",
            "2023-04-30 23:11:00 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint1.pt\n",
            "2023-04-30 23:11:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint1.pt\n",
            "2023-04-30 23:11:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint1.pt (epoch 1 @ 1348 updates, score 1.43) (writing took 22.466224065000006 seconds)\n",
            "2023-04-30 23:11:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-04-30 23:11:23 | INFO | train | epoch 001 | loss 10 | nll_loss 9.113 | ppl 553.61 | wps 5488.9 | ups 2.17 | wpb 2525.5 | bsz 120.7 | num_updates 1348 | lr 0.0001685 | gnorm 2.257 | train_wall 576 | gb_free 10.9 | wall 621\n",
            "2023-04-30 23:11:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 002:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 23:11:23 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-04-30 23:11:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1347/1348 [09:41<00:00,  2.29it/s, loss=7.763, nll_loss=6.551, ppl=93.79, wps=5779, ups=2.32, wpb=2492.2, bsz=126.6, num_updates=2600, lr=0.000325, gnorm=1.261, train_wall=43, gb_free=10.5, wall=1162]2023-04-30 23:21:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 1/13 [00:01<00:13,  1.09s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 2/13 [00:02<00:11,  1.04s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.09s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.14s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.18s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 6/13 [00:06<00:08,  1.20s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 7/13 [00:08<00:07,  1.21s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.26s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.42s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 10/13 [00:13<00:04,  1.56s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 11/13 [00:14<00:03,  1.57s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  92% 12/13 [00:16<00:01,  1.75s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 13/13 [00:19<00:00,  2.07s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 23:21:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.915 | nll_loss 6.717 | ppl 105.23 | bleu 2.28 | wps 1056.2 | wpb 1626.8 | bsz 68.2 | num_updates 2696 | best_bleu 2.28\n",
            "2023-04-30 23:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2696 updates\n",
            "2023-04-30 23:21:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint2.pt\n",
            "2023-04-30 23:21:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint2.pt\n",
            "2023-04-30 23:21:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint2.pt (epoch 2 @ 2696 updates, score 2.28) (writing took 21.071559139999863 seconds)\n",
            "2023-04-30 23:21:46 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-04-30 23:21:46 | INFO | train | epoch 002 | loss 8.03 | nll_loss 6.852 | ppl 115.48 | wps 5461.7 | ups 2.16 | wpb 2525.5 | bsz 120.7 | num_updates 2696 | lr 0.000337 | gnorm 1.49 | train_wall 579 | gb_free 11.1 | wall 1244\n",
            "2023-04-30 23:21:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 003:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 23:21:46 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-04-30 23:21:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1347/1348 [09:39<00:00,  2.27it/s, loss=7.433, nll_loss=6.18, ppl=72.48, wps=5852, ups=2.3, wpb=2540.6, bsz=115.6, num_updates=4000, lr=0.0005, gnorm=1.429, train_wall=43, gb_free=10.9, wall=1805]2023-04-30 23:31:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   8% 1/13 [00:01<00:12,  1.02s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.03s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.06s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 5/13 [00:05<00:08,  1.09s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  46% 6/13 [00:06<00:07,  1.09s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  54% 7/13 [00:07<00:06,  1.09s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  62% 8/13 [00:08<00:05,  1.13s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  69% 9/13 [00:10<00:04,  1.21s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  77% 10/13 [00:11<00:03,  1.30s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 11/13 [00:12<00:02,  1.27s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  92% 12/13 [00:14<00:01,  1.47s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 13/13 [00:16<00:00,  1.56s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 23:31:43 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.588 | nll_loss 6.278 | ppl 77.6 | bleu 2.88 | wps 1275.1 | wpb 1626.8 | bsz 68.2 | num_updates 4044 | best_bleu 2.88\n",
            "2023-04-30 23:31:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4044 updates\n",
            "2023-04-30 23:31:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint3.pt\n",
            "2023-04-30 23:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint3.pt\n",
            "2023-04-30 23:32:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint3.pt (epoch 3 @ 4044 updates, score 2.88) (writing took 20.28535100299996 seconds)\n",
            "2023-04-30 23:32:03 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-04-30 23:32:03 | INFO | train | epoch 003 | loss 7.478 | nll_loss 6.23 | ppl 75.04 | wps 5519.1 | ups 2.19 | wpb 2525.5 | bsz 120.7 | num_updates 4044 | lr 0.000497272 | gnorm 1.359 | train_wall 576 | gb_free 11 | wall 1861\n",
            "2023-04-30 23:32:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 004:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 23:32:03 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-04-30 23:32:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1347/1348 [09:39<00:00,  2.26it/s, loss=6.905, nll_loss=5.584, ppl=47.98, wps=5907.3, ups=2.31, wpb=2562, bsz=125.3, num_updates=5300, lr=0.000434372, gnorm=1.095, train_wall=43, gb_free=10.7, wall=2401]2023-04-30 23:41:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   8% 1/13 [00:01<00:13,  1.11s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  15% 2/13 [00:02<00:11,  1.05s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  23% 3/13 [00:03<00:11,  1.10s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.13s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.17s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  46% 6/13 [00:06<00:08,  1.16s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 7/13 [00:08<00:07,  1.18s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.24s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  69% 9/13 [00:10<00:05,  1.34s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  77% 10/13 [00:12<00:04,  1.45s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 11/13 [00:14<00:03,  1.52s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  92% 12/13 [00:16<00:01,  1.69s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 13/13 [00:19<00:00,  2.01s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 23:42:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.236 | nll_loss 5.883 | ppl 59.02 | bleu 4.17 | wps 1090.9 | wpb 1626.8 | bsz 68.2 | num_updates 5392 | best_bleu 4.17\n",
            "2023-04-30 23:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5392 updates\n",
            "2023-04-30 23:42:02 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint4.pt\n",
            "2023-04-30 23:42:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint4.pt\n",
            "2023-04-30 23:42:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint4.pt (epoch 4 @ 5392 updates, score 4.17) (writing took 20.66606097299973 seconds)\n",
            "2023-04-30 23:42:23 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-04-30 23:42:23 | INFO | train | epoch 004 | loss 7.102 | nll_loss 5.806 | ppl 55.95 | wps 5494.5 | ups 2.18 | wpb 2525.5 | bsz 120.7 | num_updates 5392 | lr 0.000430651 | gnorm 1.225 | train_wall 576 | gb_free 10.7 | wall 2481\n",
            "2023-04-30 23:42:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 005:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 23:42:23 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-04-30 23:42:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1347/1348 [09:37<00:00,  2.23it/s, loss=6.786, nll_loss=5.453, ppl=43.81, wps=5738.2, ups=2.36, wpb=2435.9, bsz=115.3, num_updates=6700, lr=0.000386334, gnorm=1.195, train_wall=42, gb_free=10.6, wall=3042]2023-04-30 23:52:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   8% 1/13 [00:00<00:10,  1.10it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  23% 3/13 [00:02<00:10,  1.00s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.03s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 5/13 [00:05<00:08,  1.07s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 6/13 [00:06<00:07,  1.07s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  54% 7/13 [00:07<00:06,  1.10s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  62% 8/13 [00:08<00:05,  1.15s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  69% 9/13 [00:10<00:04,  1.25s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  77% 10/13 [00:11<00:04,  1.38s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 11/13 [00:13<00:02,  1.46s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  92% 12/13 [00:15<00:01,  1.68s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 13/13 [00:18<00:00,  2.11s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 23:52:20 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.067 | nll_loss 5.7 | ppl 51.99 | bleu 5.03 | wps 1110.2 | wpb 1626.8 | bsz 68.2 | num_updates 6740 | best_bleu 5.03\n",
            "2023-04-30 23:52:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6740 updates\n",
            "2023-04-30 23:52:20 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint5.pt\n",
            "2023-04-30 23:52:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint5.pt\n",
            "2023-04-30 23:52:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint5.pt (epoch 5 @ 6740 updates, score 5.03) (writing took 20.774025053000514 seconds)\n",
            "2023-04-30 23:52:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-04-30 23:52:40 | INFO | train | epoch 005 | loss 6.75 | nll_loss 5.409 | ppl 42.5 | wps 5509.6 | ups 2.18 | wpb 2525.5 | bsz 120.7 | num_updates 6740 | lr 0.000385186 | gnorm 1.18 | train_wall 575 | gb_free 10.7 | wall 3099\n",
            "2023-04-30 23:52:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 006:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 23:52:41 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-04-30 23:52:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 1347/1348 [09:38<00:00,  2.45it/s, loss=6.437, nll_loss=5.056, ppl=33.27, wps=5815.6, ups=2.31, wpb=2522.8, bsz=126, num_updates=8000, lr=0.000353553, gnorm=1.105, train_wall=43, gb_free=10.5, wall=3640]2023-05-01 00:02:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   8% 1/13 [00:01<00:12,  1.05s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  15% 2/13 [00:02<00:11,  1.05s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  23% 3/13 [00:03<00:11,  1.16s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.20s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.25s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  46% 6/13 [00:07<00:08,  1.24s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  54% 7/13 [00:08<00:07,  1.26s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.31s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.41s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  77% 10/13 [00:13<00:04,  1.58s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 11/13 [00:15<00:03,  1.65s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  92% 12/13 [00:17<00:01,  1.96s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 13/13 [00:20<00:00,  2.26s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 00:02:41 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.957 | nll_loss 5.552 | ppl 46.93 | bleu 4.97 | wps 991.7 | wpb 1626.8 | bsz 68.2 | num_updates 8088 | best_bleu 5.03\n",
            "2023-05-01 00:02:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8088 updates\n",
            "2023-05-01 00:02:41 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint6.pt\n",
            "2023-05-01 00:02:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint6.pt\n",
            "2023-05-01 00:02:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint6.pt (epoch 6 @ 8088 updates, score 4.97) (writing took 15.00570634199994 seconds)\n",
            "2023-05-01 00:02:56 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-05-01 00:02:56 | INFO | train | epoch 006 | loss 6.49 | nll_loss 5.116 | ppl 34.68 | wps 5534 | ups 2.19 | wpb 2525.5 | bsz 120.7 | num_updates 8088 | lr 0.000351625 | gnorm 1.168 | train_wall 575 | gb_free 10.5 | wall 3714\n",
            "2023-05-01 00:02:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 007:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 00:02:56 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-05-01 00:02:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 1347/1348 [09:38<00:00,  2.33it/s, loss=6.238, nll_loss=4.831, ppl=28.46, wps=5838.3, ups=2.31, wpb=2532.2, bsz=127.4, num_updates=9400, lr=0.000326164, gnorm=1.209, train_wall=43, gb_free=10.7, wall=4277]2023-05-01 00:12:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   8% 1/13 [00:00<00:10,  1.10it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.07it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.04s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.11s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.19s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  46% 6/13 [00:06<00:08,  1.26s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  54% 7/13 [00:08<00:07,  1.27s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.32s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.43s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  77% 10/13 [00:13<00:04,  1.57s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 11/13 [00:14<00:03,  1.61s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  92% 12/13 [00:17<00:01,  1.77s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 13/13 [00:20<00:00,  2.18s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 00:12:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.872 | nll_loss 5.465 | ppl 44.17 | bleu 5.59 | wps 1023.3 | wpb 1626.8 | bsz 68.2 | num_updates 9436 | best_bleu 5.59\n",
            "2023-05-01 00:12:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 9436 updates\n",
            "2023-05-01 00:12:55 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint7.pt\n",
            "2023-05-01 00:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint7.pt\n",
            "2023-05-01 00:13:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint7.pt (epoch 7 @ 9436 updates, score 5.59) (writing took 21.233308419000423 seconds)\n",
            "2023-05-01 00:13:16 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-05-01 00:13:16 | INFO | train | epoch 007 | loss 6.288 | nll_loss 4.888 | ppl 29.6 | wps 5486.8 | ups 2.17 | wpb 2525.5 | bsz 120.7 | num_updates 9436 | lr 0.000325541 | gnorm 1.174 | train_wall 575 | gb_free 10.7 | wall 4334\n",
            "2023-05-01 00:13:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 008:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 00:13:16 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-05-01 00:13:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 1347/1348 [09:37<00:00,  2.29it/s, loss=6.231, nll_loss=4.823, ppl=28.3, wps=5749.5, ups=2.33, wpb=2472.2, bsz=110.4, num_updates=10700, lr=0.000305709, gnorm=1.221, train_wall=43, gb_free=10.6, wall=4876]2023-05-01 00:22:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   8% 1/13 [00:00<00:11,  1.02it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.02it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.09s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  31% 4/13 [00:04<00:10,  1.17s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.24s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  46% 6/13 [00:07<00:09,  1.29s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  54% 7/13 [00:08<00:08,  1.37s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  62% 8/13 [00:10<00:07,  1.41s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  69% 9/13 [00:12<00:06,  1.54s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  77% 10/13 [00:14<00:05,  1.68s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 11/13 [00:15<00:03,  1.70s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  92% 12/13 [00:18<00:01,  1.86s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 13/13 [00:21<00:00,  2.24s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 00:23:15 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.814 | nll_loss 5.406 | ppl 42.41 | bleu 5.01 | wps 977.9 | wpb 1626.8 | bsz 68.2 | num_updates 10784 | best_bleu 5.59\n",
            "2023-05-01 00:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10784 updates\n",
            "2023-05-01 00:23:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint8.pt\n",
            "2023-05-01 00:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint8.pt\n",
            "2023-05-01 00:23:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint8.pt (epoch 8 @ 10784 updates, score 5.01) (writing took 12.490293462999944 seconds)\n",
            "2023-05-01 00:23:28 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-05-01 00:23:28 | INFO | train | epoch 008 | loss 6.132 | nll_loss 4.71 | ppl 26.18 | wps 5567.9 | ups 2.2 | wpb 2525.5 | bsz 120.7 | num_updates 10784 | lr 0.000304516 | gnorm 1.204 | train_wall 574 | gb_free 10.6 | wall 4946\n",
            "2023-05-01 00:23:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 009:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 00:23:28 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-05-01 00:23:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 1347/1348 [09:39<00:00,  2.29it/s, loss=6.08, nll_loss=4.65, ppl=25.11, wps=5783.3, ups=2.33, wpb=2482.3, bsz=111.5, num_updates=12100, lr=0.00028748, gnorm=1.435, train_wall=43, gb_free=10.6, wall=5512]2023-05-01 00:33:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   8% 1/13 [00:01<00:12,  1.00s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.02it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.04s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.10s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.15s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  46% 6/13 [00:06<00:08,  1.17s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  54% 7/13 [00:07<00:07,  1.20s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.27s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  69% 9/13 [00:11<00:05,  1.41s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  77% 10/13 [00:13<00:04,  1.57s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 11/13 [00:14<00:03,  1.66s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 12/13 [00:17<00:01,  1.88s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 13/13 [00:20<00:00,  2.22s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 00:33:28 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.65 | nll_loss 5.219 | ppl 37.25 | bleu 6.76 | wps 1022.4 | wpb 1626.8 | bsz 68.2 | num_updates 12132 | best_bleu 6.76\n",
            "2023-05-01 00:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12132 updates\n",
            "2023-05-01 00:33:28 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint9.pt\n",
            "2023-05-01 00:33:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint9.pt\n",
            "2023-05-01 00:33:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint9.pt (epoch 9 @ 12132 updates, score 6.76) (writing took 20.829056628000217 seconds)\n",
            "2023-05-01 00:33:49 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-05-01 00:33:49 | INFO | train | epoch 009 | loss 5.98 | nll_loss 4.538 | ppl 23.23 | wps 5478.9 | ups 2.17 | wpb 2525.5 | bsz 120.7 | num_updates 12132 | lr 0.0002871 | gnorm 1.24 | train_wall 576 | gb_free 11.1 | wall 5567\n",
            "2023-05-01 00:33:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 010:   0% 0/1348 [00:00<?, ?it/s]2023-05-01 00:33:49 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-05-01 00:33:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 1347/1348 [09:41<00:00,  2.38it/s, loss=5.743, nll_loss=4.268, ppl=19.26, wps=5802.9, ups=2.31, wpb=2511.3, bsz=119.8, num_updates=13400, lr=0.000273179, gnorm=1.352, train_wall=43, gb_free=11, wall=6115]2023-05-01 00:43:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/13 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   8% 1/13 [00:00<00:11,  1.00it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  15% 2/13 [00:01<00:10,  1.02it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  23% 3/13 [00:03<00:10,  1.05s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  31% 4/13 [00:04<00:09,  1.10s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 5/13 [00:05<00:09,  1.16s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  46% 6/13 [00:06<00:08,  1.16s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  54% 7/13 [00:07<00:07,  1.19s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  62% 8/13 [00:09<00:06,  1.24s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  69% 9/13 [00:10<00:05,  1.37s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  77% 10/13 [00:12<00:04,  1.51s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 11/13 [00:14<00:03,  1.55s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  92% 12/13 [00:16<00:01,  1.75s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 13/13 [00:19<00:00,  2.14s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-05-01 00:43:51 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.467 | nll_loss 5.036 | ppl 32.8 | bleu 8.27 | wps 1056.5 | wpb 1626.8 | bsz 68.2 | num_updates 13480 | best_bleu 8.27\n",
            "2023-05-01 00:43:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 13480 updates\n",
            "2023-05-01 00:43:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint10.pt\n",
            "2023-05-01 00:43:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_no_bpe/checkpoint10.pt\n",
            "2023-05-01 00:44:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_no_bpe/checkpoint10.pt (epoch 10 @ 13480 updates, score 8.27) (writing took 21.628479744000288 seconds)\n",
            "2023-05-01 00:44:13 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-05-01 00:44:13 | INFO | train | epoch 010 | loss 5.732 | nll_loss 4.256 | ppl 19.1 | wps 5457.9 | ups 2.16 | wpb 2525.5 | bsz 120.7 | num_updates 13480 | lr 0.000272367 | gnorm 1.342 | train_wall 579 | gb_free 11 | wall 6191\n",
            "2023-05-01 00:44:13 | INFO | fairseq_cli.train | done training in 6190.9 seconds\n"
          ]
        }
      ],
      "source": [
        "!fairseq-train data-bin/iwslt13_fr_en_no_bpe --arch transformer \\\n",
        "    --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000  \\\n",
        "    --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 3000 --eval-bleu --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints/tf_no_bpe --patience 10 \\\n",
        "    --max-epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB3CoA_4sB_U"
      },
      "source": [
        "# Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR5oIqLP8ZN8",
        "outputId": "a6e4212b-580c-49b5-f5cf-5cb3b0f5600c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 00:44:31 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/tf_no_bpe/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/tf_no_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_no_bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 00:44:31 | INFO | fairseq.tasks.translation | [fr] dictionary: 71632 types\n",
            "2023-05-01 00:44:31 | INFO | fairseq.tasks.translation | [en] dictionary: 55752 types\n",
            "2023-05-01 00:44:31 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/tf_no_bpe/checkpoint_best.pt\n",
            "2023-05-01 00:44:35 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.fr\n",
            "2023-05-01 00:44:35 | INFO | fairseq.data.data_utils | loaded 887 examples from: data-bin/iwslt13_fr_en_no_bpe/valid.fr-en.en\n",
            "2023-05-01 00:44:35 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe valid fr-en 887 examples\n",
            "2023-05-01 00:45:09 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-05-01 00:45:09 | INFO | fairseq_cli.generate | Translated 887 sentences (26,452 tokens) in 24.8s (35.79 sentences/s, 1067.21 tokens/s)\n",
            "2023-05-01 00:45:09 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-05-01 00:45:09 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-05-01 00:45:09 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_no_bpe \\\n",
        "    --path checkpoints/tf_no_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/tf_no_bpe.pred --gen-subset valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CS-19lksLOt"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp3Q7sH9sNYa",
        "outputId": "88550879-4c2f-44cb-8fa2-4e3922f9299b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-05-01 00:45:18 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/tf_no_bpe/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/tf_no_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en_no_bpe', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-05-01 00:45:19 | INFO | fairseq.tasks.translation | [fr] dictionary: 71632 types\n",
            "2023-05-01 00:45:19 | INFO | fairseq.tasks.translation | [en] dictionary: 55752 types\n",
            "2023-05-01 00:45:19 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/tf_no_bpe/checkpoint_best.pt\n",
            "2023-05-01 00:45:22 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_no_bpe/test.fr-en.fr\n",
            "2023-05-01 00:45:22 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en_no_bpe/test.fr-en.en\n",
            "2023-05-01 00:45:22 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en_no_bpe test fr-en 1664 examples\n",
            "2023-05-01 00:46:02 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-05-01 00:46:02 | INFO | fairseq_cli.generate | Translated 1,664 sentences (43,049 tokens) in 29.8s (55.92 sentences/s, 1446.67 tokens/s)\n",
            "2023-05-01 00:46:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-05-01 00:46:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-05-01 00:46:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en_no_bpe \\\n",
        "    --path checkpoints/tf_no_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/tf_no_bpe.pred"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
