{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4yid4cQvtqc"
      },
      "source": [
        "# Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr8nCL6C5Ct2",
        "outputId": "d06a8650-4ce4-434e-b781-732ac86885d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.12.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (1837.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.13.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp310-cp310-linux_x86_64.whl (23.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.12.1\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.12.1%2Bcu113-cp310-cp310-linux_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.12.1+cu113) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.13.1+cu113) (8.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.13.1+cu113) (3.4)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0\n",
            "    Uninstalling torch-2.0.0:\n",
            "      Successfully uninstalled torch-2.0.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.0.1+cu118\n",
            "    Uninstalling torchaudio-2.0.1+cu118:\n",
            "      Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.12.1+cu113 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.12.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.12.1+cu113 torchaudio-0.12.1+cu113 torchvision-0.13.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.12.1+cu113)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.7.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.12.1+cu113)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install fairseq\n",
        "!pip install sacremoses\n",
        "!pip install sacrebleu\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK-Xg1sUloaK"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF694Ii20-Id",
        "outputId": "07aa064f-80df-4548-8758-21e71f76a01d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tdw3DQmnlsd5"
      },
      "source": [
        "# Set Working Directory for files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XqRyY75i2-An"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "project_folder = 'hw1'\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(os.path.join(drive_path, project_folder))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7Dpvjkf4A0a",
        "outputId": "e8a5a7ba-a395-4cbd-c06c-5832181b28f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PWD:\n",
            "/content/drive/MyDrive/hw1\n",
            "List of Files:\n",
            "checkpoints  iwslt13_fr_en\t   NLP203_Spring2023_A1.pdf  prepare_data.sh\n",
            "data\t     iwslt13_fr_en_no_bpe  output_dataset.ipynb      result\n",
            "data-bin     mosesdecoder\t   prepare_data_no_bpe.sh    subword-nmt\n"
          ]
        }
      ],
      "source": [
        "print(\"PWD:\")\n",
        "!pwd\n",
        "print(\"List of Files:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Hsds3cr6TZ"
      },
      "source": [
        "# Check if CUDA available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kN7Gyyj8qY-",
        "outputId": "6276dbd5-340a-4e77-f535-dc0d1779f78d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hdvCTHEz5IFh"
      },
      "source": [
        "# Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EOmDqUP55Jb6"
      },
      "outputs": [],
      "source": [
        "!bash prepare_data.sh"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wTwNvFh15WHX"
      },
      "source": [
        "# Preprocess the data with BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fd8OOB4j5PTW"
      },
      "outputs": [],
      "source": [
        "!TEXT=iwslt13_fr_en ; mkdir -p data-bin ; fairseq-preprocess --source-lang fr --target-lang en     --trainpref $TEXT/train --validpref $TEXT/dev --testpref $TEXT/test     --destdir data-bin/iwslt13_fr_en     --workers 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzihiuxcv1KW"
      },
      "source": [
        "# Train CNN with BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ze6vhPW46AI",
        "outputId": "68433ef3-e765-4e12-a091-77ce435f8657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 07:00:15.820344: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-30 07:00:15.877208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-30 07:00:16.794747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 07:00:18 | INFO | numexpr.utils | Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
            "2023-04-30 07:00:18 | INFO | numexpr.utils | NumExpr defaulting to 8 threads.\n",
            "2023-04-30 07:00:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-04-30 07:00:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.5], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/cnn_bpe', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='nag', lr_scheduler='fixed', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='fconv', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.1, sentence_avg=False, update_freq=[1], lr=[0.5], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/cnn_bpe', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/iwslt13_fr_en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, max_source_positions=1024, max_target_positions=1024, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe=None, eval_bleu_print_samples=False, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, momentum=0.99, weight_decay=0.0, force_anneal=50, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, dropout=0.1, no_seed_provided=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_layers='[(512, 3)] * 20', decoder_embed_dim=512, decoder_embed_path=None, decoder_layers='[(512, 3)] * 20', decoder_out_embed_dim=256, decoder_attention='True', share_input_output_embed=False, _name='fconv'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [0.5]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': 50, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.5]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 07:00:21 | INFO | fairseq.tasks.translation | [fr] dictionary: 30192 types\n",
            "2023-04-30 07:00:21 | INFO | fairseq.tasks.translation | [en] dictionary: 24808 types\n",
            "2023-04-30 07:00:23 | INFO | fairseq_cli.train | FConvModel(\n",
            "  (encoder): FConvEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(30192, 512, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 512, padding_idx=1)\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0-19): 20 x None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0-19): 20 x ConvTBC(512, 1024, kernel_size=(3,), padding=(1,))\n",
            "    )\n",
            "    (fc2): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (decoder): FConvDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(24808, 512, padding_idx=1)\n",
            "    (embed_positions): LearnedPositionalEmbedding(1024, 512, padding_idx=1)\n",
            "    (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (projections): ModuleList(\n",
            "      (0-19): 20 x None\n",
            "    )\n",
            "    (convolutions): ModuleList(\n",
            "      (0-19): 20 x LinearizedConvolution(512, 1024, kernel_size=(3,), padding=(2,))\n",
            "    )\n",
            "    (attention): ModuleList(\n",
            "      (0-19): 20 x AttentionLayer(\n",
            "        (in_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (fc3): Linear(in_features=256, out_features=24808, bias=True)\n",
            "  )\n",
            ")\n",
            "2023-04-30 07:00:23 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-04-30 07:00:23 | INFO | fairseq_cli.train | model: FConvModel\n",
            "2023-04-30 07:00:23 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-04-30 07:00:23 | INFO | fairseq_cli.train | num. shared model params: 110,053,328 (num. trained: 110,053,328)\n",
            "2023-04-30 07:00:23 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-04-30 07:00:23 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.fr\n",
            "2023-04-30 07:00:23 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.en\n",
            "2023-04-30 07:00:23 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en valid fr-en 829 examples\n",
            "2023-04-30 07:00:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 07:00:30 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.564 GB ; name = NVIDIA A100-SXM4-40GB                   \n",
            "2023-04-30 07:00:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 07:00:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-04-30 07:00:30 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-04-30 07:00:30 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/cnn_bpe/checkpoint_last.pt\n",
            "2023-04-30 07:00:30 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/cnn_bpe/checkpoint_last.pt\n",
            "2023-04-30 07:00:30 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-04-30 07:00:30 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en/train.fr-en.fr\n",
            "2023-04-30 07:00:31 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en/train.fr-en.en\n",
            "2023-04-30 07:00:31 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en train fr-en 156951 examples\n",
            "2023-04-30 07:00:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 001:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:00:31 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-04-30 07:00:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1319: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
            "epoch 001:   0% 5/1348 [00:03<10:14,  2.19it/s]2023-04-30 07:00:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   1% 9/1348 [00:04<04:38,  4.80it/s]2023-04-30 07:00:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "2023-04-30 07:00:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:   1% 11/1348 [00:04<03:39,  6.10it/s]2023-04-30 07:00:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 001:   1% 13/1348 [00:04<03:04,  7.26it/s]2023-04-30 07:00:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "2023-04-30 07:00:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
            "epoch 001:   1% 19/1348 [00:05<02:27,  9.02it/s]2023-04-30 07:00:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0\n",
            "epoch 001:  43% 581/1348 [00:57<01:12, 10.63it/s, loss=8.417, nll_loss=7.451, ppl=174.97, wps=27284.2, ups=10.6, wpb=2572.8, bsz=126.3, num_updates=500, lr=0.5, gnorm=0.556, clip=100, loss_scale=1, train_wall=9, gb_free=36.6, wall=52]2023-04-30 07:01:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5\n",
            "epoch 001: 100% 1346/1348 [02:09<00:00, 10.50it/s, loss=7.249, nll_loss=6.097, ppl=68.46, wps=26473.1, ups=10.56, wpb=2507.4, bsz=98.7, num_updates=1300, lr=0.5, gnorm=1.79, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.7, wall=128]2023-04-30 07:02:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 1/12 [00:00<00:01,  7.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 4/12 [00:00<00:00, 19.12it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 8/12 [00:00<00:00, 25.46it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 12/12 [00:00<00:00, 28.34it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:02:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.534 | nll_loss 6.448 | ppl 87.28 | wps 55161.3 | wpb 1752.2 | bsz 69.1 | num_updates 1340\n",
            "2023-04-30 07:02:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1340 updates\n",
            "2023-04-30 07:02:42 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint1.pt\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
            "  warnings.warn(\n",
            "2023-04-30 07:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint1.pt\n",
            "2023-04-30 07:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint1.pt (epoch 1 @ 1340 updates, score 7.534) (writing took 8.265679448000014 seconds)\n",
            "2023-04-30 07:02:50 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-04-30 07:02:50 | INFO | train | epoch 001 | loss 8.333 | nll_loss 7.344 | ppl 162.49 | wps 25230 | ups 9.9 | wpb 2549.3 | bsz 116.8 | num_updates 1340 | lr 0.5 | gnorm 1.518 | clip 100 | loss_scale 0.5 | train_wall 127 | gb_free 36.6 | wall 140\n",
            "2023-04-30 07:02:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 002:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:02:50 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-04-30 07:02:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1346/1348 [02:08<00:00, 10.93it/s, loss=6.08, nll_loss=4.74, ppl=26.72, wps=27662.6, ups=10.77, wpb=2567.6, bsz=122.3, num_updates=2600, lr=0.5, gnorm=0.364, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.6, wall=261]2023-04-30 07:04:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 19.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 28.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 30.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:04:59 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.639 | nll_loss 5.408 | ppl 42.46 | wps 55926.8 | wpb 1752.2 | bsz 69.1 | num_updates 2688 | best_loss 6.639\n",
            "2023-04-30 07:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2688 updates\n",
            "2023-04-30 07:04:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint2.pt\n",
            "2023-04-30 07:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint2.pt\n",
            "2023-04-30 07:05:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint2.pt (epoch 2 @ 2688 updates, score 6.639) (writing took 8.198403995000035 seconds)\n",
            "2023-04-30 07:05:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-04-30 07:05:08 | INFO | train | epoch 002 | loss 6.462 | nll_loss 5.184 | ppl 36.35 | wps 25005.9 | ups 9.81 | wpb 2548.6 | bsz 116.4 | num_updates 2688 | lr 0.5 | gnorm 0.604 | clip 100 | loss_scale 0.5 | train_wall 126 | gb_free 36.8 | wall 278\n",
            "2023-04-30 07:05:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 003:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:05:08 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-04-30 07:05:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1346/1348 [02:07<00:00, 10.91it/s, loss=5.533, nll_loss=4.101, ppl=17.16, wps=27361.4, ups=10.73, wpb=2551, bsz=115.1, num_updates=4000, lr=0.5, gnorm=0.342, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.5, wall=403]2023-04-30 07:07:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 18.88it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 28.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 30.69it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:07:16 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 6.194 | nll_loss 4.862 | ppl 29.09 | wps 56855.2 | wpb 1752.2 | bsz 69.1 | num_updates 4036 | best_loss 6.194\n",
            "2023-04-30 07:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4036 updates\n",
            "2023-04-30 07:07:16 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint3.pt\n",
            "2023-04-30 07:07:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint3.pt\n",
            "2023-04-30 07:07:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint3.pt (epoch 3 @ 4036 updates, score 6.194) (writing took 7.962952962000031 seconds)\n",
            "2023-04-30 07:07:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-04-30 07:07:24 | INFO | train | epoch 003 | loss 5.698 | nll_loss 4.292 | ppl 19.59 | wps 25160.9 | ups 9.87 | wpb 2548.6 | bsz 116.4 | num_updates 4036 | lr 0.5 | gnorm 0.353 | clip 100 | loss_scale 0.5 | train_wall 125 | gb_free 36.7 | wall 414\n",
            "2023-04-30 07:07:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 004:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:07:24 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-04-30 07:07:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1347/1348 [02:08<00:00, 10.86it/s, loss=5.174, nll_loss=3.68, ppl=12.82, wps=27197.7, ups=10.66, wpb=2550.3, bsz=116.6, num_updates=5300, lr=0.5, gnorm=0.315, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.8, wall=535]2023-04-30 07:09:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 17.52it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 26.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 29.32it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:09:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.885 | nll_loss 4.488 | ppl 22.45 | wps 55721.4 | wpb 1752.2 | bsz 69.1 | num_updates 5384 | best_loss 5.885\n",
            "2023-04-30 07:09:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5384 updates\n",
            "2023-04-30 07:09:33 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint4.pt\n",
            "2023-04-30 07:09:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint4.pt\n",
            "2023-04-30 07:09:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint4.pt (epoch 4 @ 5384 updates, score 5.885) (writing took 9.125865530000056 seconds)\n",
            "2023-04-30 07:09:42 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-04-30 07:09:42 | INFO | train | epoch 004 | loss 5.231 | nll_loss 3.742 | ppl 13.38 | wps 24842.2 | ups 9.75 | wpb 2548.6 | bsz 116.4 | num_updates 5384 | lr 0.5 | gnorm 0.339 | clip 100 | loss_scale 0.5 | train_wall 126 | gb_free 36.6 | wall 553\n",
            "2023-04-30 07:09:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 005:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:09:42 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-04-30 07:09:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1346/1348 [02:08<00:00, 11.00it/s, loss=4.925, nll_loss=3.386, ppl=10.46, wps=28019.6, ups=10.79, wpb=2596.2, bsz=115.2, num_updates=6700, lr=0.5, gnorm=0.312, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.6, wall=678]2023-04-30 07:11:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 18.83it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 27.06it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 29.98it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:11:51 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.702 | nll_loss 4.284 | ppl 19.48 | wps 55406.7 | wpb 1752.2 | bsz 69.1 | num_updates 6732 | best_loss 5.702\n",
            "2023-04-30 07:11:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6732 updates\n",
            "2023-04-30 07:11:51 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint5.pt\n",
            "2023-04-30 07:11:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint5.pt\n",
            "2023-04-30 07:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint5.pt (epoch 5 @ 6732 updates, score 5.702) (writing took 8.159937652000053 seconds)\n",
            "2023-04-30 07:11:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-04-30 07:11:59 | INFO | train | epoch 005 | loss 4.901 | nll_loss 3.355 | ppl 10.23 | wps 25077.4 | ups 9.84 | wpb 2548.6 | bsz 116.4 | num_updates 6732 | lr 0.5 | gnorm 0.318 | clip 100 | loss_scale 0.5 | train_wall 125 | gb_free 36.7 | wall 690\n",
            "2023-04-30 07:11:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 006:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:11:59 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-04-30 07:11:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 1346/1348 [02:07<00:00, 11.27it/s, loss=4.659, nll_loss=3.076, ppl=8.43, wps=27629, ups=10.77, wpb=2565.3, bsz=125.8, num_updates=8000, lr=0.5, gnorm=0.293, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.6, wall=810]2023-04-30 07:14:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 18.39it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 27.46it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 30.66it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:14:08 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.592 | nll_loss 4.107 | ppl 17.23 | wps 56522.3 | wpb 1752.2 | bsz 69.1 | num_updates 8080 | best_loss 5.592\n",
            "2023-04-30 07:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8080 updates\n",
            "2023-04-30 07:14:08 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint6.pt\n",
            "2023-04-30 07:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint6.pt\n",
            "2023-04-30 07:14:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint6.pt (epoch 6 @ 8080 updates, score 5.592) (writing took 8.027978421999933 seconds)\n",
            "2023-04-30 07:14:16 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-04-30 07:14:16 | INFO | train | epoch 006 | loss 4.668 | nll_loss 3.081 | ppl 8.46 | wps 25176.2 | ups 9.88 | wpb 2548.6 | bsz 116.4 | num_updates 8080 | lr 0.5 | gnorm 0.306 | clip 100 | loss_scale 0.5 | train_wall 125 | gb_free 36.5 | wall 826\n",
            "2023-04-30 07:14:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 007:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:14:16 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-04-30 07:14:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 1347/1348 [02:08<00:00, 11.29it/s, loss=4.498, nll_loss=2.887, ppl=7.4, wps=27625.5, ups=10.68, wpb=2585.7, bsz=120.5, num_updates=9400, lr=0.5, gnorm=0.288, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.6, wall=952]2023-04-30 07:16:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 18.13it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 27.74it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 30.50it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:16:25 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.481 | nll_loss 4.022 | ppl 16.24 | wps 56681.7 | wpb 1752.2 | bsz 69.1 | num_updates 9428 | best_loss 5.481\n",
            "2023-04-30 07:16:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 9428 updates\n",
            "2023-04-30 07:16:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint7.pt\n",
            "2023-04-30 07:16:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint7.pt\n",
            "2023-04-30 07:16:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint7.pt (epoch 7 @ 9428 updates, score 5.481) (writing took 9.418512890999864 seconds)\n",
            "2023-04-30 07:16:34 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-04-30 07:16:34 | INFO | train | epoch 007 | loss 4.49 | nll_loss 2.872 | ppl 7.32 | wps 24832.8 | ups 9.74 | wpb 2548.6 | bsz 116.4 | num_updates 9428 | lr 0.5 | gnorm 0.297 | clip 100 | loss_scale 0.5 | train_wall 125 | gb_free 36.5 | wall 964\n",
            "2023-04-30 07:16:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 008:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:16:34 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-04-30 07:16:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 1347/1348 [02:07<00:00, 11.15it/s, loss=4.423, nll_loss=2.793, ppl=6.93, wps=26929.1, ups=10.66, wpb=2525.8, bsz=103.2, num_updates=10700, lr=0.5, gnorm=0.298, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.7, wall=1085]2023-04-30 07:18:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 17.39it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 27.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 30.49it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:18:43 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.434 | nll_loss 3.944 | ppl 15.39 | wps 56571.9 | wpb 1752.2 | bsz 69.1 | num_updates 10776 | best_loss 5.434\n",
            "2023-04-30 07:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10776 updates\n",
            "2023-04-30 07:18:43 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint8.pt\n",
            "2023-04-30 07:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint8.pt\n",
            "2023-04-30 07:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint8.pt (epoch 8 @ 10776 updates, score 5.434) (writing took 8.646810469999991 seconds)\n",
            "2023-04-30 07:18:51 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-04-30 07:18:51 | INFO | train | epoch 008 | loss 4.34 | nll_loss 2.697 | ppl 6.49 | wps 25060 | ups 9.83 | wpb 2548.6 | bsz 116.4 | num_updates 10776 | lr 0.5 | gnorm 0.29 | clip 100 | loss_scale 0.5 | train_wall 125 | gb_free 36.6 | wall 1101\n",
            "2023-04-30 07:18:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 009:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:18:51 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-04-30 07:18:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 1347/1348 [02:07<00:00, 11.04it/s, loss=4.246, nll_loss=2.589, ppl=6.02, wps=27340.1, ups=10.75, wpb=2542.3, bsz=117.2, num_updates=12100, lr=0.5, gnorm=0.288, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.5, wall=1227]2023-04-30 07:20:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 3/12 [00:00<00:00, 22.52it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  58% 7/12 [00:00<00:00, 29.01it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  92% 11/12 [00:00<00:00, 31.08it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:20:59 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.412 | nll_loss 3.917 | ppl 15.11 | wps 56788.1 | wpb 1752.2 | bsz 69.1 | num_updates 12124 | best_loss 5.412\n",
            "2023-04-30 07:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12124 updates\n",
            "2023-04-30 07:20:59 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint9.pt\n",
            "2023-04-30 07:21:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint9.pt\n",
            "2023-04-30 07:21:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint9.pt (epoch 9 @ 12124 updates, score 5.412) (writing took 8.21810991800021 seconds)\n",
            "2023-04-30 07:21:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-04-30 07:21:07 | INFO | train | epoch 009 | loss 4.208 | nll_loss 2.542 | ppl 5.83 | wps 25274.1 | ups 9.92 | wpb 2548.6 | bsz 116.4 | num_updates 12124 | lr 0.5 | gnorm 0.283 | clip 100 | loss_scale 0.5 | train_wall 124 | gb_free 36.6 | wall 1237\n",
            "2023-04-30 07:21:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 010:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 07:21:07 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-04-30 07:21:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 1346/1348 [02:07<00:00, 10.70it/s, loss=4.135, nll_loss=2.461, ppl=5.51, wps=27297.4, ups=10.77, wpb=2533.4, bsz=115.3, num_updates=13400, lr=0.5, gnorm=0.279, clip=100, loss_scale=0.5, train_wall=9, gb_free=36.6, wall=1358]2023-04-30 07:23:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 2/12 [00:00<00:00, 18.10it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 6/12 [00:00<00:00, 27.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 10/12 [00:00<00:00, 30.19it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 07:23:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.362 | nll_loss 3.842 | ppl 14.34 | wps 55496.9 | wpb 1752.2 | bsz 69.1 | num_updates 13472 | best_loss 5.362\n",
            "2023-04-30 07:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 13472 updates\n",
            "2023-04-30 07:23:15 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint10.pt\n",
            "2023-04-30 07:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/cnn_bpe/checkpoint10.pt\n",
            "2023-04-30 07:23:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/cnn_bpe/checkpoint10.pt (epoch 10 @ 13472 updates, score 5.362) (writing took 9.319966986000054 seconds)\n",
            "2023-04-30 07:23:24 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-04-30 07:23:24 | INFO | train | epoch 010 | loss 4.095 | nll_loss 2.41 | ppl 5.31 | wps 25055 | ups 9.83 | wpb 2548.6 | bsz 116.4 | num_updates 13472 | lr 0.5 | gnorm 0.28 | clip 100 | loss_scale 0.5 | train_wall 124 | gb_free 36.7 | wall 1375\n",
            "2023-04-30 07:23:24 | INFO | fairseq_cli.train | done training in 1373.0 seconds\n"
          ]
        }
      ],
      "source": [
        "!fairseq-train \\\n",
        "    data-bin/iwslt13_fr_en \\\n",
        "    --arch fconv \\\n",
        "    --dropout 0.1 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --optimizer nag --clip-norm 0.1 \\\n",
        "    --lr 0.5 --lr-scheduler fixed --force-anneal 50 \\\n",
        "    --max-tokens 3000 \\\n",
        "    --save-dir checkpoints/cnn_bpe \\\n",
        "    --fp16 --patience 10 \\\n",
        "    --max-epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdQUH-rDv5RU"
      },
      "source": [
        "# Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEIEY7W-qo5k",
        "outputId": "1b77ab0f-9415-4944-ae8d-f4567dfa26dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 07:23:39.068510: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-30 07:23:39.123649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-30 07:23:40.043638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 07:23:42 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-04-30 07:23:43 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cnn_bpe/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/cnn_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 07:23:44 | INFO | fairseq.tasks.translation | [fr] dictionary: 30192 types\n",
            "2023-04-30 07:23:44 | INFO | fairseq.tasks.translation | [en] dictionary: 24808 types\n",
            "2023-04-30 07:23:44 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cnn_bpe/checkpoint_best.pt\n",
            "2023-04-30 07:23:47 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.fr\n",
            "2023-04-30 07:23:47 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.en\n",
            "2023-04-30 07:23:47 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en valid fr-en 829 examples\n",
            "2023-04-30 07:24:10 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-04-30 07:24:10 | INFO | fairseq_cli.generate | Translated 829 sentences (19,972 tokens) in 15.0s (55.36 sentences/s, 1333.65 tokens/s)\n",
            "2023-04-30 07:24:10 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-04-30 07:24:10 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-04-30 07:24:10 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en \\\n",
        "    --path checkpoints/cnn_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --remove-bpe \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/cnn_bpe.pred --gen-subset valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBioU9aMv9rz"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma3BlxvnsFtu",
        "outputId": "f47723e4-e79f-475b-d8b1-62ac75bc29ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 07:24:18.423149: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-04-30 07:24:18.479400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-30 07:24:19.393470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-30 07:24:21 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-04-30 07:24:23 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/cnn_bpe/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/cnn_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 07:24:23 | INFO | fairseq.tasks.translation | [fr] dictionary: 30192 types\n",
            "2023-04-30 07:24:23 | INFO | fairseq.tasks.translation | [en] dictionary: 24808 types\n",
            "2023-04-30 07:24:23 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/cnn_bpe/checkpoint_best.pt\n",
            "2023-04-30 07:24:26 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en/test.fr-en.fr\n",
            "2023-04-30 07:24:26 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en/test.fr-en.en\n",
            "2023-04-30 07:24:26 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en test fr-en 1664 examples\n",
            "2023-04-30 07:24:59 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-04-30 07:24:59 | INFO | fairseq_cli.generate | Translated 1,664 sentences (35,290 tokens) in 21.3s (77.98 sentences/s, 1653.87 tokens/s)\n",
            "2023-04-30 07:24:59 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-04-30 07:24:59 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-04-30 07:24:59 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en \\\n",
        "    --path checkpoints/cnn_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --remove-bpe \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/cnn_bpe.pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyrV8q0-wBNF"
      },
      "source": [
        "# Train Transformer with BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM29Uqpct9u8",
        "outputId": "751e04b8-1327-4156-977d-9ce37ba95f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 20:34:31 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/tf_bpe', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 10, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='label_smoothed_cross_entropy', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', scoring='bleu', task='translation', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=3000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=3000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='transformer', max_epoch=10, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='checkpoints/tf_bpe', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='bleu', maximize_best_checkpoint_metric=True, patience=10, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, data='data-bin/iwslt13_fr_en', source_lang=None, target_lang=None, load_alignments=False, left_pad_source=True, left_pad_target=False, upsample_primary=-1, truncate_source=False, num_batch_buckets=0, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_tokenized_bleu=False, eval_bleu_remove_bpe='@@ ', eval_bleu_print_samples=True, label_smoothing=0.1, report_accuracy=False, ignore_prefix_size=0, adam_betas='(0.9, 0.98)', adam_eps=1e-08, weight_decay=0.0001, use_old_adam=False, fp16_adam_stats=False, warmup_updates=4000, warmup_init_lr=-1, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.3, no_seed_provided=False, encoder_embed_path=None, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_attention_heads=8, encoder_normalize_before=False, encoder_learned_pos=False, decoder_embed_path=None, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=False, decoder_learned_pos=False, attention_dropout=0.0, activation_dropout=0.0, activation_fn='relu', adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, share_all_embeddings=False, no_token_positional_embeddings=False, adaptive_input=False, no_cross_attention=False, cross_self_attention=False, decoder_output_dim=512, decoder_input_dim=512, no_scale_embedding=False, layernorm_embedding=False, tie_adaptive_weights=False, checkpoint_activations=False, offload_activations=False, encoder_layers_to_keep=None, decoder_layers_to_keep=None, encoder_layerdrop=0, decoder_layerdrop=0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, _name='transformer'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 20:34:31 | INFO | fairseq.tasks.translation | [fr] dictionary: 30192 types\n",
            "2023-04-30 20:34:31 | INFO | fairseq.tasks.translation | [en] dictionary: 24808 types\n",
            "2023-04-30 20:34:32 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(30192, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(24808, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=24808, bias=False)\n",
            "  )\n",
            ")\n",
            "2023-04-30 20:34:32 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-04-30 20:34:32 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2023-04-30 20:34:32 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2023-04-30 20:34:32 | INFO | fairseq_cli.train | num. shared model params: 91,200,512 (num. trained: 91,200,512)\n",
            "2023-04-30 20:34:32 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-04-30 20:34:32 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.fr\n",
            "2023-04-30 20:34:32 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.en\n",
            "2023-04-30 20:34:32 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en valid fr-en 829 examples\n",
            "2023-04-30 20:34:34 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2023-04-30 20:34:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 20:34:34 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 39.564 GB ; name = NVIDIA A100-SXM4-40GB                   \n",
            "2023-04-30 20:34:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-04-30 20:34:34 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-04-30 20:34:34 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-04-30 20:34:34 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/tf_bpe/checkpoint_last.pt\n",
            "2023-04-30 20:34:34 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/tf_bpe/checkpoint_last.pt\n",
            "2023-04-30 20:34:34 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-04-30 20:34:34 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en/train.fr-en.fr\n",
            "2023-04-30 20:34:34 | INFO | fairseq.data.data_utils | loaded 156,951 examples from: data-bin/iwslt13_fr_en/train.fr-en.en\n",
            "2023-04-30 20:34:34 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en train fr-en 156951 examples\n",
            "2023-04-30 20:34:34 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-04-30 20:34:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 001:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:34:34 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-04-30 20:34:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1347/1348 [02:25<00:00,  9.74it/s, loss=8.567, nll_loss=7.617, ppl=196.29, wps=23623.6, ups=9.39, wpb=2515.8, bsz=100, num_updates=1300, lr=0.0001625, gnorm=1.58, train_wall=10, gb_free=36.9, wall=140]2023-04-30 20:36:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:37:00 | INFO | fairseq.tasks.translation | example hypothesis: It's a lot of it.\n",
            "2023-04-30 20:37:00 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   8% 1/12 [00:00<00:09,  1.19it/s]\u001b[A2023-04-30 20:37:01 | INFO | fairseq.tasks.translation | example hypothesis: And what you're going to do is what you're going to do.\n",
            "2023-04-30 20:37:01 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  17% 2/12 [00:01<00:07,  1.35it/s]\u001b[A2023-04-30 20:37:01 | INFO | fairseq.tasks.translation | example hypothesis: It's not not not not a lot of a lot of a lot of a lot of a lot of a lot of a lot of it.\n",
            "2023-04-30 20:37:01 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  25% 3/12 [00:02<00:06,  1.39it/s]\u001b[A2023-04-30 20:37:02 | INFO | fairseq.tasks.translation | example hypothesis: So I'm going to do this is a little bit of a lot of a little bit of a lot of a little bit.\n",
            "2023-04-30 20:37:02 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  33% 4/12 [00:02<00:05,  1.38it/s]\u001b[A2023-04-30 20:37:03 | INFO | fairseq.tasks.translation | example hypothesis: You know, \"You know, you know, you know, you know, you know, you know, you're going to do it?\"\n",
            "2023-04-30 20:37:03 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  42% 5/12 [00:03<00:05,  1.35it/s]\u001b[A2023-04-30 20:37:04 | INFO | fairseq.tasks.translation | example hypothesis: And I'm going to do that I'm going to do this is that I'm going to be going to do that I'm going to be going to tell you.\n",
            "2023-04-30 20:37:04 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  50% 6/12 [00:04<00:04,  1.28it/s]\u001b[A2023-04-30 20:37:05 | INFO | fairseq.tasks.translation | example hypothesis: It's not not a lot of people, or not a lot of the world, or or or or the world.\n",
            "2023-04-30 20:37:05 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  58% 7/12 [00:05<00:04,  1.21it/s]\u001b[A2023-04-30 20:37:06 | INFO | fairseq.tasks.translation | example hypothesis: We're going to do a lot of the world that we're going to make a lot of the world.\n",
            "2023-04-30 20:37:06 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  67% 8/12 [00:06<00:03,  1.12it/s]\u001b[A2023-04-30 20:37:07 | INFO | fairseq.tasks.translation | example hypothesis: They had a lot of the world, and they had a lot of the world, and they had a lot of the world.\n",
            "2023-04-30 20:37:07 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  75% 9/12 [00:07<00:02,  1.03it/s]\u001b[A2023-04-30 20:37:08 | INFO | fairseq.tasks.translation | example hypothesis: You know, in the world, in the world, in the world in the world, in the world, and the world is the world in the world in the world in the world in the world.\n",
            "2023-04-30 20:37:08 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  83% 10/12 [00:09<00:02,  1.08s/it]\u001b[A2023-04-30 20:37:10 | INFO | fairseq.tasks.translation | example hypothesis: And in fact, we're going to be a lot of the world, in the world, in the world that we're going to be a lot of the world, in the world that we're going to do that we're going to do in the world, and then we're going to do that we're going to be in the world in the world in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world in the world, in the world, in the world that we're in the world, in the world, in the world,\n",
            "2023-04-30 20:37:10 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:  92% 11/12 [00:10<00:01,  1.30s/it]\u001b[A2023-04-30 20:37:13 | INFO | fairseq.tasks.translation | example hypothesis: And in the world is that you're going to see the world in the world, and then you're going to see the world in the world in the world, and then you're going to get the world.\n",
            "2023-04-30 20:37:13 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 001 | valid on 'valid' subset: 100% 12/12 [00:13<00:00,  1.73s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:37:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.556 | nll_loss 7.563 | ppl 189.16 | bleu 1.46 | wps 1525.2 | wpb 1752.2 | bsz 69.1 | num_updates 1348\n",
            "2023-04-30 20:37:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1348 updates\n",
            "2023-04-30 20:37:13 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint1.pt\n",
            "2023-04-30 20:37:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint1.pt\n",
            "2023-04-30 20:37:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint1.pt (epoch 1 @ 1348 updates, score 1.46) (writing took 9.377683608999632 seconds)\n",
            "2023-04-30 20:37:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-04-30 20:37:22 | INFO | train | epoch 001 | loss 9.744 | nll_loss 8.984 | ppl 506.4 | wps 20576.4 | ups 8.07 | wpb 2548.6 | bsz 116.4 | num_updates 1348 | lr 0.0001685 | gnorm 2.079 | train_wall 142 | gb_free 36.7 | wall 168\n",
            "2023-04-30 20:37:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 002:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:37:22 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-04-30 20:37:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1347/1348 [02:24<00:00,  9.46it/s, loss=7.577, nll_loss=6.49, ppl=89.88, wps=24066, ups=9.37, wpb=2568.7, bsz=121.8, num_updates=2600, lr=0.000325, gnorm=1.286, train_wall=10, gb_free=37, wall=303]2023-04-30 20:39:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:39:48 | INFO | fairseq.tasks.translation | example hypothesis: It's like a child.\n",
            "2023-04-30 20:39:48 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   8% 1/12 [00:00<00:07,  1.44it/s]\u001b[A2023-04-30 20:39:48 | INFO | fairseq.tasks.translation | example hypothesis: And you want to be able to be able to be able to be able to be able to be able.\n",
            "2023-04-30 20:39:48 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  17% 2/12 [00:01<00:06,  1.48it/s]\u001b[A2023-04-30 20:39:49 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't have a lot of it.\n",
            "2023-04-30 20:39:49 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  25% 3/12 [00:02<00:06,  1.47it/s]\u001b[A2023-04-30 20:39:50 | INFO | fairseq.tasks.translation | example hypothesis: So I'm a little bit of a little bit of this.\n",
            "2023-04-30 20:39:50 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  33% 4/12 [00:02<00:05,  1.43it/s]\u001b[A2023-04-30 20:39:51 | INFO | fairseq.tasks.translation | example hypothesis: You know, \"You know,\" You know, what? \"\n",
            "2023-04-30 20:39:51 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  42% 5/12 [00:03<00:05,  1.38it/s]\u001b[A2023-04-30 20:39:52 | INFO | fairseq.tasks.translation | example hypothesis: He said, \"I'm going to be going to be going to go back and go back and go back to the next time.\"\n",
            "2023-04-30 20:39:52 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  50% 6/12 [00:04<00:04,  1.29it/s]\u001b[A2023-04-30 20:39:52 | INFO | fairseq.tasks.translation | example hypothesis: The number of those who are not going to be able to be able to be able to be able to be able to be able to be in the middle of the United States, which will be a lot of people who are going to be able to be able to be going\n",
            "2023-04-30 20:39:52 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  58% 7/12 [00:05<00:04,  1.21it/s]\u001b[A2023-04-30 20:39:54 | INFO | fairseq.tasks.translation | example hypothesis: We have a lot of ways that we're trying to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able.\n",
            "2023-04-30 20:39:54 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  67% 8/12 [00:06<00:03,  1.12it/s]\u001b[A2023-04-30 20:39:55 | INFO | fairseq.tasks.translation | example hypothesis: They had the same thing that they had to do with the same thing, and they had a little bit of the same thing that they had to do, they had to go out, and they had a little bit of them.\n",
            "2023-04-30 20:39:55 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  75% 9/12 [00:07<00:02,  1.03it/s]\u001b[A2023-04-30 20:39:56 | INFO | fairseq.tasks.translation | example hypothesis: You know, in the middle of the middle of the middle of the middle of the middle of the world, and you know, which is not a group of people who don't have a group of people who don't have a child in the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the world,\n",
            "2023-04-30 20:39:56 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  83% 10/12 [00:08<00:02,  1.09s/it]\u001b[A2023-04-30 20:39:58 | INFO | fairseq.tasks.translation | example hypothesis: And in the world, we've got to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to\n",
            "2023-04-30 20:39:58 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:  92% 11/12 [00:10<00:01,  1.30s/it]\u001b[A2023-04-30 20:40:01 | INFO | fairseq.tasks.translation | example hypothesis: And when you look at the middle of the world, you're seeing the world, you're going to see the world in the world, and you're going to see the United States in the world, which is the United States of the world, which is the United States, which is the United States that you're going to go from the world.\n",
            "2023-04-30 20:40:01 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 002 | valid on 'valid' subset: 100% 12/12 [00:13<00:00,  1.75s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:40:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.774 | nll_loss 6.7 | ppl 103.96 | bleu 3.03 | wps 1518.3 | wpb 1752.2 | bsz 69.1 | num_updates 2696 | best_bleu 3.03\n",
            "2023-04-30 20:40:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2696 updates\n",
            "2023-04-30 20:40:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint2.pt\n",
            "2023-04-30 20:40:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint2.pt\n",
            "2023-04-30 20:40:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint2.pt (epoch 2 @ 2696 updates, score 3.03) (writing took 10.072243043000071 seconds)\n",
            "2023-04-30 20:40:11 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-04-30 20:40:11 | INFO | train | epoch 002 | loss 7.931 | nll_loss 6.892 | ppl 118.77 | wps 20378.4 | ups 8 | wpb 2548.6 | bsz 116.4 | num_updates 2696 | lr 0.000337 | gnorm 1.413 | train_wall 142 | gb_free 37 | wall 337\n",
            "2023-04-30 20:40:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 003:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:40:11 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-04-30 20:40:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003: 100% 1347/1348 [02:25<00:00,  9.52it/s, loss=6.843, nll_loss=5.651, ppl=50.24, wps=23613.2, ups=9.38, wpb=2517.6, bsz=113, num_updates=4000, lr=0.0005, gnorm=1.396, train_wall=10, gb_free=37, wall=478]2023-04-30 20:42:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:42:37 | INFO | fairseq.tasks.translation | example hypothesis: It's like a person.\n",
            "2023-04-30 20:42:37 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   8% 1/12 [00:00<00:07,  1.47it/s]\u001b[A2023-04-30 20:42:38 | INFO | fairseq.tasks.translation | example hypothesis: And what you're going to be going to be able to be able to be able to be able to talk about his own own own language.\n",
            "2023-04-30 20:42:38 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  17% 2/12 [00:01<00:06,  1.47it/s]\u001b[A2023-04-30 20:42:39 | INFO | fairseq.tasks.translation | example hypothesis: It's not a non-law, but it's really a really good solution.\n",
            "2023-04-30 20:42:39 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  25% 3/12 [00:02<00:06,  1.45it/s]\u001b[A2023-04-30 20:42:39 | INFO | fairseq.tasks.translation | example hypothesis: So, I'm going to -- I'm one of that I'm doing with this one I do with a simple program.\n",
            "2023-04-30 20:42:39 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  33% 4/12 [00:02<00:05,  1.40it/s]\u001b[A2023-04-30 20:42:40 | INFO | fairseq.tasks.translation | example hypothesis: So let's tell you, you know, this one, and it's like a person, \"What does it mean?\"\n",
            "2023-04-30 20:42:40 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  42% 5/12 [00:03<00:05,  1.32it/s]\u001b[A2023-04-30 20:42:41 | INFO | fairseq.tasks.translation | example hypothesis: He started at the old age of the old year, and I'm going to be going to be going to be able to be this song.\n",
            "2023-04-30 20:42:41 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  50% 6/12 [00:04<00:04,  1.26it/s]\u001b[A2023-04-30 20:42:42 | INFO | fairseq.tasks.translation | example hypothesis: The second generation and the third generation are not going to be going to go back, which is a lot of people, which are much more likely to be a lot of different or a lot of different.\n",
            "2023-04-30 20:42:42 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  58% 7/12 [00:05<00:04,  1.21it/s]\u001b[A2023-04-30 20:42:43 | INFO | fairseq.tasks.translation | example hypothesis: When you're going to look at the most important things we can't do in a little bit more than we can do in a little bit of biology.\n",
            "2023-04-30 20:42:43 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  67% 8/12 [00:06<00:03,  1.13it/s]\u001b[A2023-04-30 20:42:44 | INFO | fairseq.tasks.translation | example hypothesis: They have these pictures on the planet, and I tried to look at the face of the face of a face, and they tried to see that it was not just one, and they found that it was not just a little bit of paper.\n",
            "2023-04-30 20:42:44 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  75% 9/12 [00:07<00:02,  1.03it/s]\u001b[A2023-04-30 20:42:45 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the technologies and a team and a team of these technologies and a community that can live in the community, which is not in the middle of those areas, not only in the middle of the middle of the middle of the middle of the middle of the middle of the trees, which don't have the trees, which are not in the trees, which are not in the trees, which are not in the trees.\n",
            "2023-04-30 20:42:45 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  83% 10/12 [00:08<00:02,  1.09s/it]\u001b[A2023-04-30 20:42:47 | INFO | fairseq.tasks.translation | example hypothesis: And we're doing things where we're in the Middle East where we're not going to live in the world in a world where they can be in a world, and they can also have a sense of the world, so they can actually be a sense of the world.\n",
            "2023-04-30 20:42:47 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:  92% 11/12 [00:10<00:01,  1.30s/it]\u001b[A2023-04-30 20:42:50 | INFO | fairseq.tasks.translation | example hypothesis: And when you look at the world, you're not the world, and you're not going to be in the United States, where you can see a million people who are living in the United States, which is where you can see in the United States.\n",
            "2023-04-30 20:42:50 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 003 | valid on 'valid' subset: 100% 12/12 [00:13<00:00,  1.74s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:42:50 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.093 | nll_loss 5.863 | ppl 58.2 | bleu 8.28 | wps 1514 | wpb 1752.2 | bsz 69.1 | num_updates 4044 | best_bleu 8.28\n",
            "2023-04-30 20:42:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4044 updates\n",
            "2023-04-30 20:42:50 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint3.pt\n",
            "2023-04-30 20:42:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint3.pt\n",
            "2023-04-30 20:43:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint3.pt (epoch 3 @ 4044 updates, score 8.28) (writing took 11.51466213599997 seconds)\n",
            "2023-04-30 20:43:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-04-30 20:43:01 | INFO | train | epoch 003 | loss 7.157 | nll_loss 6.011 | ppl 64.48 | wps 20111.8 | ups 7.89 | wpb 2548.6 | bsz 116.4 | num_updates 4044 | lr 0.000497272 | gnorm 1.343 | train_wall 142 | gb_free 36.9 | wall 508\n",
            "2023-04-30 20:43:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 004:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:43:02 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-04-30 20:43:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004: 100% 1347/1348 [02:25<00:00,  9.52it/s, loss=5.942, nll_loss=4.627, ppl=24.71, wps=23942.7, ups=9.4, wpb=2548, bsz=118.2, num_updates=5300, lr=0.000434372, gnorm=1.266, train_wall=10, gb_free=37, wall=643]2023-04-30 20:45:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:45:28 | INFO | fairseq.tasks.translation | example hypothesis: It's like to ask someone to the sun's sun.\n",
            "2023-04-30 20:45:28 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   8% 1/12 [00:00<00:07,  1.54it/s]\u001b[A2023-04-30 20:45:28 | INFO | fairseq.tasks.translation | example hypothesis: And you're looking at what it would be to be able to talk about its own language.\n",
            "2023-04-30 20:45:28 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  17% 2/12 [00:01<00:06,  1.62it/s]\u001b[A2023-04-30 20:45:29 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't affect the reality of reality; but rather the solution of the reality.\n",
            "2023-04-30 20:45:29 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  25% 3/12 [00:01<00:05,  1.63it/s]\u001b[A2023-04-30 20:45:29 | INFO | fairseq.tasks.translation | example hypothesis: So here I'm -- this is a point of a storyteller that I'm doing with a simple piece of equation.\n",
            "2023-04-30 20:45:29 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  33% 4/12 [00:02<00:05,  1.53it/s]\u001b[A2023-04-30 20:45:30 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and as a person, say, \"What about this fish?\n",
            "2023-04-30 20:45:30 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  42% 5/12 [00:03<00:04,  1.46it/s]\u001b[A2023-04-30 20:45:31 | INFO | fairseq.tasks.translation | example hypothesis: He started to feel like the old old old boy in him: I'm going to lose it and I'm going to lose it through this song.\n",
            "2023-04-30 20:45:31 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  50% 6/12 [00:04<00:04,  1.37it/s]\u001b[A2023-04-30 20:45:32 | INFO | fairseq.tasks.translation | example hypothesis: The second rate of the second generation and generations are not going to turn up to a lot of sugar, which will be much more powerful as the kinds of types of different.\n",
            "2023-04-30 20:45:32 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.35it/s]\u001b[A2023-04-30 20:45:33 | INFO | fairseq.tasks.translation | example hypothesis: So when you're looking at some important pieces of pieces of pieces, we can't grow them into a bacterium.\n",
            "2023-04-30 20:45:33 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  67% 8/12 [00:05<00:03,  1.22it/s]\u001b[A2023-04-30 20:45:34 | INFO | fairseq.tasks.translation | example hypothesis: They took these pictures by land, tried to look behind the face of a form, a combination of paper, so it didn't have the five of the lines of the Earth.\n",
            "2023-04-30 20:45:34 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.13it/s]\u001b[A2023-04-30 20:45:35 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the deep-based, and one of those of those of the people in the forest is the ability to live in the plants.\n",
            "2023-04-30 20:45:35 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  83% 10/12 [00:08<00:01,  1.04it/s]\u001b[A2023-04-30 20:45:36 | INFO | fairseq.tasks.translation | example hypothesis: And so we're doing the limits of the sphere where we're putting our public places in places like this.\n",
            "2023-04-30 20:45:36 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:  92% 11/12 [00:09<00:01,  1.03s/it]\u001b[A2023-04-30 20:45:37 | INFO | fairseq.tasks.translation | example hypothesis: And when you look in the world, you find that these are non-cultures. These are the people of the South Bronx, which is a lot of people who live in the South Korea.\n",
            "2023-04-30 20:45:37 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 004 | valid on 'valid' subset: 100% 12/12 [00:09<00:00,  1.05it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:45:37 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.435 | nll_loss 5.076 | ppl 33.74 | bleu 14.57 | wps 2069.9 | wpb 1752.2 | bsz 69.1 | num_updates 5392 | best_bleu 14.57\n",
            "2023-04-30 20:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5392 updates\n",
            "2023-04-30 20:45:37 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint4.pt\n",
            "2023-04-30 20:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint4.pt\n",
            "2023-04-30 20:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint4.pt (epoch 4 @ 5392 updates, score 14.57) (writing took 10.687705273999654 seconds)\n",
            "2023-04-30 20:45:48 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-04-30 20:45:48 | INFO | train | epoch 004 | loss 6.263 | nll_loss 4.993 | ppl 31.84 | wps 20684.5 | ups 8.12 | wpb 2548.6 | bsz 116.4 | num_updates 5392 | lr 0.000430651 | gnorm 1.374 | train_wall 142 | gb_free 36.8 | wall 674\n",
            "2023-04-30 20:45:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 005:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:45:48 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-04-30 20:45:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005: 100% 1347/1348 [02:25<00:00,  9.50it/s, loss=5.428, nll_loss=4.038, ppl=16.42, wps=24195.2, ups=9.3, wpb=2600.7, bsz=112.6, num_updates=6700, lr=0.000386334, gnorm=1.255, train_wall=10, gb_free=36.9, wall=815]2023-04-30 20:48:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:48:13 | INFO | fairseq.tasks.translation | example hypothesis: It's like to ask someone's sun.\n",
            "2023-04-30 20:48:13 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   8% 1/12 [00:00<00:06,  1.80it/s]\u001b[A2023-04-30 20:48:14 | INFO | fairseq.tasks.translation | example hypothesis: And you'll discover what it would be to be able to talk about its own language.\n",
            "2023-04-30 20:48:14 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  17% 2/12 [00:01<00:05,  1.81it/s]\u001b[A2023-04-30 20:48:15 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't make a training of reality; it's a solution of reality.\n",
            "2023-04-30 20:48:15 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  25% 3/12 [00:01<00:04,  1.81it/s]\u001b[A2023-04-30 20:48:15 | INFO | fairseq.tasks.translation | example hypothesis: So I'm here -- it's a moment of Fiacal that I do with a simple piece of equation.\n",
            "2023-04-30 20:48:15 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  33% 4/12 [00:02<00:04,  1.78it/s]\u001b[A2023-04-30 20:48:16 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and as a sophisticated person, say, \"What knows this fish?\n",
            "2023-04-30 20:48:16 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  42% 5/12 [00:02<00:04,  1.73it/s]\u001b[A2023-04-30 20:48:16 | INFO | fairseq.tasks.translation | example hypothesis: He started feeling the old old girl coming up with him: I'm going to lose it and I'm going to be inspired by this song at all.\n",
            "2023-04-30 20:48:16 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  50% 6/12 [00:03<00:03,  1.64it/s]\u001b[A2023-04-30 20:48:17 | INFO | fairseq.tasks.translation | example hypothesis: The second and third generation is not going to happen, the ones that turn the sugar into much more powerful like this or different types of wheat.\n",
            "2023-04-30 20:48:17 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.53it/s]\u001b[A2023-04-30 20:48:18 | INFO | fairseq.tasks.translation | example hypothesis: When you go to very important bits, we can't grow them in a bacteria. We've reached the limits of modern biology. So we're moved to other mechanisms.\n",
            "2023-04-30 20:48:18 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  67% 8/12 [00:05<00:02,  1.34it/s]\u001b[A2023-04-30 20:48:19 | INFO | fairseq.tasks.translation | example hypothesis: They got these pictures out of land, so they tried to look at the face to give a form, an idea didn't find and found out that it was from the Earth. And so they had the five to get them.\n",
            "2023-04-30 20:48:19 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.21it/s]\u001b[A2023-04-30 20:48:20 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the profound aspects of the journey and one of these research research research research research research research research is the possibility of living among those who haven't heard the old, who still heard their past in the wind, which is in the rain, which is in the rain, by the rain, by the rain.\n",
            "2023-04-30 20:48:20 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  83% 10/12 [00:07<00:01,  1.08it/s]\u001b[A2023-04-30 20:48:22 | INFO | fairseq.tasks.translation | example hypothesis: And we're going to be able to do the sphere where we're seeing our audience in places like cultural events that they can't stop to be aware of what they saw, and therefore hope that will recognize, in a way that we have seen, by the way, by the way, by the way, the way, the way, is to find out what the world can actually live in the world.\n",
            "2023-04-30 20:48:22 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:  92% 11/12 [00:09<00:01,  1.19s/it]\u001b[A2023-04-30 20:48:25 | INFO | fairseq.tasks.translation | example hypothesis: And when you look in the world, you find that they're not cultures to disappear. These are people living and dynamic dynamics of their existence. Or from the European forest, which is the famous tribe in the European country -- one of the people in South Asia, and it's the people who died from the forest, and it's going to go to the European forest, and it's the way to the European forest, where it's the European forest, and it's the European forest, and it's the European forest, where it's the European forest, and it's the European forest, and it's the European forest of people who died from the European forest.\n",
            "2023-04-30 20:48:25 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 005 | valid on 'valid' subset: 100% 12/12 [00:11<00:00,  1.66s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:48:25 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.952 | nll_loss 4.527 | ppl 23.05 | bleu 18.78 | wps 1706.6 | wpb 1752.2 | bsz 69.1 | num_updates 6740 | best_bleu 18.78\n",
            "2023-04-30 20:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6740 updates\n",
            "2023-04-30 20:48:25 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint5.pt\n",
            "2023-04-30 20:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint5.pt\n",
            "2023-04-30 20:48:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint5.pt (epoch 5 @ 6740 updates, score 18.78) (writing took 10.047957614000097 seconds)\n",
            "2023-04-30 20:48:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-04-30 20:48:35 | INFO | train | epoch 005 | loss 5.56 | nll_loss 4.19 | ppl 18.25 | wps 20535.6 | ups 8.06 | wpb 2548.6 | bsz 116.4 | num_updates 6740 | lr 0.000385186 | gnorm 1.291 | train_wall 142 | gb_free 37 | wall 841\n",
            "2023-04-30 20:48:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 006:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:48:35 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-04-30 20:48:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006: 100% 1346/1348 [02:25<00:00,  9.46it/s, loss=5.096, nll_loss=3.661, ppl=12.65, wps=24039.8, ups=9.34, wpb=2572.7, bsz=123.6, num_updates=8000, lr=0.000353553, gnorm=1.376, train_wall=10, gb_free=36.7, wall=977]2023-04-30 20:51:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:51:01 | INFO | fairseq.tasks.translation | example hypothesis: It's like asking someone to resist the sun.\n",
            "2023-04-30 20:51:01 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   8% 1/12 [00:00<00:07,  1.54it/s]\u001b[A2023-04-30 20:51:02 | INFO | fairseq.tasks.translation | example hypothesis: And you'll discover what it would be like to be able to be able to talk about its own language.\n",
            "2023-04-30 20:51:02 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  17% 2/12 [00:01<00:06,  1.54it/s]\u001b[A2023-04-30 20:51:02 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't cause a reality training of reality; but rather a solution of reality.\n",
            "2023-04-30 20:51:02 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  25% 3/12 [00:01<00:05,  1.56it/s]\u001b[A2023-04-30 20:51:03 | INFO | fairseq.tasks.translation | example hypothesis: So, I'm here -- this is a moment of Siacacacacacacacacacacacacacacacan that I do with a simple equation of equation.\n",
            "2023-04-30 20:51:03 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  33% 4/12 [00:02<00:05,  1.49it/s]\u001b[A2023-04-30 20:51:04 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and, as a sophisticated person, say, \"What does this fish know?\n",
            "2023-04-30 20:51:04 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  42% 5/12 [00:03<00:04,  1.45it/s]\u001b[A2023-04-30 20:51:04 | INFO | fairseq.tasks.translation | example hypothesis: He started to feel the old anxiety of getting into him. I'm going to lose it and I'm going to be overwhelmed by this song at all.\n",
            "2023-04-30 20:51:04 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  50% 6/12 [00:04<00:04,  1.43it/s]\u001b[A2023-04-30 20:51:05 | INFO | fairseq.tasks.translation | example hypothesis: The second and third generations are not going to blow into, those that turn sugar into much more powerful like the oil or the different kinds of turins.\n",
            "2023-04-30 20:51:05 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.34it/s]\u001b[A2023-04-30 20:51:06 | INFO | fairseq.tasks.translation | example hypothesis: When you get to large pieces of important pieces, you can't grow them easily in a giant bacteria. We have reached the limitations of molecular biology. So we've gone to other mechanisms.\n",
            "2023-04-30 20:51:06 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  67% 8/12 [00:05<00:03,  1.26it/s]\u001b[A2023-04-30 20:51:07 | INFO | fairseq.tasks.translation | example hypothesis: They picked up these photographs by the ground, tried to look behind the face to distinguish a form, an idea, and found nothing that it was maps of the Earth. And so they broke the five-year-olds.\n",
            "2023-04-30 20:51:07 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.21it/s]\u001b[A2023-04-30 20:51:08 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the intense threats of the journey and one of those research research research research research is the possibility of living among those who have not forgotten old people, who have never seen their past in the wind, who are touching the rocks by the rain, in the trees.\n",
            "2023-04-30 20:51:08 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  83% 10/12 [00:07<00:01,  1.04it/s]\u001b[A2023-04-30 20:51:10 | INFO | fairseq.tasks.translation | example hypothesis: And we do travel to the sphere where we take our audience in places such cultural places that they can't prevent by what they've seen, and therefore hope to understand it as a sense of consciousness and measure it, by the way, through a sense of consciousness: what we can really find in a way that we can actually live in a collective world where we can actually contribute to all of our collective wisdom.\n",
            "2023-04-30 20:51:10 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:  92% 11/12 [00:09<00:01,  1.08s/it]\u001b[A2023-04-30 20:51:11 | INFO | fairseq.tasks.translation | example hypothesis: And when you look around the world, you find that it's not cultures to disappear. These are people who are living, and they're living in some of their existence that affect their adaptation. Whether it's the famous plant in the developing countries -- one of the South Asia, who lived in the river, and it's a place for the people who have now moved to the south of the river, and it's a place where they're in the north of Russia.\n",
            "2023-04-30 20:51:11 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 006 | valid on 'valid' subset: 100% 12/12 [00:10<00:00,  1.17s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:51:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.641 | nll_loss 4.16 | ppl 17.88 | bleu 22.11 | wps 1917.2 | wpb 1752.2 | bsz 69.1 | num_updates 8088 | best_bleu 22.11\n",
            "2023-04-30 20:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8088 updates\n",
            "2023-04-30 20:51:11 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint6.pt\n",
            "2023-04-30 20:51:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint6.pt\n",
            "2023-04-30 20:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint6.pt (epoch 6 @ 8088 updates, score 22.11) (writing took 10.006996302000061 seconds)\n",
            "2023-04-30 20:51:21 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-04-30 20:51:21 | INFO | train | epoch 006 | loss 5.147 | nll_loss 3.718 | ppl 13.16 | wps 20667.5 | ups 8.11 | wpb 2548.6 | bsz 116.4 | num_updates 8088 | lr 0.000351625 | gnorm 1.262 | train_wall 142 | gb_free 36.8 | wall 1007\n",
            "2023-04-30 20:51:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 007:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:51:21 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-04-30 20:51:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007: 100% 1347/1348 [02:25<00:00,  9.63it/s, loss=4.837, nll_loss=3.368, ppl=10.32, wps=24085.6, ups=9.31, wpb=2586.2, bsz=121.8, num_updates=9400, lr=0.000326164, gnorm=1.267, train_wall=10, gb_free=37, wall=1149]2023-04-30 20:53:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:53:47 | INFO | fairseq.tasks.translation | example hypothesis: It's like asking somebody to break the sun.\n",
            "2023-04-30 20:53:47 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   8% 1/12 [00:00<00:06,  1.63it/s]\u001b[A2023-04-30 20:53:48 | INFO | fairseq.tasks.translation | example hypothesis: And you'll discover what it would be to be able to talk about its own language.\n",
            "2023-04-30 20:53:48 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  17% 2/12 [00:01<00:05,  1.74it/s]\u001b[A2023-04-30 20:53:48 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't cause reality training, but rather a solution for reality.\n",
            "2023-04-30 20:53:48 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  25% 3/12 [00:01<00:05,  1.74it/s]\u001b[A2023-04-30 20:53:49 | INFO | fairseq.tasks.translation | example hypothesis: So here I am -- this is a couple of words I do with a simple equation.\n",
            "2023-04-30 20:53:49 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  33% 4/12 [00:02<00:04,  1.71it/s]\u001b[A2023-04-30 20:53:50 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and, as a sophisticated person, say, \"What knows this fish?\n",
            "2023-04-30 20:53:50 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  42% 5/12 [00:02<00:04,  1.68it/s]\u001b[A2023-04-30 20:53:50 | INFO | fairseq.tasks.translation | example hypothesis: He started feeling the old anxiety in him. I'm going to lose it and I'm going to be overwhelmed with this song forever.\n",
            "2023-04-30 20:53:50 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  50% 6/12 [00:03<00:03,  1.64it/s]\u001b[A2023-04-30 20:53:51 | INFO | fairseq.tasks.translation | example hypothesis: The fossil fuels of second and third generations are not going to go to appear, the ones that turn the sugar into a lot more powerful than the oil.\n",
            "2023-04-30 20:53:51 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.48it/s]\u001b[A2023-04-30 20:53:52 | INFO | fairseq.tasks.translation | example hypothesis: When you get large bits of bits, you can't grow them very easily in a bacteria. We get the limits of modern biology. So we're getting to other mechanisms.\n",
            "2023-04-30 20:53:52 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  67% 8/12 [00:05<00:02,  1.40it/s]\u001b[A2023-04-30 20:53:53 | INFO | fairseq.tasks.translation | example hypothesis: They picked up these photos on the ground, tried to watch behind the face to distinguish a form, a notion, didn't find anything and dismissed that it was the hell cards. So they missed the five rupees.\n",
            "2023-04-30 20:53:53 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.28it/s]\u001b[A2023-04-30 20:53:54 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the intense pleasures of the journey and one of the research of the graph is the ability to live among those who didn't forget the old old people, who still feel their hands in the wind, who's touching the rocks by the rain.\n",
            "2023-04-30 20:53:54 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  83% 10/12 [00:07<00:01,  1.20it/s]\u001b[A2023-04-30 20:53:55 | INFO | fairseq.tasks.translation | example hypothesis: And we make travel to the sphere where we take our audience in places of such cultural wonder that they can't prevent themselves from being overwhelmed by what they've seen, and therefore, let's understand it as a measure, and to a measure, by the sense of anthropology: what's worth in all its diversity, that we can find in a way that we can live in a truly collective world where we can contribute to all of our collective wisdom.\n",
            "2023-04-30 20:53:55 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:  92% 11/12 [00:08<00:00,  1.07it/s]\u001b[A2023-04-30 20:53:56 | INFO | fairseq.tasks.translation | example hypothesis: And when you look at the world, you find that it's not cultures to disappear. These are living and dynamic people who are living in their own lives of their adaptation. Whether it's the famous person in the country's country -- or South Asia, which is a people who live in rural Asia, which is a free group of people who are now to the Japanese forest, and to see the Japanese, which is the Japanese, where they're now the Japanese, and they can release the Japanese trees.\n",
            "2023-04-30 20:53:56 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 007 | valid on 'valid' subset: 100% 12/12 [00:09<00:00,  1.06s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:53:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.483 | nll_loss 4.006 | ppl 16.06 | bleu 23.48 | wps 2154.7 | wpb 1752.2 | bsz 69.1 | num_updates 9436 | best_bleu 23.48\n",
            "2023-04-30 20:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 9436 updates\n",
            "2023-04-30 20:53:56 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint7.pt\n",
            "2023-04-30 20:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint7.pt\n",
            "2023-04-30 20:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint7.pt (epoch 7 @ 9436 updates, score 23.48) (writing took 10.795664622999993 seconds)\n",
            "2023-04-30 20:54:07 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-04-30 20:54:07 | INFO | train | epoch 007 | loss 4.874 | nll_loss 3.407 | ppl 10.61 | wps 20707.4 | ups 8.13 | wpb 2548.6 | bsz 116.4 | num_updates 9436 | lr 0.000325541 | gnorm 1.226 | train_wall 142 | gb_free 36.7 | wall 1173\n",
            "2023-04-30 20:54:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 008:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:54:07 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-04-30 20:54:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008: 100% 1347/1348 [02:25<00:00,  9.14it/s, loss=4.722, nll_loss=3.236, ppl=9.42, wps=23808.2, ups=9.36, wpb=2542.6, bsz=108.4, num_updates=10700, lr=0.000305709, gnorm=1.223, train_wall=10, gb_free=37, wall=1310]2023-04-30 20:56:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:56:33 | INFO | fairseq.tasks.translation | example hypothesis: It's like asking someone to erase the sun.\n",
            "2023-04-30 20:56:33 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   8% 1/12 [00:00<00:06,  1.78it/s]\u001b[A2023-04-30 20:56:34 | INFO | fairseq.tasks.translation | example hypothesis: And you'll discover what it would be to be unable to talk about its own language.\n",
            "2023-04-30 20:56:34 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  17% 2/12 [00:01<00:05,  1.69it/s]\u001b[A2023-04-30 20:56:35 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't cause a reality training, but rather a solution of reality.\n",
            "2023-04-30 20:56:35 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  25% 3/12 [00:01<00:05,  1.75it/s]\u001b[A2023-04-30 20:56:35 | INFO | fairseq.tasks.translation | example hypothesis: So I -- this is a couple of things I do with a simple equation.\n",
            "2023-04-30 20:56:35 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  33% 4/12 [00:02<00:04,  1.72it/s]\u001b[A2023-04-30 20:56:36 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and as a sophisticated person, say, \"What does this fish know?\n",
            "2023-04-30 20:56:36 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  42% 5/12 [00:02<00:04,  1.62it/s]\u001b[A2023-04-30 20:56:37 | INFO | fairseq.tasks.translation | example hypothesis: He started to feel the old anxiety of him. I'm going to lose it and I'm going to be upset with this song forever.\n",
            "2023-04-30 20:56:37 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  50% 6/12 [00:03<00:03,  1.53it/s]\u001b[A2023-04-30 20:56:37 | INFO | fairseq.tasks.translation | example hypothesis: The second and third generations are not going to be able to appear, the ones that turn the sugar into much more powerful like this or so-called the oil or different kinds of barks.\n",
            "2023-04-30 20:56:37 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.41it/s]\u001b[A2023-04-30 20:56:38 | INFO | fairseq.tasks.translation | example hypothesis: When you come to very large bits of pieces, you can't grow them very easily in a bacterium. We've reached the limits of modern molecular biology. So we've gone to other mechanisms.\n",
            "2023-04-30 20:56:38 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  67% 8/12 [00:05<00:03,  1.32it/s]\u001b[A2023-04-30 20:56:39 | INFO | fairseq.tasks.translation | example hypothesis: They took these photos out of the ground, tried to look behind the face to distinguish a form, a figure, didn't find anything and they found it out that it was some maps coming out of the devil. So they dropped the five towers to the face.\n",
            "2023-04-30 20:56:39 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.23it/s]\u001b[A2023-04-30 20:56:40 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the intense pleasures of the journey and one of the things that's looking at is the possibility of living in between those who have not forgotten the old kill, who still feel their past in the wind, who touch it in the dark rocks by the rain, the fall in the leaves of the plants.\n",
            "2023-04-30 20:56:40 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  83% 10/12 [00:07<00:01,  1.13it/s]\u001b[A2023-04-30 20:56:42 | INFO | fairseq.tasks.translation | example hypothesis: And we're making travel to the country where we're taking our public into places like cultural wonders that they can't prevent from being overwhelmed by what they've seen, and so hopefully, let's understand it as the way and measure, through a sense, through the basis of anthropology: that what's worth of the world in all its diversity, that we can find is that we can find a way to live in a truly diverse world where all of the peoples can contribute to our collective wisdom.\n",
            "2023-04-30 20:56:42 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:  92% 11/12 [00:09<00:01,  1.14s/it]\u001b[A2023-04-30 20:56:45 | INFO | fairseq.tasks.translation | example hypothesis: And when you look around the world, you find out that they're not all cultures to disappear. These are living people and dynamic people out of their existence with common currents. Whether it's the famous devent of the country in the country of Venezuela -- one of the people in southern Asia -- there are some people who live in a forest that lived freely in the previous generation, who are now ready to see where it is to take care of the river to the coast of Russia, which is in the east River, where it's ready to take place where it's ready to take place where it's ready to see the river is to take place where it's in the east to see the east side of the river, or to see it's ready to take place where it's ready to take place where it's ready to take place where it's ready to take place where it's ready to see it's in the river is to take place where it is to go to go to see the east side of the city of the city of the river is to go from, or to see it's been to see it's been to see the east of the river of the city of\n",
            "2023-04-30 20:56:45 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 008 | valid on 'valid' subset: 100% 12/12 [00:11<00:00,  1.57s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:56:45 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.348 | nll_loss 3.831 | ppl 14.23 | bleu 24.76 | wps 1742.4 | wpb 1752.2 | bsz 69.1 | num_updates 10784 | best_bleu 24.76\n",
            "2023-04-30 20:56:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 10784 updates\n",
            "2023-04-30 20:56:45 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint8.pt\n",
            "2023-04-30 20:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint8.pt\n",
            "2023-04-30 20:56:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint8.pt (epoch 8 @ 10784 updates, score 24.76) (writing took 10.433732254000006 seconds)\n",
            "2023-04-30 20:56:55 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-04-30 20:56:55 | INFO | train | epoch 008 | loss 4.684 | nll_loss 3.192 | ppl 9.14 | wps 20448.7 | ups 8.02 | wpb 2548.6 | bsz 116.4 | num_updates 10784 | lr 0.000304516 | gnorm 1.212 | train_wall 142 | gb_free 36.8 | wall 1341\n",
            "2023-04-30 20:56:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 009:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:56:55 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-04-30 20:56:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009: 100% 1347/1348 [02:25<00:00,  9.87it/s, loss=4.535, nll_loss=3.025, ppl=8.14, wps=23566.7, ups=9.38, wpb=2513.7, bsz=111.6, num_updates=12100, lr=0.00028748, gnorm=1.259, train_wall=10, gb_free=36.8, wall=1484]2023-04-30 20:59:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 20:59:22 | INFO | fairseq.tasks.translation | example hypothesis: It's like asking someone to explode the sun.\n",
            "2023-04-30 20:59:22 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   8% 1/12 [00:00<00:05,  1.94it/s]\u001b[A2023-04-30 20:59:22 | INFO | fairseq.tasks.translation | example hypothesis: And you'll discover what it would be like to be unable to talk about its own language.\n",
            "2023-04-30 20:59:22 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  17% 2/12 [00:01<00:05,  1.83it/s]\u001b[A2023-04-30 20:59:23 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't cause reality training, but rather a solution to reality.\n",
            "2023-04-30 20:59:23 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  25% 3/12 [00:01<00:05,  1.75it/s]\u001b[A2023-04-30 20:59:23 | INFO | fairseq.tasks.translation | example hypothesis: So here I -- this is a moment from Fiacanti. What I do with a simple equation of equation.\n",
            "2023-04-30 20:59:23 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  33% 4/12 [00:02<00:04,  1.74it/s]\u001b[A2023-04-30 20:59:24 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and, as a sophisticated person, say, \"What does this fish know?\n",
            "2023-04-30 20:59:24 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  42% 5/12 [00:02<00:04,  1.72it/s]\u001b[A2023-04-30 20:59:25 | INFO | fairseq.tasks.translation | example hypothesis: He started to feel the old anxiety by him. I'm going to lose it and I'm going to be tortured by this song forever.\n",
            "2023-04-30 20:59:25 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  50% 6/12 [00:03<00:03,  1.63it/s]\u001b[A2023-04-30 20:59:25 | INFO | fairseq.tasks.translation | example hypothesis: The second and third generation is not going to appear, the ones that turn the fuel sugar into much more powerful, like the oil supply or different kinds of ruods.\n",
            "2023-04-30 20:59:25 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.51it/s]\u001b[A2023-04-30 20:59:26 | INFO | fairseq.tasks.translation | example hypothesis: When you get to the size of important pieces, you can't grow them very easily in a bacteria bacteria. We hit the limitations of modern molecular biology tools. So we've turned to other mechanisms.\n",
            "2023-04-30 20:59:26 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  67% 8/12 [00:05<00:02,  1.45it/s]\u001b[A2023-04-30 20:59:27 | INFO | fairseq.tasks.translation | example hypothesis: They picked these photos up by the ground, tried to look behind the face to distinguish a shape, a figure, didn't find anything, and they blew it up that it was maps coming from the devil. So they beat the five rupees.\n",
            "2023-04-30 20:59:27 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.28it/s]\u001b[A2023-04-30 20:59:28 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the intense pleasures of travel and one of the most intense pleasures of research research is the possibility of living among those who have not forgotten the old killing, who still feel their past in the wind, who touch it in the rain, the pen in the leaves of the plants.\n",
            "2023-04-30 20:59:28 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  83% 10/12 [00:07<00:01,  1.17it/s]\u001b[A2023-04-30 20:59:29 | INFO | fairseq.tasks.translation | example hypothesis: And we're making travel to globalization, where we're taking our public into places like cultural wonder that they can't prevent by what they've seen, and therefore, hopefully understand it in the way and, by one, by a cultural basis, is that what the world deserves in all of its diversity, that we can find a way to live in a truly secular world where all of the peoples can contribute to all of our collective wisdom.\n",
            "2023-04-30 20:59:29 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:  92% 11/12 [00:08<00:01,  1.01s/it]\u001b[A2023-04-30 20:59:32 | INFO | fairseq.tasks.translation | example hypothesis: And when you look around the world, you find that it's not sustainable cultures to disappear. These are living and dynamic people out of their existence through common currents that are facing their adaptation. Whether it's the famous devert to the country of the Noran -- one of the south people of South Asia, Sarak -- a people who lived freely in the forest until the first generation, and now are pushed down to the coast of slavery and down to the coast of the country. Or you can see the Japanese rivers, where you can see it's in the West Park.\n",
            "2023-04-30 20:59:32 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 009 | valid on 'valid' subset: 100% 12/12 [00:10<00:00,  1.48s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 20:59:32 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.275 | nll_loss 3.767 | ppl 13.61 | bleu 25.6 | wps 1851.5 | wpb 1752.2 | bsz 69.1 | num_updates 12132 | best_bleu 25.6\n",
            "2023-04-30 20:59:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12132 updates\n",
            "2023-04-30 20:59:32 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint9.pt\n",
            "2023-04-30 20:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint9.pt\n",
            "2023-04-30 20:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint9.pt (epoch 9 @ 12132 updates, score 25.6) (writing took 10.228094417999728 seconds)\n",
            "2023-04-30 20:59:42 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-04-30 20:59:42 | INFO | train | epoch 009 | loss 4.531 | nll_loss 3.018 | ppl 8.1 | wps 20546.9 | ups 8.06 | wpb 2548.6 | bsz 116.4 | num_updates 12132 | lr 0.0002871 | gnorm 1.194 | train_wall 142 | gb_free 36.8 | wall 1508\n",
            "2023-04-30 20:59:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1348\n",
            "epoch 010:   0% 0/1348 [00:00<?, ?it/s]2023-04-30 20:59:42 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-04-30 20:59:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010: 100% 1347/1348 [02:25<00:00,  9.59it/s, loss=4.371, nll_loss=2.839, ppl=7.16, wps=23821.6, ups=9.39, wpb=2536.6, bsz=122.6, num_updates=13400, lr=0.000273179, gnorm=1.201, train_wall=10, gb_free=37, wall=1646]2023-04-30 21:02:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A2023-04-30 21:02:09 | INFO | fairseq.tasks.translation | example hypothesis: It's like asking someone to ruin the sun.\n",
            "2023-04-30 21:02:09 | INFO | fairseq.tasks.translation | example reference: It's like asking somebody to swallow the sun.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   8% 1/12 [00:00<00:06,  1.59it/s]\u001b[A2023-04-30 21:02:09 | INFO | fairseq.tasks.translation | example hypothesis: And you will discover what it would be like to be unable to speak about its own language.\n",
            "2023-04-30 21:02:09 | INFO | fairseq.tasks.translation | example reference: And you'll suddenly discover what it would be like to be unable to speak your own language.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  17% 2/12 [00:01<00:05,  1.75it/s]\u001b[A2023-04-30 21:02:10 | INFO | fairseq.tasks.translation | example hypothesis: It doesn't cause reality training, but rather a solution to reality.\n",
            "2023-04-30 21:02:10 | INFO | fairseq.tasks.translation | example reference: It doesn't create the distortion of reality; it creates the dissolution of reality.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  25% 3/12 [00:01<00:05,  1.77it/s]\u001b[A2023-04-30 21:02:10 | INFO | fairseq.tasks.translation | example hypothesis: So here I am -- this is a couple of Fiacacthis that I'm doing with a simple equation program.\n",
            "2023-04-30 21:02:10 | INFO | fairseq.tasks.translation | example reference: So here I'm -- This is a Fibonacci sequence that I'm making with a simple equation program.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  33% 4/12 [00:02<00:04,  1.75it/s]\u001b[A2023-04-30 21:02:11 | INFO | fairseq.tasks.translation | example hypothesis: To conclude. You're supposed to read this, and, as a sophisticated person, say, \"What does this fish know?\n",
            "2023-04-30 21:02:11 | INFO | fairseq.tasks.translation | example reference: So to conclude. You're supposed to read this cartoon, and, being a sophisticated person, say, \"Ah! What does this fish know?\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  42% 5/12 [00:03<00:04,  1.58it/s]\u001b[A2023-04-30 21:02:12 | INFO | fairseq.tasks.translation | example hypothesis: He started to feel the old anxiety going up to him. I'm going to lose it and I'm going to be tortured by this song forever.\n",
            "2023-04-30 21:02:12 | INFO | fairseq.tasks.translation | example reference: So he starts to feel all of that old anxiety start to rise in him like, \"I'm going to lose this thing, and then I'm going to be haunted by this song forever.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  50% 6/12 [00:03<00:04,  1.46it/s]\u001b[A2023-04-30 21:02:13 | INFO | fairseq.tasks.translation | example hypothesis: The second and third generation is not going to be able to appear, the ones that transform the sugar into much stronger fuels like the oil supply.\n",
            "2023-04-30 21:02:13 | INFO | fairseq.tasks.translation | example reference: We have second- and third-generation fuels that will be coming out relatively soon that are sugar, to much higher-value fuels like octane or different types of butanol.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  58% 7/12 [00:04<00:03,  1.39it/s]\u001b[A2023-04-30 21:02:14 | INFO | fairseq.tasks.translation | example hypothesis: When you get to the sizes of important pieces, you can't grow them in a bacteria that's easily. We hit the boundaries of the tools of modern molecular biology. So we went to other mechanisms.\n",
            "2023-04-30 21:02:14 | INFO | fairseq.tasks.translation | example reference: When we get into these really large pieces over 100,000 base pairs, they won't any longer grow readily in E. coli -- it exhausts all the modern tools of molecular biology -- and so we turned to other mechanisms.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  67% 8/12 [00:05<00:03,  1.23it/s]\u001b[A2023-04-30 21:02:15 | INFO | fairseq.tasks.translation | example hypothesis: They picked up these photos of the earth, tried to look behind the face to distinguish a form, a figure, didn't find anything in it. And so it was maps coming from the devil. And so they burned the five rupees at a time.\n",
            "2023-04-30 21:02:15 | INFO | fairseq.tasks.translation | example reference: They picked up these photographs from the forest floor, tried to look behind the face to find the form or the figure, found nothing, and concluded that these were calling cards from the devil, so they speared the five missionaries to death.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  75% 9/12 [00:06<00:02,  1.19it/s]\u001b[A2023-04-30 21:02:16 | INFO | fairseq.tasks.translation | example hypothesis: You know, one of the intense pleasures of the journey and one of these graphs of research is the ability to live among those who have not forgotten the old tuds of mine, who still feels their way into the wind, who touches it in the dark rocks, the pond in the leaves of the plant.\n",
            "2023-04-30 21:02:16 | INFO | fairseq.tasks.translation | example reference: You know, one of the intense pleasures of travel and one of the delights of ethnographic research is the opportunity to live amongst those who have not forgotten the old ways, who still feel their past in the wind, touch it in stones polished by rain, taste it in the bitter leaves of plants.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  83% 10/12 [00:07<00:01,  1.12it/s]\u001b[A2023-04-30 21:02:17 | INFO | fairseq.tasks.translation | example hypothesis: And we're making travel to Christianity, where we're taking our audience to places like cultural wonders that they can't prevent from being overwhelmed by what they've seen, and therefore, let's understand it as the collective and measure, one by one, the central insight of anthropology: what this world deserves in all its diversity, that we can find a way to live in a truly spiritual world where all of people can contribute to our collective wisdom.\n",
            "2023-04-30 21:02:17 | INFO | fairseq.tasks.translation | example reference: And what we're doing is a series of journeys to the ethnosphere where we're going to take our audience to places of such cultural wonder that they cannot help but come away dazzled by what they have seen, and hopefully, therefore, embrace gradually, one by one, the central revelation of anthropology: that this world deserves to exist in a diverse way, that we can find a way to live in a truly multicultural, pluralistic world where all of the wisdom of all peoples can contribute to our collective well-being.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:  92% 11/12 [00:09<00:01,  1.14s/it]\u001b[A2023-04-30 21:02:20 | INFO | fairseq.tasks.translation | example hypothesis: And when you look around the world, you find that these are not human-to-away crops. These are living people, and dynamic people outside their existence with common adaptation. Whether it's the famous dededeward in the country -- a people in South Asia, from Sarak -- a people who lived free in the forest, who are now at the end of slavery, where we can see where it is, in the country where it's the ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫ ♫\n",
            "2023-04-30 21:02:20 | INFO | fairseq.tasks.translation | example reference: Wherever you look around the world, you discover that these are not cultures destined to fade away; these are dynamic living peoples being driven out of existence by identifiable forces that are beyond their capacity to adapt to: whether it's the egregious deforestation in the homeland of the Penan -- a nomadic people from Southeast Asia, from Sarawak -- a people who lived free in the forest until a generation ago, and now have all been reduced to servitude and prostitution on the banks of the rivers, where you can see the river itself is soiled with the silt that seems to be carrying half of Borneo away to the South China Sea, where the Japanese freighters hang light in the horizon ready to fill their holds with raw logs ripped from the forest -- or, in the case of the Yanomami, it's the disease entities that have come in, in the wake of the discovery of gold.\n",
            "\n",
            "epoch 010 | valid on 'valid' subset: 100% 12/12 [00:11<00:00,  1.63s/it]\u001b[A\n",
            "                                                                        \u001b[A2023-04-30 21:02:20 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.204 | nll_loss 3.668 | ppl 12.71 | bleu 25.98 | wps 1701.5 | wpb 1752.2 | bsz 69.1 | num_updates 13480 | best_bleu 25.98\n",
            "2023-04-30 21:02:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 13480 updates\n",
            "2023-04-30 21:02:20 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint10.pt\n",
            "2023-04-30 21:02:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/hw1/checkpoints/tf_bpe/checkpoint10.pt\n",
            "2023-04-30 21:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/tf_bpe/checkpoint10.pt (epoch 10 @ 13480 updates, score 25.98) (writing took 10.152841503999753 seconds)\n",
            "2023-04-30 21:02:30 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-04-30 21:02:30 | INFO | train | epoch 010 | loss 4.411 | nll_loss 2.883 | ppl 7.38 | wps 20430.6 | ups 8.02 | wpb 2548.6 | bsz 116.4 | num_updates 13480 | lr 0.000272367 | gnorm 1.196 | train_wall 142 | gb_free 36.9 | wall 1677\n",
            "2023-04-30 21:02:30 | INFO | fairseq_cli.train | done training in 1676.5 seconds\n"
          ]
        }
      ],
      "source": [
        "!fairseq-train data-bin/iwslt13_fr_en \\\n",
        "    --arch transformer --share-decoder-input-output-embed \\\n",
        "    --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr 5e-4 --lr-scheduler inverse_sqrt \\\n",
        "    --warmup-updates 4000 \\\n",
        "    --dropout 0.3 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
        "    --max-tokens 3000 --eval-bleu --eval-bleu-args '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}' \\\n",
        "    --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples \\\n",
        "    --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
        "    --save-dir checkpoints/tf_bpe \\\n",
        "    --patience 10 \\\n",
        "    --max-epoch 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB3CoA_4sB_U"
      },
      "source": [
        "# Evaluate on Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR5oIqLP8ZN8",
        "outputId": "df27fd80-21f1-4a10-b4e4-88fcf3c5059c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 21:02:45 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/tf_bpe/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/tf_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'valid', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 21:02:45 | INFO | fairseq.tasks.translation | [fr] dictionary: 30192 types\n",
            "2023-04-30 21:02:45 | INFO | fairseq.tasks.translation | [en] dictionary: 24808 types\n",
            "2023-04-30 21:02:45 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/tf_bpe/checkpoint_best.pt\n",
            "2023-04-30 21:02:47 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.fr\n",
            "2023-04-30 21:02:47 | INFO | fairseq.data.data_utils | loaded 829 examples from: data-bin/iwslt13_fr_en/valid.fr-en.en\n",
            "2023-04-30 21:02:47 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en valid fr-en 829 examples\n",
            "2023-04-30 21:03:03 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-04-30 21:03:03 | INFO | fairseq_cli.generate | Translated 829 sentences (19,991 tokens) in 8.6s (96.62 sentences/s, 2329.86 tokens/s)\n",
            "2023-04-30 21:03:03 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-04-30 21:03:03 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-04-30 21:03:03 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en \\\n",
        "    --path checkpoints/tf_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --remove-bpe \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/tf_bpe.pred --gen-subset valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CS-19lksLOt"
      },
      "source": [
        "# Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp3Q7sH9sNYa",
        "outputId": "42fff7ae-39b3-476b-9e8a-570583197854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-30 21:03:12 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/tf_bpe/checkpoint_best.pt', 'post_process': 'subword_nmt', 'quiet': False, 'model_overrides': '{}', 'results_path': 'result/tf_bpe.pred'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': True, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': 'data-bin/iwslt13_fr_en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'sacrebleu', 'sacrebleu_tokenizer': '13a', 'sacrebleu_lowercase': False, 'sacrebleu_char_level': False}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-04-30 21:03:12 | INFO | fairseq.tasks.translation | [fr] dictionary: 30192 types\n",
            "2023-04-30 21:03:12 | INFO | fairseq.tasks.translation | [en] dictionary: 24808 types\n",
            "2023-04-30 21:03:12 | INFO | fairseq_cli.generate | loading model(s) from checkpoints/tf_bpe/checkpoint_best.pt\n",
            "2023-04-30 21:03:15 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en/test.fr-en.fr\n",
            "2023-04-30 21:03:15 | INFO | fairseq.data.data_utils | loaded 1,664 examples from: data-bin/iwslt13_fr_en/test.fr-en.en\n",
            "2023-04-30 21:03:15 | INFO | fairseq.tasks.translation | data-bin/iwslt13_fr_en test fr-en 1664 examples\n",
            "2023-04-30 21:03:33 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n",
            "2023-04-30 21:03:33 | INFO | fairseq_cli.generate | Translated 1,664 sentences (34,096 tokens) in 7.3s (228.92 sentences/s, 4690.75 tokens/s)\n",
            "2023-04-30 21:03:33 | WARNING | sacrebleu | That's 100 lines that end in a tokenized period ('.')\n",
            "2023-04-30 21:03:33 | WARNING | sacrebleu | It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "2023-04-30 21:03:33 | WARNING | sacrebleu | If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        }
      ],
      "source": [
        "!fairseq-generate data-bin/iwslt13_fr_en \\\n",
        "    --path checkpoints/tf_bpe/checkpoint_best.pt \\\n",
        "    --batch-size 128 \\\n",
        "    --beam 5 \\\n",
        "    --remove-bpe \\\n",
        "    --scoring sacrebleu --sacrebleu --results-path result/tf_bpe.pred"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
